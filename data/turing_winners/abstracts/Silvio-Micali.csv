2019,Algorand: A secure and efficient distributed ledger.,"Abstract
A distributed ledger is a tamperproof sequence of data that can be publicly accessed and augmented by everyone, without being maintained by a centralized party. Distributed ledgers stand to revolutionize the way a modern society operates. They can secure all kinds of traditional transactions, such as payments, asset transfers and titles, in the exact order in which the transactions occur; and enable totally new transactions, such as cryptocurrencies and smart contracts. They can remove intermediaries and usher in a new paradigm for trust. As currently implemented, however, distributed ledgers scale poorly and cannot achieve their enormous potential.
In this paper we propose Algorand, an alternative, secure and efficient distributed ledger. Algorand is permissionless and works in a highly asynchronous environment. Unlike prior implementations of distributed ledgers based on ‚Äúproof of work,‚Äù Algorand dispenses with ‚Äúminers‚Äù and requires only a negligible amount of computation. Moreover, its transaction history ‚Äúforks‚Äù only with negligible probability: that is, Algorand guarantees the finality of a transaction the moment the transaction enters the ledger."
2017,"Collusion, efficiency, and dominant strategies.","Abstract
Green and Laffont proved that no collusion-resilient dominant-strategy mechanism, whose strategies consist of individual valuations, guarantees efficiency in multi-unit auctions. Chen and Micali bypassed this impossibility by slightly enlarging the strategy spaces, yet assuming knowledge of the maximum value a player may have for a copy of the good, and the ability of imposing high fines on the players. For unrestricted combinatorial auctions, efficiency in collusion-resilient dominant strategies has remained open, with or without the above two assumptions. We fully generalize the notion of a collusion-resilient dominant-strategy mechanism by allowing for arbitrary strategy spaces; construct one such mechanism for multi-unit auctions, without relying on the above two assumptions; and prove that no such mechanism exists for unrestricted combinatorial auctions, with or without any additional assumptions. Our results hold when the mechanism does not know who colludes with whom, and players in the same coalition can perfectly coordinate their strategies."
2017,Very Simple and Efficient Byzantine Agreement.,"We present a very simple, cryptographic, binary Byzantine-agreement protocol that, with n >= 3t+1 >= 3 players, at most t of which are malicious, halts in expected 9 rounds."
2017,Algorand: Scaling Byzantine Agreements for Cryptocurrencies.,"Algorand is a new cryptocurrency that confirms transactions with latency on the order of a minute while scaling to many users. Algorand ensures that users never have divergent views of confirmed transactions, even if some of the users are malicious and the network is temporarily partitioned. In contrast, existing cryptocurrencies allow for temporary forks and therefore require a long time, on the order of an hour, to confirm transactions with high confidence.

Algorand uses a new Byzantine Agreement (BA) protocol to reach consensus among users on the next set of transactions. To scale the consensus to many users, Algorand uses a novel mechanism based on Verifiable Random Functions that allows users to privately check whether they are selected to participate in the BA to agree on the next set of transactions, and to include a proof of their selection in their network messages. In Algorand's BA protocol, users do not keep any private state except for their private keys, which allows Algorand to replace participants immediately after they send a message. This mitigates targeted attacks on chosen participants after their identity is revealed.

We implement Algorand and evaluate its performance on 1,000 EC2 virtual machines, simulating up to 500,000 users. Experimental results show that Algorand confirms transactions in under a minute, achieves 125x Bitcoin's throughput, and incurs almost no penalty for scaling to more users."
2016,Reconstructing Markov processes from independent and anonymous experiments.,"Abstract
We investigate the problem of exactly reconstructing, with high confidence and up to isomorphism, the ball of radius
centered at the starting state of a Markov process from independent and anonymous experiments. In an anonymous experiment, the states are visited according to the underlying transition probabilities, but no global state names are known: one can only recognize whether two states, reached within the same experiment, are the same.
We prove quite tight bounds for such exact reconstruction in terms of both the number of experiments and their lengths."
2016,Leveraging Possibilistic Beliefs in Unrestricted Combinatorial Auctions.,"In unrestricted combinatorial auctions, we put forward a mechanism that guarantees a meaningful revenue benchmark based on the possibilistic beliefs that the players have about each otherís valuations. In essence, the mechanism guarantees, within a factor of two, the maximum revenue that the ìbest informed playerî would be sure to obtain if he/she were to sell the goods to his/her opponents via take-it-or-leave-it offers. Our mechanism is probabilistic and of an extensive form. It relies on a new solution concept, for analyzing extensive-form games of incomplete information, which assumes only mutual belief of rationality. Moreover, our mechanism enjoys several novel properties with respect to privacy, computation and collusion. View Full-Text"
2016,Mechanisms With Costly Knowledge.,"We propose investigating the design and analysis of game theoretic mechanisms when the players have very unstructured initial knowledge about themselves, but can refine their own knowledge at a cost.

We consider several set-theoretic models of ""costly knowledge"". Specifically, we consider auctions of a single good in which a player i's only knowledge about his own valuation, ?i, is that it lies in a given interval [a,b]. However, the player can pay a cost, depending on a and b (in several ways), and learn a possibly arbitrary but shorter (in several metrics) sub-interval, which is guaranteed to contain ?i.

In light of the set-theoretic uncertainty they face, it is natural for the players to act so as to minimize their regret. As a first step, we analyze the performance of the second-price mechanism in regret-minimizing strategies, and show that, in all our models, it always returns an outcome of very high social welfare."
2016,Auction Revenue in the General Spiteful-Utility Model.,"It is well accepted that, in some auctions, a player's ""true utility"" may depend not only on the price he pays and whether or not he wins the good, but also on various forms of externalities, such as the prices paid by his competitors, and the identity and true value of the actual winner.

In this work, we study revenue generation in single-good auctions under a very general model of externalities: the General Spiteful-Utility Model. Specifically, we

Put forward new revenue benchmarks and solution concepts;
Design new mechanisms when some information about the players' externalities is known; and
Analyze the revenue of the second-price mechanism when only the players have information about each other."
2015,Mechanism design with possibilistic beliefs.,"Abstract
We study mechanism design in non-Bayesian settings of incomplete information, when the designer has no information about the players, and the players have arbitrary, heterogeneous, first-order, and possibilistic beliefs about their opponents' payoff types.
Using such beliefs, in auctions of a single good, we
‚Ä¢
define a revenue benchmark at least as high as the second-highest valuation, and sometimes much higher;
‚Ä¢
prove that it is not meaningfully achievable via traditional notions of implementation; and
‚Ä¢
prove that it is achievable via a notion of implementation based only on mutual belief of rationality."
2015,Better Outcomes from More Rationality.,"Mechanism design enables a social planner to obtain a desired outcome by leveraging the players' rationality and their beliefs. It is thus a fundamental, yet unproven, intuition that the higher the level of rationality of the players, the better the set of obtainable outcomes.

In this paper we prove this fundamental intuition for players with possibilistic beliefs, the traditional model of epistemic game theory. Specifically,

We define a sequence of monotonically increasing revenue benchmarks for single-good auctions, G0 < G1 < G2, where each Gi is defined over the players' beliefs and G0 is the second-highest valuation (i.e., the revenue benchmark achieved by the second-price mechanism).

We (1) construct a single, interim individually rational, auction mechanism that, without any clue about the rationality level of the players, guarantees revenue Gk if all players have rationality levels > k+1, and (2) prove that no such mechanism can guarantee revenue even close to Gk when at least two players are at most level-k rational."
2014,"Cryptography miracles, secure auctions, matching problem verification.","A solution to the persistent problem of preventing collusion in Vickrey auctions.
"
2014,The Query Complexity of Scoring Rules.,"Proper scoring rules are crucial tools to elicit truthful information from experts. A scoring rule maps X, an expert-provided distribution over the set of all possible states of the world, and ?, a realized state of the world, to a real number representing the expertís reward for his provided information. To compute this reward, a scoring rule queries the distribution X at various states. The number of these queries is thus a natural measure of the complexity of the scoring rule.

We prove that any bounded and strictly proper scoring rule that is deterministic must make a number of queries to its input distribution that is a quarter of the number of states of the world. When the state space is very large, this makes the computation of such scoring rules impractical. We also show a new stochastic scoring rule that is bounded, strictly proper, and which makes only two queries to its input distribution. Thus, using randomness allows us to have significant savings when computing scoring rules."
2014,Rational and resilient protocols.,"Cryptography and distributed computation have been very successful in advancing the study of interaction of distinct computing agents. Moreover, both fields have been very successful in conversing with each other, sharing models and techniques. Notably, they both model agents as being either 'good' (i.e., following their prescribed programs) or 'bad' (i.e., deviating from their prescribed program, by stopping, by acting maliciously, or even by coordinating their malicious strategies). I believe however, that we have been neglecting a fundamental ingredient, UTILITY, which has long been recognized and studied by another scientific field, game theory, and in particular by a beautiful subfield of it, mechanism design. Mechanism design aims at obtaining a desired outcome by engineering a game that, rationally played, yields a desired outcome. In such games, multiple players interact very much as in a cryptographic/distributed protocol. But here players are not good or malicious. Rather, every player is RATIONAL, that is, always acts so as to maximize HIS OWN utility.
I believe that properly incorporating utility/rationality in our models will dramatically increase our range of action. Viceversa, mechanism design stands to gain a lot by properly incorporating cryptographic/distributed notions and techniques. In particular, rational players may, by colluding (and making side-payments to one another), increase their utilities. And they too value privacy, which may indeed represent their strategic interests in unforeseen and not yet modeled interactions. Thus, privacy and collusion can disrupt the intended course of an engineered game, and ultimately prevent a desired outcome from being achieved. Mechanism design has been only moderately successful in protecting against collusion, has largely ignored privacy, and might gain precious resiliency by taking into consideration our notions and techniques.
In sum, there is an opportunity for cryptography, distributed computation, and mechanism design to join forces to study more general and accurate models of interaction, and to design more realistic and resilient protocols that simultaneously take into account utility, collusion, and privacy. No sufficiently complex and sufficiently large system, no organism can successfully work or merely sustain its existence without recognizing and harmonizing these basic forces. To be successful, this designing effort will require a good deal of modeling and the development of new conceptual frameworks. It will require open minds and open hearts, so as to leverage past and successful scientific experiences, without being trapped or confined by them. There is the promise of a great deal of fun, challenge, and excitement, and we must recruit as much talent as possible to this effort.
As a concrete, simple, and hopefully provocative example, I will describe a (quite) resilient mechanism, designed by me and Jing Chen, for achieving a (quite) alternative revenue benchmark in unrestricted combinatorial auctions. In such auctions there are multiple distinct goods for sale, each player privately attributes an arbitrary value to any possible subset of the goods, and the seller has no information about the players' valuations."
2014,Knightian self uncertainty in the vcg mechanism for unrestricted combinatorial auctions.,"We study the social welfare performance of the VCG mechanism in the well-known and challenging model of self uncertainty initially put forward by Frank H. Knight and later formalized by Truman F. Bewley. Namely, the only information that each player i has about his own true valuation consists of a set of distributions, from one of which i's valuation has been drawn. We assume that each player knows his true valuation up to an additive inaccuracy ?, and study the social welfare performance of the VCG mechanism relative to ? > 0. Denoting by MSW the maximum social welfare, we have already shown in [Chiesa, Micali and Zhu 2012] that, even in single-good auctions, no mechanism can guarantee any social welfare greater than MSW / n in dominant strategies or ex-post Nash equilibrium strategies, where n is the number of players. In a separate paper [CMZ14], we have proved that for multi-unit auctions, where it coincides with the Vickrey mechanism, the VCG mechanism performs very well in (Knightian) undominated strategies. Namely, in an n-player m-unit auction, the Vickrey mechanism guarantees a social welfare ? - MSW - 2m?, when each Knightian player chooses an arbitrary undominated strategy to bid in the auction. In this paper we focus on the social welfare performance of the VCG mechanism in unrestricted combinatorial auctions, both in undominated strategies and regret-minimizing strategies. (Indeed, both solution concepts naturally extend to the Knightian setting with player self uncertainty.) Our first theorem proves that, in an n-player m-good combinatorial auction, the VCG mechanism may produce outcomes whose social welfare is ? - MSW - ?(2m ?), even when n=2 and each player chooses an undominated strategy. We also geometrically characterize the set of undominated strategies in this setting. Our second theorem shows that the VCG mechanism performs well in regret-minimizing strategies: the guaranteed social welfare is ?-MSW - 2min{m,n}? if each player chooses a pure regret-minimizing strategy, and ?- MSW - O(n≤ ?) if mixed strategies are allowed. Finally, we prove a lemma bridging two standard models of rationality: utility maximization and regret minimization. A special case of our lemma implies that, in any game (Knightian or not), every implementation for regret-minimizing players also applies to utility-maximizing players who use regret ONLY to break ties among their undominated strategies. This bridging lemma thus implies that the VCG mechanism continues to perform very well also for the latter players."
2013,Parametric digital auctions.,"We study revenue maximization for digital auctions, where there are infinitely many copies of a good for sale. There are n buyers, each of whom is interested in obtaining one copy of the good. The buyers' private valuations are drawn from a joint distribution vec{F}. The seller does not know this distribution. The only information that she has are the mean ui and variance ?i2 of each buyer i's marginal distribution Fi. We call such auctions parametric auctions. We construct a deterministic parametric auction that, for a wide class of distributions, guarantees a constant fraction of the optimal revenue achievable when the seller precisely knows the distribution F. Furthermore, our auction is a posted price mechanism and it is maximin optimal among all such mechanisms. That is, it is the posted price mechanism that maximizes revenue in the worst case over an adversarial choice of the distribution."
2013,Super-efficient rational proofs.,"Information asymmetry is a central problem in both computer science and economics. In many fundamental problems, an uninformed principal wants to obtain some knowledge from an untrusted expert. This models several real-world situations, such as a manager's relation with her employees, or the delegation of computational tasks in mechanical turk.

Because the expert is untrusted, the principal needs some guarantee that the provided knowledge is correct. In computer science, this guarantee is usually provided via a proof, which the principal can verify. Thus, a dishonest expert will get caught and penalized (with very high probability). In many economic settings, the guarantee that the knowledge is correct is usually provided via incentives. That is, a game is played between expert and principal such that the expert maximizes her utility by being honest.

A rational proof is an interactive proof where the prover, Merlin, is neither honest nor malicious, but rational. That is, Merlin acts in order to maximize his own utility. Rational proofs have been previously studied when the verifier, Arthur, is a probabilistic polynomial-time machine \cite{AzarMicali}.

In this paper, we study super efficient rational proofs, that is, rational proofs where Arthur runs in logarithmic time. Our new rational proofs are very practical. Not only are they much faster than their classical analogues, but they also provide very tangible incentives for the expert to be honest. Arthur only needs a polynomial-size budget, yet he can penalize Merlin by a large quantity if he deviates from the truth.

We give the following characterizations of which problems admit super-efficient rational proofs.

(1)Uniform TC0 coincides with the set of languages L that admit a rational proof using O(log n) time, O(log n) communication, a constant number of rounds and a polynomial size budget.

P?NPcoincides with the set of languages having a rational proof using O(log n) time, poly(n) communication, one round and a polynomial-size budget.

Furthermore, we show that when Arthur is restricted to have a polynomial-size budget, the set of languages which admit rational proofs with polynomial time verification, polynomial communication and one round is P?MA"
2013,Optimal and Efficient Parametric Auctions.,"Consider a seller who seeks to provide service to a collection of interested parties, subject to feasibility constraints on which parties may be simultaneously served. Assuming that a distribution is known on the value of each party for serviceóarguably a strong assumptionóMyerson's seminal work provides revenue optimizing auctions [12]. We show instead that, for very general feasibility constraints, only knowledge of the median of each party's value distribution, or any other quantile of these distributions, or approximations thereof, suffice for designing simple auctions that simultaneously approximate both the optimal revenue and the optimal welfare. Our results apply to all downward-closed feasibility constraints under the assumption that the underlying, unknown value distributions are monotone hazard rate, and to all matroid feasibility constraints under the weaker assumption of regularity of the underlying distributions. Our results jointly generalize the single-item results obtained by Azar and Micali [2] on parametric auctions, and Daskalakis and Pierrakos [6] for simultaneously approximately optimal and efficient auctions.
"
2012,Collusive dominant-strategy truthfulness.,"Abstract
We show that collusion and wrong beliefs may cause a dramatic efficiency loss in the Vickrey mechanism for auctioning a single good in limited supply. We thus put forward a new mechanism guaranteeing efficiency in a very adversarial collusion model, where the players can partition themselves into arbitrarily many coalitions, exchange money with each other, and perfectly coordinate their actions. Our mechanism bypasses classic impossibility results (such as those of Green and Laffont, and of Schummer) by providing the players with a richer set of strategies, making it dominant for every coalition C to instruct each of its members to report truthfully not only his own valuation, but also his belonging to C. Our mechanism is coalitionally rational, which implies being individually rational for independent players."
2012,Mechanism design with approximate valuations.,"We study single-good auctions when each player knows his own valuation only within a constant multiplicative factor ? ? (0, 1), known to the mechanism designer. The classical notions of implementation in dominant strategies and implementation in undominated strategies are naturally extended to this setting, but their power is vastly different.

On the negative side, we prove that no dominant-strategy mechanism can guarantee social welfare that is significantly better than that achievable by assigning the good to a random player.

On the positive side, we provide tight upper and lower bounds for the fraction of the maximum social welfare achievable in undominated strategies, whether deterministically or probabilistically."
2012,Crowdsourced Bayesian auctions.,"We investigate the problem of optimal mechanism design, where an auctioneer wants to sell a set of goods to buyers, in order to maximize revenue. In a Bayesian setting the buyers' valuations for the goods are drawn from a prior distribution D, which is often assumed to be known by the seller. In this work, we focus on cases where the seller has no knowledge at all, and ""the buyers know each other better than the seller knows them"". In our model, D is not necessarily common knowledge. Instead, each buyer individually knows a posterior distribution associated with D. Since the seller relies on the buyers' knowledge to help him set a price, we call these types of auctions crowdsourced Bayesian auctions.

For this crowdsourced Bayesian model and many environments of interest, we show that, for arbitrary valuation distributions D (in particular, correlated ones), it is possible to design mechanisms matching to a significant extent the performance of the optimal dominant-strategy-truthful mechanisms where the seller knows D.

To obtain our results, we use two techniques: (1) proper scoring rules to elicit information from the players; and (2) a reverse version of the classical Bulow-Klemperer inequality. The first lets us build mechanisms with a unique equilibrium and good revenue guarantees, even when the players' second and higher-order beliefs about each other are wrong. The second allows us to upper bound the revenue of an optimal mechanism with n players by an n/n--1 fraction of the revenue of the optimal mechanism with n -- 1 players. We believe that both techniques are new to Bayesian optimal auctions and of independent interest for future work."
2012,Rational proofs.,"We study a new type of proof system, where an unbounded prover and a polynomial time verifier interact, on inputs a string x and a function f, so that the Verifier may learn f(x). The novelty of our setting is that there no longer are ""good"" or ""malicious"" provers, but only rational ones. In essence, the Verifier has a budget c and gives the Prover a reward r ? [0,c] determined by the transcript of their interaction; the prover wishes to maximize his expected reward; and his reward is maximized only if he the verifier correctly learns f(x). Rational proof systems are as powerful as their classical counterparts for polynomially many rounds of interaction, but are much more powerful when we only allow a constant number of rounds. Indeed, we prove that if f ? #P, then f is computable by a one-round rational Merlin-Arthur game, where, on input x, Merlin's single message actually consists of sending just the value f(x). Further, we prove that CH, the counting hierarchy, coincides with the class of languages computable by a constant-round rational Merlin-Arthur game. Our results rely on a basic and crucial connection between rational proof systems and proper scoring rules, a tool developed to elicit truthful information from experts."
2011,Perfect implementation.,"Abstract
Privacy and trust affect our strategic thinking, yet have not been precisely modeled in mechanism design. In settings of incomplete information, traditional implementations of a normal-form mechanism‚Äîby disregarding the players' privacy, or assuming trust in a mediator‚Äîmay fail to reach the mechanism's objectives. We thus investigate implementations of a new type.
We put forward the notion of a perfect implementation of a normal-form mechanism
: in essence, a concrete extensive-form mechanism exactly preserving all strategic properties of
, without relying on trusted mediators or violating the players' privacy.
We prove that any normal-form mechanism can be perfectly implemented by a verifiable mediator using envelopes and an envelope-randomizing device. Differently from a trusted mediator, a verifiable one only performs prescribed public actions, so that everyone can verify that he is acting properly, and that he never learns any information that should remain private."
2011,Crowdsourced Bayesian Auctions - (Abstract).,"Abstract
We investigate the problem of optimal mechanism design, where an auctioneer wants to sell a set of goods to buyers, in order to maximize revenue. In a Bayesian setting the buyers‚Äô valuations for the goods are drawn from a prior distribution
D
D
, which is often assumed to be known by the seller. In this work, we focus on cases where the seller has no knowledge at all, and ‚Äúthe buyers know each other better than the seller knows them‚Äù. In our model,
D
D
is not necessarily common knowledge. Instead, each player individually knows a posterior distribution associated with
D
D
. Since the seller relies on the buyers‚Äô knowledge to help him set a price, we call these types of auctions crowdsourced Bayesian auctions.
For this crowdsourced Bayesian model and many environments of interest, we show that, for arbitrary type distributions
D
D
(in particular, correlated ones), it is possible to design mechanisms matching to a significant extent the performance of the optimal classical mechanisms where the seller knows
D
D
. Our results are ‚Äúexistential‚Äù for a broad class of environments (including combinatorial auctions) and ‚Äúconstructive‚Äù for auctions of a single good.
To obtain our results, we use two techniques: (1) Proper scoring rules to elicit information from the bidders; and (2) a quantitative version of the classical Bulow-Klemperer inequality. The first technique lets us build mechanisms that guarantee good revenue, even when the players‚Äô second and higher-order beliefs about each other are wrong. The second allows us to upper bound the revenue of an optimal mechanism with n players by a
(1‚àí
1
n
)
(
fraction of the revenue of the optimal mechanism with n‚Äâ‚àí‚Äâ1 players. We believe that both techniques are new to Bayesian optimal auctions and of independent interest for future work."
2011,Mechanism Design with Set-Theoretic Beliefs.,"Abstract:
In settings of incomplete information, we put forward (1) a very conservative -- indeed, purely set-theoretic -- model of the beliefs (including totally wrong ones) that each player may have about the payoff types of his opponents, and (2) a new and robust solution concept, based on mutual belief of rationality, capable of leveraging such conservative beliefs. We exemplify the applicability of our new approach for single-good auctions, by showing that, under our solution concept, a normal-form, simple, and deterministic mechanism guarantees -- up to an arbitrarily small, additive constant -- a revenue benchmark that is always greater than or equal to the second-highest valuation, and sometimes much greater. By contrast, we also prove that the same benchmark cannot even be approximated within any positive factor, under classical solution concepts."
2010,Optimal Error Correction for Computationally Bounded Noise.,"Abstract:
For adversarial but computationally bounded models of error, we construct appealingly simple and efficient cryptographic encoding and unique decoding schemes whose error-correction capability is much greater than classically possible. In particular: 1) For binary alphabets, we construct positive-rate coding schemes that are uniquely decodable under a 1/2 - Œ≥ error rate for any constant Œ≥ > 0. 2) For large alphabets, we construct coding schemes that are uniquely decodable under a 1 - R error rate for any information rate R > 0. Our results for large alphabets are actually optimal, since the ""computationally bounded but adversarial channel"" can simulate the behavior of the q-ary symmetric channel, where q denotes the size of the alphabet, the capacity of which is known to be upper-bounded by 1 - R. Our results hold under minimal assumptions on the communication infrastructure, namely: 1) we allow the channel to be more powerful than the receiver and 2) we only assume that some information about the sender-a public key-is known. (In particular, we do not require any shared secret key or joint local state between sender and receivers)."
2010,Perfect concrete implementation of arbitrary mechanisms: a quick summary of joint work with Sergei Izmalkov and Matt Lepinski.,"Privacy and trust affect our everyday thinking and, in particular, the way we approach a concrete game. Accordingly, we hope that a rigorous treatment of privacy and trust will become integral part of mechanism design. As of now, the field has been very successful in finding many ingenious mechanisms as solutions to a variety of problems. But these mechanisms are theoretical constructions and not enough attention has been devoted to their concrete implementation. Indeed, It should be appreciated that the outcome function of a simple normal-form mechanism does not spontaneously evaluate itself on the ""messages"" that the players have selected in ""their own minds."" To be practically useful in a real strategic setting, any mechanism M, whether of normal or extensive form, must be concretely implemented. But then, in such concrete implementations, issues of privacy and trust may arise so as to undermine the valuable theoretical properties of M.
"
2010,Robustly Leveraging Collusion in Combinatorial Auctions.,"Because of its devastating effects in auctions and other mechanisms, collusion is prohibited and legally prosecuted. Yet, colluders have always existed, and may continue to exist. We thus raise the following question for mechanism design: What desiderata are achievable, and by what type of mechanisms, when any set of players who wish to collude are free to do so without any restrictions on the way in which they cooperate and coordinate their actions?
In response to this question we put forward and exemplify the notion of a collusion-leveraging mechanism. In essence, this is a mechanism aligning its desiderata with the incentives of all its players, including colluders, to a significant and mutually beneficial extent. Of course such mechanisms may exist only for suitable desiderata. In unrestricted combinatorial auctions, where classical mechanisms essentially guarantee 0 social welfare and 0 revenue in the presence of just two colluders, we prove that it is possible for collusion-leveraging mechanisms to guarantee that the sum of social welfare and revenue is always high, even when all players are collusive. To guarantee better performance, collusion-leveraging mechanisms in essence ""welcome"" collusive players, rather than pretending they do not exist, raising a host of new questions at the intersection of cooperative and non-cooperative game theory."
2010,Robust Perfect Revenue From Perfectly Informed Players.,"Maximizing revenue in the presence of perfectly informed players is a well known goal in mechanism design. Yet, all current mechanisms for this goal are vulnerable to equilibrium selection, collusion, privacy and complexity problems, and therefore far from guaranteeing that maximum revenue will be obtained. In this paper we both clarify and rectify this situation by
  ïProving that no weakly dominant-strategy mechanism (traditionally considered immune to equilibrium selection) guarantees an arbitrarily small fraction of the maximum possible revenue; and, more importantly,
  ïConstructing a new mechanism, of extensive-form and with a unique sub-game-perfect equilibrium, which
(a) guarantees a fraction arbitrarily close to 1 of the maximum possible revenue;
(b) is provably robust against equilibrium selection, collusion, complexity, and privacy problems; and
(c) works for any number of players n > 1, and without relying on special conditions for the players utilities."
2009,A new approach to auctions and resilient mechanism design.,"We put forward a new approach to mechanism design, and exemplify it via a new mechanism guaranteeing significant revenue in unrestricted combinatorial auctions. Our mechanism (1) succeeds in a new and very adversarial collusion model; (2) works in a new, equilibrium-less, and very strong solution concept; (3) benchmarks its performance against the knowledge that the players have about each other; (4) is computationally efficient and preserves the players' privacy to an unusual extent. "
2009,Purely Rational Secret Sharing (Extended Abstract).,"Abstract
Rational secret sharing is a problem at the intersection of cryptography and game theory. In essence, a dealer wishes to engineer a communication game that, when rationally played, guarantees that each of the players learns the dealer‚Äôs secret. Yet, all solutions proposed so far did not rely solely on the players‚Äô rationality, but also on their beliefs, and were also quite inefficient.
After providing a more complete definition of the problem, we exhibit a very efficient and purely rational solution to it with a verifiable trusted channel."
2008,Online-Untransferable Signatures.,"Abstract
Non-transferability of digital signatures is an important security concern, traditionally achieved via interactive verification protocols. Such protocols, however, are vulnerable to ‚Äúonline transfer attacks‚Äù ‚Äî i.e., attacks mounted during the protocols‚Äô executions.
In this paper, we show how to guarantee online untransferability of signatures, via a reasonable public-key infrastructure and general assumptions, without random oracles. Our untransferable signatures are as efficient as prior ones that provably provide weaker types of untransferability."
2008,Verifiably Secure Devices.,"Abstract
We put forward the notion of a verifiably secure device, in essence a stronger notion of secure computation, and achieve it in the ballot-box model. Verifiably secure devices
1
Provide a perfect solution to the problem of achieving correlated equilibrium, an important and extensively investigated problem at the intersection of game theory, cryptography and efficient algorithms; and
 1
Enable the secure evaluation of multiple interdependent functions.
 "
2006,Input-Indistinguishable Computation.,"Abstract:
We put forward a first definition of general secure computation that, without any trusted set-up, handles an arbitrary number of concurrent executions; and is implementable based on standard complexity assumptions. In contrast to previous definitions of secure computation, ours is not simulation-based"
2006,Independent Zero-Knowledge Sets.,"Abstract
We define and construct Independent Zero-Knowledge Sets (ZKS) protocols. In a ZKS protocols, a Prover commits to a set S, and for any x, proves non-interactively to a Verifier if x ‚ààS or x ‚àâS without revealing any other information about S. In the independent ZKS protocols we introduce, the adversary is prevented from successfully correlate her set to the one of a honest prover. Our notion of independence in particular implies that the resulting ZKS protocol is non-malleable.
On the way to this result we define the notion of independence for commitment schemes. It is shown that this notion implies non-malleability, and we argue that this new notion has the potential to simplify the design and security proof of non-malleable commitment schemes.
Efficient implementations of ZKS protocols are based on the notion of mercurial commitments. Our efficient constructions of independent ZKS protocols requires the design of new commitment schemes that are simultaneously independent (and thus non-malleable) and mercurial."
2006,Local zero knowledge.,"We put forward the notion of Local Zero Knowledge and provide its first implementations in a variety of settings under standard complexity assumptions.Whereas the classical notion of Zero Knowledge guarantees the secrecy only of information that is hard to compute, the new one meaningfully guarantees the secrecy of any information (in case of perfect zero-knowledge, and asymptotically in all other cases). Consequently, Local Zero Knowledge remains very meaningful even if DP = NP.
"
2005,Rational Secure Computation and Ideal Mechanism Design.,"Abstract:
Secure computation essentially guarantees that whatever computation n players can do with the help of a trusted party, they can also do by themselves. Fundamentally, however, this notion depends on the honesty of at least some players. We put forward and implement a stronger notion, rational secure computation, that does not depend on player honesty, but solely on player rationality. The key to our implementation is showing that the ballot-box - the venerable device used throughout the world to tally secret votes securely - can actually be used to securely compute any function. Our work bridges the fields of game theory and cryptography, and has broad implications for mechanism design."
2005,Collusion-free protocols.,"Secure protocols attempt to minimize the injuries to privacy and correctness inflicted by malicious participants who collude during run-time. They do not, however, prevent malicious parties from colluding and coordinating their actions in the first place!Eliminating such collusion of malicious parties during the execution of a protocol is an important and exciting direction for research in Cryptography. We contribute the first general result in this direction: (1) We provide a rigorous definition of what a collusion-free protocol is; and (2) We prove that, under standard physical and computational assumptions ---i.e., plain envelopes and trapdoor permutations---collusion-free protocols exist for all finite protocol tasks with publicly observable actions. (Note that such tasks are allowed to have secret global state, and thus include Poker, Bridge, and other such games.Our solution is tight in the sense that, for a collusion-free protocol to exist, each of (a) the finiteness of the game of interest, (b) the public observability of its actions, and (c) the use of some type of physically private channel is provably essential.
"
2005,Optimal Error Correction Against Computationally Bounded Noise.,"Abstract
For computationally bounded adversarial models of error, we construct appealingly simple, efficient, cryptographic encoding and unique decoding schemes whose error-correction capability is much greater than classically possible. In particular:
1
For binary alphabets, we construct positive-rate coding schemes which are uniquely decodable from a 1/2 ‚Äì Œ≥ error rate for any constant Œ≥> 0.
 2
For large alphabets, we construct coding schemes which are uniquely decodable from a
1‚àí
R
‚àí
‚àí
‚àö
1
error rate for any information rate R> 0.
  Our results are qualitatively stronger than related work: the construction works in the public-key model (requiring no shared secret key or joint local state) and allows the channel to know everything that the receiver knows. In addition, our techniques can potentially be used to construct coding schemes that have information rates approaching the Shannon limit. Finally, our construction is qualitatively optimal: we show that unique decoding under high error rates is impossible in several natural relaxations of our model."
2005,Fair-Zero Knowledge.,"Abstract
We introduce Fair Zero-Knowledge, a multi-verifier ZK system where every proof is guaranteed to be ‚Äúzero-knowledge for all verifiers.‚Äù That is, if an honest verifier accepts a fair zero-knowledge proof, then he is assured that all other verifiers also learn nothing more than the verity of the statement in question, even if they maliciously collude with a cheating prover.
We construct Fair Zero-Knowledge systems based on standard complexity assumptions (specifically, the quadratic residuosity assumption) and an initial, one-time use of a physically secure communication channel (specifically, each verifier sends the prover a private message in an envelope). All other communication occurs (and must occur) on a broadcast channel.
The main technical challenge of our construction consists of provably removing any possibility of using steganography in a ZK proof. To overcome this technical difficulty, we introduce tools ‚Äîsuch as Unique Zero Knowledge‚Äî that may be of independent interest."
2004,Sequential Aggregate Signatures from Trapdoor Permutations.,"Abstract
An aggregate signature scheme (recently proposed by Boneh, Gentry, Lynn, and Shacham) is a method for combining n signatures from n different signers on n different messages into one signature of unit length. We propose sequential aggregate signatures, in which the set of signers is ordered. The aggregate signature is computed by having each signer, in turn, add his signature to it. We show how to realize this in such a way that the size of the aggregate signature is independent of n. This makes sequential aggregate signatures a natural primitive for certificate chains, whose length can be reduced by aggregating all signatures in a chain. We give a construction in the random oracle model based on families of certified trapdoor permutations, and show how to instantiate our scheme based on RSA."
2004,Completely fair SFE and coalition-safe cheap talk.,"Secure function evaluation (SFE) enables a group of players, by themselves, to evaluate a function on private inputs as securely as if a trusted third party had done it for them. A completely fair SFE is a protocol in which, conceptually, the function values are learned atomically.We provide a completely fair SFE protocol which is secure for any number of malicious players, using a novel combination of computational and physical channel assumptions.We also show how completely fair SFE has striking applications togame theory. In particular, it enables ""cheap-talk"" protocol that

(a) achieve correlated-equilibrium payoffs in any game,
(b) are the first protocols which provably give no additional power to any coalition of players, and
(c) are exponentially more efficient than prior counterparts."
2004,Algorithmic Tamper-Proof (ATP) Security: Theoretical Foundations for Security against Hardware Tampering.,"Abstract
Traditionally, secure cryptographic algorithms provide security against an adversary who has only black-box access to the secret information of honest parties. However, such models are not always adequate. In particular, the security of these algorithms may completely break under (feasible) attacks that tamper with the secret key.
In this paper we propose a theoretical framework to investigate the algorithmic aspects related to tamper-proof security. In particular, we define a model of security against an adversary who is allowed to apply arbitrary feasible functions f to the secret key sk, and obtain the result of the cryptographic algorithms using the new secret key f(sk).
We prove that in the most general setting it is impossible to achieve this strong notion of security. We then show minimal additions to the model, which are needed in order to obtain provable security.
We prove that these additions are necessary and also sufficient for most common cryptographic primitives, such as encryption and signature schemes.
We discuss the applications to portable devices protected by PINs and show how to integrate PIN security into the generic security design.
Finally we investigate restrictions of the model in which the tampering powers of the adversary are limited. These restrictions model realistic attacks (like differential fault analysis) that have been demonstrated in practice. In these settings we show security solutions that work even without the additions mentioned above."
2004,Physically Observable Cryptography (Extended Abstract).,"Abstract
Complexity-theoretic cryptography considers only abstract notions of computation, and hence cannot protect against attacks that exploit the information leakage (via electromagnetic fields, power consumption, etc.) inherent in the physical execution of any cryptographic algorithm. Such ‚Äúphysical observation attacks‚Äù bypass the impressive barrier of mathematical security erected so far, and successfully break mathematically impregnable systems. The great practicality and the inherent availability of physical attacks threaten the very relevance of complexity-theoretic security.
To respond to the present crisis, we put forward physically observable cryptography: a powerful, comprehensive, and precise model for defining and delivering cryptographic security against an adversary that has access to information leaked from the physical execution of cryptographic algorithms. Our general model allows for a variety of adversaries. In this paper, however, we focus on the strongest possible adversary, so as to capture what is cryptographically possible in the worst possible, physically observable setting. In particular, we
consider an adversary that has full (and indeed adaptive) access to any leaked information;
show that some of the basic theorems and intuitions of traditional cryptography no longer hold in a physically observable setting; and
construct pseudorandom generators that are provably secure against all physical-observation attacks.
Our model makes it easy to meaningfully restrict the power of our general physically observing adversary. Such restrictions may enable schemes that are more efficient or rely on weaker assumptions, while retaining security against meaningful physical observations attacks."
2003,Plaintext Awareness via Key Registration.,"Abstract
In this paper, we reconsider the notion of plaintext awareness. We present a new model for plaintext-aware encryption that is both natural and useful. We achieve plaintext-aware encryption without random oracles by using a third party. However, we do not need to trust the third party: even when the third party is dishonest, we still guarantee security against adaptive chosen ciphertext attacks. We show a construction that achieves this definition under general assumptions. We further motivate this achievement by showing an important and natural application: giving additional real-world meaningfulness to the Dolev-Yao model."
2003,Fractal Merkle Tree Representation and Traversal.,"Abstract
We introduce a technique for traversal of Merkle trees, and propose an efficient algorithm that generates a sequence of leaves along with their associated authentication paths. For one choice of parameters, and a total of N leaves, our technique requires a worst-case computational effort of 2 logN/loglog N hash function evaluations per output, and a total storage capacity of less than 1.5 log2 N/loglogN hash values. This is a simultaneous improvement both in space and time complexity over any previously published algorithm."
2003,Zero-Knowledge Sets.,"Abstract:
We show how a polynomial-time prover can commit to an arbitrary finite set S of strings so that, later on, he can, for any string x, reveal with a proof whether x /spl isin/ S or x /spl notin/ S, without revealing any knowledge beyond the verity of these membership assertions. Our method is non interactive. Given a public random string, the prover commits to a set by simply posting a short and easily computable message. After that, each time it wants to prove whether a given element is in the set, it simply posts another short and easily computable proof, whose correctness can be verified by any one against the public random string. Our scheme is very efficient; no reasonable prior way to achieve our desiderata existed. Our new primitive immediately extends to providing zero-knowledge databases."
2003,Simple and fast optimistic protocols for fair electronic exchange.,"Assume each of two parties has something the other wants. Then, a fair exchange is an electronic protocol guaranteeing that either both parties get what they want, or none of them does. Protocols relying on traditional trusted parties easily guarantee such exchanges, but are inefficient (because a trusted party must be part of every execution) and expensive (because trusted parties want to be paid for each execution).In this paper weï Quickly review fair exchanges that are optimistic, that is, relying on a trusted party that (1) does not participate at all in an honest execution, and yet (2) guarantees the fairness of all executions; andï Present some older and surprisingly simple optimistic protocols for fair certified e-mail and contract signing that never appeared in the scientific literature.
"
2002,Improving the Exact Security of Digital Signature Schemes.,"Abstract
We put forward a new method of constructing Fiat-Shamir-like signature schemes that yields better ‚Äúexact security‚Äù than the original Fiat-Shamir method. (We also point out, however, that such tight security does not make our modified schemes always preferable to the original ones. Indeed, there exist particularly efficient Fiat-Shamir-like schemes that, though only enjoying ‚Äúloose security,‚Äù by using longer keys may provably provide more security at a lower computational cost than their ‚Äútight-security‚Äù counterparts.)"
2002,Micropayments Revisited.,"Abstract
We present new micropayment schemes that are more efficient and user friendly than previous ones.
These schemes reduce bank processing costs by several orders of magnitude, while preserving a simple interface for both users and merchants. The schemes utilize a probabilistic deposit protocol that, in some of the schemes, may be entirely hidden from the users."
2002,Transitive Signature Schemes.,"Abstract
We introduce and provide the first example of a transitive digital signature scheme. Informally, this is a way to digitally sign vertices and edges of a dynamically growing, transitively closed, graph G so as to guarantee the following properties:
Given the signatures of edges (u, v) and (v,w), anyone can easily derive the digital signature of the edge (u,w).
It is computationaly hard for any adversary to forge the digital signature of any new vertex or other edge of G, even if he can request the legitimate signer to digitally sign any number of G‚Äôs vertices and edges of his choice in an adaptive fashion (i.e., even if he can choose which vertices and edges the legitimate signer should sign next after he sees the legitimate signatures of the ones requested so far)."
2001,Mutually Independent Commitments.,"Abstract
We study the two-party commitment problem, where two players have secret values they wish to commit to each other. Traditional commitment schemes cannot be used here because they do not guarantee independence of the committed values. We present three increasingly strong definitions of independence in this setting and give practical protocols for each. Our work is related to work in non-malleable cryptography. However, the two-party commitment problem can be solved much more efficiently than by using non-malleability techniques."
2001,Accountable-subgroup multisignatures: extended abstract.,"Formal models and security proofs are especially important for multisignatures: in contrast to threshold signatures, no precise definitions were ever provided for such schemes, and some proposals were subsequently broken.In this paper, we formalize and implement a variant of multi-signature schemes, Accountable-Subgroup Multisignatures (ASM). In essence, ASM schemes enable any subgroup, S, of a given group, G, of potential signers, to sign efficiently a message M so that the signature provably reveals the identities of the signers in S to any verifier.Specifically, we provide:The first formal model of security for multisignature schemes that explicitly includes key generation (without relying on trusted third parties);A protocol, based on Schnorr's signature scheme [33], that is both provable and efficient:Only three rounds of communication are required per signature.The signing time per signer is the same as for the single-signer Schnorr scheme, regardless of the number of signers.The verification time is only slightly greater than that for the single-signer Schnorr scheme.The signature length is the same as for the single signer Schnorr scheme, regardless of the number of signers.Our proof of security relies on random oracles and the hardness of the Discrete Log Problem."
2001,Soundness in the Public-Key Model.,"Abstract
The public-key model for interactive proofs has proved to be quite effective in improving protocol efficiency [CGGM00]. We argue, however, that its soundness notion is more subtle and complex than in the classical model, and that it should be better understood to avoid designing erroneous protocols. Specifically, for the public-key model, we
identify four meaningful notions of soundness;
prove that, under minimal complexity assumptions, these four notions are distinct;
identify the exact soundness notions satisfied by prior interactive protocols; and
identify the round complexity of some of the new notions."
2001,Min-round Resettable Zero-Knowledge in the Public-Key Model.,"Abstract
In STOC 2000, Canetti, Goldreich, Goldwasser, and Micali put forward the strongest notion of zero-knowledge to date, resettable zero-knowledge (RZK) and implemented it in constant rounds in a new model, where the verifier simply has a public key registered before any interaction with the prover.
To achieve ultimate round efficiency, we advocate a slightly stronger model. Informally, we show that, as long as the honest verifier does not use a given public key more than a fixed-polynomial number of times, there exist 3-round (which we prove optimal) RZK protocols for all of NP."
2001,Identification Protocols Secure against Reset Attacks.,"Abstract
We provide identification protocols that are secure even when the adversary can reset the internal state and/or randomization source of the user identifying itself, and when executed in an asynchronous environment like the Internet that gives the adversary concurrent access to instances of the user. These protocols are suitable for use by devices (like smartcards) which when under adversary control may not be able to reliably maintain their internal state between invocations."
2001,Amortized E-Cash.,"Abstract
We present an e-cash scheme which provides a trade-off between anonymity and efficiency, by amortizing the cost of zero-knowledge and signature computation in the cash generation phase.
Our work solves an open problem of Okamoto in divisible e-cash. Namely, we achieve results similar to those of Okamoto, but (1) based on traditional complexity assumptions (rather than ad hoc ones), and (2) within a much crisper definitional framework that highlights the anonymity properties, and (3) in a simple fashion."
2000,Reducibility and Completeness in Private Computations.,"We define the notions of reducibility and completeness in (two-party and multiparty) private computations. Let g be an n-argument function. We say that a function f is reducible to a function g if n honest-but-curious players can compute the function fn -privately, given a black box for g (for which they secretly give inputs and get the result of operating g on these inputs). We say that g is complete (for private computations) if every function f is reducible to g.

In this paper, we characterize the complete boolean functions: we show that a boolean function g is complete if and only if g itself cannot be computed n-privately (when there is no black box available). Namely, for n-argument boolean functions, the notions of completeness and n-privacy are complementary. This characterization provides a huge collection of complete functions any nonprivate boolean function!) compared to very few examples that were given (implicitly) in previous work. On the other hand, for nonboolean functions, we show that these two notions are not complementary.
"
2000,Computationally Sound Proofs.,"This paper puts forward a new notion of a proof based on computational complexity and explores its implications for computation at large.

Computationally sound proofs provide, in a novel and meaningful framework, answers to old and new questions in complexity theory. In particular, given a random oracle or a new complexity assumption, they enable us to prove that verifying is easier than deciding for all theorems; provide a quite effective way to prove membership in computationally hard languages (such as ${\cal C}o$-$\cal N \cal P$-complete ones); and show that every computation possesses a short certificate vouching its correctness.

Finally, if a special type of computationally sound proof exists, we show that Blum's notion of program checking can be meaningfully broadened so as to prove that $\cal N \cal P$-complete languages are checkable.
"
2000,Parallel Reducibility for Information-Theoretically Secure Computation.,"Abstract
Secure Function Evaluation (SFE) protocols are very hard to design, and reducibility has been recognized as a highly desirable property of SFE protocols. Informally speaking, reducibility (sometimes called modular composition) is the automatic ability to break up the design of complex SFE protocols into several simpler, individually secure components. Despite much effort, only the most basic type of reducibility, sequential reducibility (where only a single sub-protocol can be run at a time), has been considered and proven to hold for a specific class of SFE protocols. Unfortunately, sequential reducibility does not allow one to save on the number of rounds (often the most expensive resource in a distributed setting), and achieving more general notions is not easy (indeed, certain SFE notions provably enjoy sequential reducibility, but fail to enjoy more general ones).
In this paper, for information-theoretic SFE protocols, we
Formalize the notion of parallel reducibility, where sub-protocols can be run at the same time;
Clarify that there are two distinct forms of parallel reducibility:
Concurrent reducibility, which applies when the order of the sub-protocol calls is not important (and which reduces the round complexity dramatically as compared to sequential reducibility); and
Synchronous reducibility, which applies when the sub-protocols must be executed simultaneously (and which allows modular design in settings where sequential reducibility does not even apply).
Show that a large class of SFE protocols (i.e., those satisfying a slight modification of the original definition of Micali and Rogaway [15]) provably enjoy (both forms of) parallel reducibility."
2000,Public-Key Encryption in a Multi-user Setting: Security Proofs and Improvements.,"Abstract
This paper addresses the security of public-key cryptosystems in a ‚Äúmulti-user‚Äù setting, namely in the presence of attacks involving the encryption of related messages under different public keys, as exemplified by H√•stad‚Äôs classical attacks on RSA. We prove that security in the single-user setting implies security in the multi-user setting as long as the former is interpreted in the strong sense of ‚Äúindistinguishability,‚Äù thereby pin-pointing many schemes guaranteed to be secure against H√•stad-type attacks. We then highlight the importance, in practice, of considering and improving the concrete security of the general reduction, and present such improvements for two Diffie-Hellman based schemes, namely El Gamal and Cramer-Shoup."
2000,Resettable zero-knowledge (extended abstract).,"We introduce the notion of Resettable Zero-Knowledge (rZK), a new security measure for cryptographic protocols which strengthens the classical notion of zero-knowledge. In essence, an rZK protocol is one that remains zero knowledge even if an adversary can interact with the prover many times, each time resetting the prover to its initial state and forcing it to use the same random tape. All known examples of zero-knowledge proofs and arguments are trivially breakable in this setting. Moreover, by definition, all zero-knowledge proofs of knowledge are breakable in this setting. Under general complexity assumptions, which hold for example if the Discrete Logarithm Problem is hard, we construct: ï Resettable Zero-Knowledge proof-systems for NP with non-constant number of rounds. * Five-round Resettable Witness-Indistinguishable proofsystems for NP. e Four-round Resettabie Zero-Knowledge arguments for NP in the public key model: where verifiers have fixed, public keys associated with them. In addition to shedding new light on what makes zero knowledge possible (by constructing ZK protocols that use randomness in a dramatically weaker way than before), rZK has great relevance to applications. Firstly, rZK protocols are closed under parallel and concurrent execution and thus are guaranteed to be secure when implemented in fully asynchronous networks, even if an adversary schedules the arrival of every message sent so as to foil security. Secondly, rZK protocols enlarge the range of physical ways in which provers of ZK protocols can be securely implemented, including devices which cannot reliably toss coins on line, nor keep state "
1999,Improving the Exact Security of Fiat-Shamir Signature Schemes.,"Abstract
We provide two contributions to exact security analysis of digital signatures: 1. We put forward a new method of constructing Fiat-Shamir-like signature schemes that yields better ‚Äúexact security‚Äù than the original Fiat-Shamir method; and 2. We extend exact security analysis to exact cost-security analysis by showing that digital signature schemes with ‚Äúloose security‚Äù may be preferable for reasonable measures of cost."
1999,The All-or-Nothing Nature of Two-Party Secure Computation.,"Abstract
A function f is computationally securely computable if two computationally-bounded parties Alice, having a secret input x, and Bob, having a secret input y, can talk back and forth so that (even if one of them is malicious) (1) Bob learns essentially only f(x,y) while (2) Alice learns essentially nothing.
We prove that, if any non-trivial function can be so computed, then so can every function. Consequently, the complexity assumptions sufficient and/or required for computationally securely computing f are the same for every non-trivial function f."
1999,Lower Bounds for Oblivious Transfer Reductions.,"Abstract
We prove the first general and non-trivial lower bound for the number of times a 1-out-of-n Oblivious Transfer of strings of length l should be invoked so as to obtain, by an information-theoretically secure reduction, a 1-out-of-N Oblivious Transfer of strings of length L. Our bound is tight in many significant cases.
We also prove the first non-trivial lower bound for the number of random bits needed to implement such a reduction whenever the receiver sends no messages to the sender. This bound is also tight in many significant cases."
1999,Computationally Private Information Retrieval with Polylogarithmic Communication.,"Abstract
We present a single-database computationally private information retrieval scheme with polylogarithmic communication complexity. Our construction is based on a new, but reasonable intractability assumption, which we call the Œ¶-Hiding Assumption (Œ¶HA): essentially the difficulty of deciding whether a small prime divides Œ¶(m), where m is a composite integer of unknown factorization."
1999,Verifiable Random Functions.,"Abstract:
We efficiently combine unpredictability and verifiability by extending the Goldreich-Goldwasser-Micali (1986) construction of pseudorandom functions f/sub s/ from a secret seed s, so that knowledge of s not only enables one to evaluate f/sub s/ at any point x, but also to provide an NP-proof that the value f/sub s/(x) is indeed correct without compromising the unpredictability of f/sub s/ at any other point for which no such a proof was provided."
1998,Computationally-Sound Checkers.,"Abstract
We show that CS proofs have important implications for validating one-sided heuristics for NP. Namely, generalizing a prior notion of Blum's, we put forward the notion of a CS checker and show that special-type of CS proofs imply CS checkers for NP-complete languages."
1997,An Optimal Probabilistic Protocol for Synchronous Byzantine Agreement.,"Broadcasting guarantees the recipient of a message that everyone else has received the same message. This guarantee no longer exists in a setting in which all communication is person-to-person and some of the people involved are untrustworthy: though he may claim to send the same message to everyone, an untrustworthy sender may send different messages to different people. In such a setting, Byzantine agreement offers the ""best alternative"" to broadcasting. Thus far, however, reaching Byzantine agreement has required either many rounds of communication (i.e., messages had to be sent back and forth a number of times that grew with the size of the network) or the help of some external trusted party.

In this paper, for the standard communication model of synchronous networks in which each pair of processors is connected by a private communication line, we exhibit a protocol that, in probabilistic polynomial time and without relying on any external trusted party, reaches Byzantine agreement in an expected constant number of rounds and in the worst natural fault model. In fact, our protocol successfully tolerates that up to 1/3 of the processors in the network may deviate from their prescribed instructions in an arbitrary way, cooperate with each other, and perform arbitrarily long computations.

Our protocol effectively demonstrates the power of randomization and zero-knowledge computation against errors. Indeed, it proves that ""privacy"" (a fundamental ingredient of one of our primitives), even when is not a desired goal in itself (as for the Byzantine agreement problem), can be a crucial tool for achieving correctness.

Our protocol also introduces three new primitives---graded broadcast, graded verifiable secret sharing, and oblivious common coin---that are of independent interest, and may be effectively used in more practical protocols than ours.
"
1996,On-Line/Off-Line Digital Signatures.,"A new type of signature scheme is proposed. It consists of two phases. The first phase is performed off-line, before the message to be signed is even known. The second phase is performed on-line, once the message to be signed is known, and is supposed to be very fast. A method for constructing such on-line/off-line signature schemes is presented. The method uses one-time signature schemes, which are very fast, for the on-line signing. An ordinary signature scheme is used for the off-line stage.

In a practical implementation of our scheme, we use a variant of Rabin's signature scheme (based on factoring) and DES. In the on-line phase all we use is a moderate amount of DES computation and a single modular multiplication. We stress that the costly modular exponentiation operation is performed off-line. This implementation is ideally suited for electronic wallets or smart cards."
1996,A Secure Protocol for the Oblivious Transfer (Extended Abstract).,"There has been a great deal of interest lately in cryptographic protocols. Vaguely, a cryptographic protocol is a mean whereby two or more people can interact in such a manner as to exchange a certain amount of information, while keeping other information secret (either from one of the participants or from another party). One interesting task for which one would like a protocol is the ""oblivious transfer"" introduced by Rabin in [4]. This involves two parties A and B; A would like to send a message of some sort to B, with the constraint that B has a 50% chance of receiving the message, and the other half of the time B receives no information at all about the message. The additional constraint is that A has no idea whether or not B received the message. Oblivious transfer can be viewed as a special type of coin tossing. Although it is not useful in and of itself, it appears to be very useful as a means towards other ends, and in fact has been used in a number of other protocols by a number of different researchers [1]-[3]. In [4], Rabin proposed a protocol for the oblivious transfer. It was intended that the protocol be correct, assuming only that it is hard to factor certain large composite numbers. However, as described below, there is a potential flaw in his protocol; it is possible that B can cheat and obtain extra information from A, even if the assumption about the difficulty of factoring is true. Although we cannot prove that B can cheat in this way, no one has yet been able to prove that B cannot. In Section 4 we present a new protocol for the oblivious transfer. It is similar to Rabin's, but we fix the potential flaw so that it is possible to prove that our protocol works, subject only to the assumption about the difficulty of factoring."
1996,Practical and Provably-Secure Commitment Schemes from Collision-Free Hashing.,"Abstract
We present a very practical string-commitment scheme which is provably secure based solely on collision-free hashing. Our scheme enables a computationally bounded party to commit strings to an unbounded one, and is optimal (within a small constant factor) in terms of interaction, communication, and computation.
Our result also proves that constant round statistical zero-knowledge arguments and constant-round computational zero-knowledge proofs for NP exist based on the existence of collision-free hash functions."
1995,"A Simple Method for Generating and Sharing Pseudo-Random Functions, with Applications to Clipper-like Escrow Systems.","Abstract
We present a very simple method for generating a shared pseudo-random function from a poly-random collection of functions. We discuss the applications of our construction to key escrow."
1995,Verifiable Secret Sharing as Secure Computation.,"Abstract
Verifiable Secret Sharing is a fundamental primitive for secure cryp- tographic design. We present a stronger notion of verifiable secret sharing and exhibit a protocol implementing it. We show that our new notion is preferable to the old ones whenever verifiable secret sharing is used as a tool within larger protocols, rather than being a goal in itself. Indeed our definition, and so our protocol satisfying it, provably guarantees reducibility. Applications of this new notion in the field of secure multiparty computation are also provided."
1994,CS Proofs (Extended Abstracts).,"Abstract:
This paper puts forward a computationally-based notion of proof and explores its implications to computation at large. In particular, given a random oracle or a suitable cryptographic assumption, we show that every computation possesses a short certificate vouching its correctness, and that, under a cryptographic assumption, any program for a /spl Nscr//spl Pscr/-complete problem is checkable in polynomial time. In addition, our work provides the beginnings of a theory of computational complexity that is based on ""individual inputs"" rather than languages.< >"
1994,Reducibility and Completeness in Multi-Party Private Computations.,"Abstract:
We define the notions of reducibility and completeness in multi-party private computations. Let g be an n-argument function. We say that a function f is reducible to g if n honest-but-curious players can compute the function f n-privately, given a black-box for g (for which they secretly give inputs and get the result of operating g on these inputs). We say that g is complete (for multi-party private computations) if every function f is reducible to g. In this paper, we characterize the complete Boolean functions: we show that a Boolean function g is complete if and only if g itself cannot be computed n-privately (when there is no black-box available). Namely, for Boolean functions, the notions of completeness and n-privacy are complementary. This characterization gives a huge collection of complete functions (any non-private Boolean function!) compared to very few examples given (implicitly) in previous work. On the other hand, for non-Boolean functions, we show that these two notions are not complementary. Our results can be viewed as a generalization (for multi-party protocols and for (n/spl ges/2)-argument functions) of the two-party case, where it was known that Oblivious Transfer protocol (and its variants) are complete.< >"
1993,Secret-Key Agreement without Public-Key Cryptography.,"Abstract
In this paper, we describe novel approaches to secret-key agreement. Our schemes are not based on public-key cryptography nor number theory. They are extremely efficient implemented in software or make use of very simple unexpensive hardware. Our technology is particularly well-suited for use in cryptographic scenarios like those of the Clipper Chip, the recent encryption proposal put forward by the Clinton Administration."
1992,How to Sign Given Any Trapdoor Permutation.,"We present a digital signature scheme based on trapdoor permutations. This scheme is secure against existential forgery under adaptive chosen message attack. The only previous scheme with the same level of security was based on factoring.

Although the main thrust of our work is the question of reduced assumptions, we believe that the scheme itself is of some independent interest. We mention improvements on the basic scheme which lead to a memoryless and more efficient version."
1992,Fair Public-Key Cryptosystems.,"Abstract
We show how to construct public-key cryptosystems that are fair, that is, strike a good balance, in a democratic country, between the needs of the Government and those of the Citizens. Fair public-key cryptosystems guarantee that: (1) the system cannot be misused by criminal organizations and (2) the Citizens mantain exactly the same rights to privacy they currently have under the law.
We actually show how to transform any public-key cryptosystem into a fair one. The transformed systems preserve the security and efficiency of the original ones. Thus one can still use whatever system he believes to be more secure, and enjoy the additional properties of fairness. Moreover, for today‚Äôs best known cryptosystems, we show that the transformation to fair ones is particularly efficient and convenient.
As we shall explain, our solution compares favorably with the Clipper Chip, the encryption proposal more recently put forward by the Clinton Administration for solving similar problems."
1991,Proofs that Yield Nothing But Their Validity for All Languages in NP Have Zero-Knowledge Proof Systems.,"In this paper we demonstrate the generality and wide applicability of zero-knowledge proofs, a notion introduced by Goldwasser, Micali and Rackoff. These are probabilistic and interactive proofs that, for the members x of a language L, efficiently demonstrate membership in the language without conveying any additional knowledge. So far, zero-knowledge proofs were known only for some number theoretic languages in NP ? Co-NP."
1991,"Efficient, Perfect Polynomial Random Number Generators.","Abstract
Let N be a positive integer and let P Œµ ‚Ñ§ [x] be a polynomial that is nonlinear on the set ‚Ñ§ N of integers modulo N. If, by choosing x at random in an initial segment of ‚Ñ§ N , P(x) (mod N) appears to be uniformly distributed in ‚Ñ§ N to any polynomial-time observer, then it is possible to construct very efficient pseudorandom number generators that pass any polynomial-time statistical test. We analyse this generator from two points of view. A complexity theoretic analysis relates the perfectness of the generator to the security of the RSA-scheme. A statistical analysis proves that the least-significant bits of P(x) (mod N) are statistically random."
1991,Noninteractive Zero-Knowledge.,"This paper investigates the possibility of disposing of interaction between prover and verifier in a zero-knowledge proof if they share beforehand a short random string.

Without any assumption, it is proven that noninteractive zero-knowledge proofs exist for some number-theoretic languages for which no efficient algorithm is known.

If deciding quadratic residuosity (modulo composite integers whose factorization is not known) is computationally hard, it is shown that the NP-complete language of satisfiability also possesses noninteractive zero-knowledge proofs.
"
1991,Secure Computation (Abstract).,"Abstract
We define what it means for a network of communicating players to securely compute a function of privately held inputs. Intuitively, we wish to correctly compute its value in a manner which protects the privacy of each player‚Äôs contribution, even though a powerful adversary may endeavor to disrupt this enterprise.
This highly general and desirable goal has been around a long time, inspiring a large body protocols, definitions, and ideas, starting with Yao [1982, 1986] and Goldreich, Micali and Wigderson [1987]. But all the while, it had resisted a full and satisfactory formulation.
Our definition is built on several new ideas. Among them:
‚Ä¢
Closely mimicking an ideal evaluation. A secure protocol must mimic this abstraction in a run-by-run manner, our definition depending as much on individual executions as on global properties of ensembles.
 ‚Ä¢
Blending privacy and correctness in a novel way, using a special type of simulator designed for the purpose.
 ‚Ä¢
Requiring adversarial awareness‚Äîcapturing the idea that the adversary should know, in a very strong sense, certain information associated to the execution of a protocol.
  Among the noteworthy and desirable properties of our definition is the reducibility of secure protocols, which we believe to be a cornerstone in a mature theory of secure computation."
1990,A fair protocol for signing contracts.,"Abstract:
Two parties, A and B, want to sign a contract C over a communication network. To do so, they must simultaneously exchange their commitments to C. Since simultaneous exchange is usually impossible in practice, protocols are needed to approximate simultaneity by exchanging partial commitments in piece-by-piece manner. During such a protocol, one party or another may have a slight advantage; a fair protocol keeps this advantage within acceptable limits. A new protocol is proposed. It is fair in the sense that, at any stage in its execution, the conditional probability that one party cannot commit both parties to the contract given that the other party can, is close to zero. This is true even if A and B have vastly different computing powers and is proved under very weak cryptographic assumptions.< >"
1990,Collective Coin Tossing Without Assumptions nor Broadcasting.,"Abstract
To obtain security, one needs to utilize many resources. Among these are one-way functions, physically secure communication channels, and ‚Äîthough less well known‚Äî broadcasting."
1990,Perfect Zero-Knowledge in Constant Rounds.,"Quadratic residuosity and graph isomorphism are classic problems and the canonical examples of zero-knowledge languages. However, despite much research effort, all previous zero-knowledge proofs for them required either cryptography (and thus unproven assumptions) or an unbounded number of rounds of message exchange. For both (and similar) languages, we exhibit zero-knowledge proofs that require 5 rounds and no unproven assumptions. Our solution is essentially optimal, in this setting, due to a recent lower bound argument of Goldreich and Krawzcyk."
1990,The (True) Complexity of Statistical Zero Knowledge.,"Statistical zero-knowledge is a very strong privacy constraint which is not dependent on computational limitations. In this paper we show that given a complexity assumption a much weaker condition suffices to attain statistical zero-knowledge. As a result we are able to simplify statistical zero-knowledge and to better characterize, on many counts, the class of languages that posess statistical zero-knowledge proofs."
1990,The Round Complexity of Secure Protocols (Extended Abstract).,"In a network of n players, each player I having private input xi, we show how the players can collaboratively evaluate a function f(x1,Ö,xn) in a way that does not compromise the privacy of the players' inputs, and yet requires only a constant number of rounds of interaction. The underlying model of computation is a complete network of private channels, with broadcast, and a majority of the players must behave honestly. Our solution assumes the existence of a one-way function."
1989,The Knowledge Complexity of Interactive Proof Systems.,"Usually, a proof of a theorem contains more knowledge than the mere fact that the theorem is true. For instance, to prove that a graph is Hamiltonian it suffices to exhibit a Hamiltonian tour in it; however, this seems to contain more knowledge than the single bit Hamiltonian/non-Hamiltonian.

In this paper a computational complexity theory of the ìknowledgeî contained in a proof is developed. Zero-knowledge proofs are defined as those proofs that convey no additional knowledge other than the correctness of the proposition in question. Examples of zero-knowledge proof systems are given for the languages of quadratic residuosity and 'quadratic nonresiduosity. These are the first examples of zero-knowledge proofs for languages not known to be efficiently recognizable."
1989,On-Line/Off-Line Digital Schemes.,"Abstract
We introduce and exemplify the new concept of ON-LINE/OFF-LINE digital signature schemes. In these schemes the signing of a message is broken into two phases. The first phase is off-line. Though it requires a moderate amount of compu- tation, it presents the advantage that it can be performed leisurely, before the message to be signed is even known. The second phase is on-line. It starts after the message becomes known, it utilizes the precomputation of the first phase and is much faster.
A general construction which transforms any (ordinary) digital signature scheme to an on-line/off-line signature scheme is presented, entailing a small overhead. For each message to be signed, the time required for the off-line phase is essentially the same as in the underlying signature scheme; the time required for the on-line phase is essentially negligible. The time required for the verification is essentially the same as in the underlying signature scheme.
In a practical implementation of our general construction, we use a variant of Rabin‚Äôs signature scheme (based on factoring) and DES. In the on-line phase, all we use is a moderate amount of DES computation. This implementation is ideally suited for electronic wallets or smart cards.
On-line/Off-line digital schemes may also become useful in case substantial pro- gress is made on, say, factoring. In this case, the length of the composite numbers used in signature schemes may need to be increased and signing may become imprac- tical even for the legitimate user. In our scheme, all costly computations are per- formed in the off-line stage while the time for the on-line stage remains essentially unchanged.
An additional advantage of our method is that in some cases the transformed signature scheme is invulnerable to chosen message attack even if the underlying (ordinary) digital signature scheme is not In particular, it allows us to prove that the existence of signature schemes which are unforgeable by known message attack is a (necessary and) sufficient condition for the existence of signature schemes which are unforgeable by chosen message attack."
1989,Minimum Resource Zero-Knowledge Proofs (Extended Abstract).,"Abstract
What are the resources of a zero-knowledge Proof? Interaction, communication, and en- velops. That interaction, that is the number of rounds of a protocol, is a resource is clear. Actually, it is not a very available one: having someone on the line to answer your questions all the time is quite a luxury. Thus, minimizing the number of rounds in zero-knowledge proofs will make these proofs much more attractive from a practical standpoint. That communication, that is the number of bits exchanged in a protocol, is a resource is also immediately clear. Perhaps, what is less clear is why envelopes are a resource. Let us explain why this is the case."
1989,Non-Interactive Oblivious Transfer and Spplications.,"Abstract
We show how to implement oblivious transfer without interaction, through the medium of a public file. As an application we can get non-interactive zero knowledge proofs via the same public file."
1989,Minimum Resource Zero-Knowledge Proofs (Extended Abstract).,"Abstract:
Several resources relating to zero-knowledge protocols are considered. They are the number of envelopes used in the protocol, the number of oblivious transfer protocols executed during the protocol, and the total amount of communication required by the protocol. It is shown that after a preprocessing stage consisting of O(k) executions of oblivious transfer, any polynomial number of NP-theorems of any polysize can be proved noninteractively and in zero knowledge, on the basis of the existence of any one-way function, so that the probability of accepting a false theorem is less than 1/2/sup k/.< >"
1989,An Optimal Probabilistic Algorithm For Synchronous Byzantine Agreement.,"Broadcasting guarantees the recipient of a message that everyone else has received the same message. This guarantee no longer exists in a setting in which all communication is person-to-person and some of the people involved are untrustworthy: though he may claim to send the same message to everyone, an untrustworthy sender may send different messages to different people. In such a setting, Byzantine agreement offers the ""best alternative"" to broadcasting. Thus far, however, reaching Byzantine agreement has required either many rounds of communication (i.e., messages had to be sent back and forth a number of times that grew with the size of the network) or the help of some external trusted party.
In this paper, for the standard communication model of synchronous networks in which each pair of processors is connected by a private communication line, we exhibit a protocol that, in probabilistic polynomial time and without relying on any external trusted party, reaches Byzantine agreement in an expected constant number of rounds and in the worst natural fault model. In fact, our protocol successfully tolerates that up to 1/3 of the processors in the network may deviate from their prescribed instructions in an arbitrary way, cooperate with each other, and perform arbitrarily long computations.
Our protocol effectively demonstrates the power of randomization and zero-knowledge computation against errors. Indeed, it proves that ""privacy"" (a fundamental ingredient of one of our primitives), even when is not a desired goal in itself (as for the Byzantine agreement problem), can be a crucial tool for achieving correctness.
Our protocol also introduces three new primitives---graded broadcast, graded verifiable secret sharing, and oblivious common coin---that are of independent interest, and may be effectively used in more practical protocols than ours."
1989,"""Perfect"" Pseudo-Random Number Generation.",n/a
1988,A Digital Signature Scheme Secure Against Adaptive Chosen-Message Attacks.,"We present a digital signature scheme based on the computational difficulty of integer factorization.

The scheme possesses the novel property of being robust against an adaptive chosen-message attack: an adversary who receives signatures for messages of his choice (where each message may be chosen in a way that depends on the signatures of previously chosen messages) cannot later forge the signature of even a single additional message. This may be somewhat surprising, since in the folklore the properties of having forgery being equivalent to factoring and being invulnerable to an adaptive chosen-message attack were considered to be contradictory.

More generally, we show how to construct a signature scheme with such properties based on the existence of a ìclaw-freeî pair of permutationsóa potentially weaker assumption than the intractibility of integer factorization.

The new scheme is potentially practical: signing and verifying signatures are reasonably fast, and signatures are compact.
"
1988,The Notion of Security for Probabilistic Cryptosystems.,"Three very different formal definitions of security for public-key cryptosystems have been proposedótwo by Goldwasser and Micali and one by Yao. We prove all of them to be equivalent. This equivalence provides evidence that the right formalization of the notion of security has been reached.
"
1988,Everything Provable is Provable in Zero-Knowledge.,"Abstract
Assuming the existence of a secure probabilistic encryption scheme, we show that every language that admits an interactive proof admits a (computational) zero-knowledge interactive proof. This result extends the result of Goldreich, Micali and Wigderson, that, under the same assumption, all of NP admits zero-knowledge interactive proofs. Assuming envelopes for bit commitment, we show tht every language that admits an interactive proof admits a perfect zero-knowledge interactive proof."
1988,"Efficient, Perfect Random Number Generators.","Abstract
We describe a method that transforms every perfect random number generator into one that can be accelerated by parallel evaluation. Our method of parallelization is perfect, m parallel processors speed the generation of pseudo-random bits by a factor m; these parallel processors need not to communicate. Using sufficiently many parallel processors we can generate pseudo-random bits with nearly any speed. These parallel generators enable fast retrieval of substrings of very long pseudo-random strings. Individual bits of pseudo-random strings of length 1020 can be accessed within a few seconds. We improve and extend the RSA-random number generator to a polynomial generator that is almost as efficient as the linear congruential generator. We question the existence of polynomial random number generators that are perfect and use a prime modulus."
1988,How To Sign Given Any Trapdoor Function.,"Abstract
We present a digital signature scheme based on trapdoor permutations. This scheme is secure against existential forgery under adaptive chosen message attack. The only previous scheme with the same level of security was based on factoring.
Although the main thrust of our work is the question of reduced assumptions, we believe that the scheme itself is of some independent interest. We mention improvements on the basic scheme which lead to a memoryless and more efficient version."
1988,An Improvement of the Fiat-Shamir Identification and Signature Scheme.,"Abstract
In 1986 Fiat and Shamir exhibited zero-knowledge based identification and digital signature schemes which require only 10 to 30 modular multiplications per party. In this paper we describe an improvement of this scheme which reduces the verifier‚Äôs complexity to less than 2 modular multiplications and leaves the prover‚Äôs complexity unchanged.
The new variant is particularly useful when a central computer has to verify in real time signed messages from thousands of remote terminals, or when the same signature has to be repeatedly verified."
1988,Proving Security Against Chosen Cyphertext Attacks.,"Abstract
The relevance of zero knowledge to cryptography has become apparent in the recent years. In this paper we advance this theory by showing that interaction in any zero-knowledge proof can be replaced by sharing a common, short, random string. This advance finds immediate application in the construction of the first public-key cryptosystem secure against chosen ciphertext attack.
Our solution, though not yet practical, is of theoretical significance, since the existence of cryptosystems secure against chosen ciphertext attack has been a famous long-standing open problem in the field."
1988,Non-Interactive Zero-Knowledge with Preprocessing.,"Abstract
Non-Interactive Zero-Knowledge Proof Systems have been proven to exist under a specific complexity assumption; namely, under the Quadratic Residuosity Assumption which gives rise to a specific secure probabilistic encryption scheme.
In this paper we prove that the existence of any secure probabilistic encryption scheme, actually any one-way encryption scheme, is enough for Non-Interactive Zero-Knowledge in a modified model. That is, we show that the ability to prove a randomly chosen theorem allows to subsequently prove non-interactively and in Zero-Knowledge any smaller size theorem whose proof is discovered."
1988,How to Sign Given Any Trapdoor Function (Extended Abstract).,"We present a digital signature scheme which combines high security with the property of being based on a very general assumption: the existence of trapdoor permutations. Previous signature schemes with comparable levels of security were based on assumptions of the computational hardness of particular algebraic problems such as factoring. Our contribution is to free this important cryptographic primitive from the fortunes of any specific algebraic problem by establishing a truly general signature scheme.
"
1988,Non-Interactive Zero-Knowledge and Its Applications (Extended Abstract).,"The intriguing notion of a Zero-Knowledge Proof System has been introduced by Goldwasser, Micali and Rackoff [GMR] and its wide applicability has been demonstrated by Goldreich, Micali and Wigderson [GMW1]-[GMW2].

Based on complexity theoretic assumptions, Zero-Knowledge Proof Systems exist, provided that
(i)
The prover and the verifier are allowed to talk back and forth.

 
(ii)
The verifier is allowed to flip coins whose result the prover cannot see.

 
Blum, Feldman and Micali [BFM] have recently shown that, based on specific complexity theoretic assumption (the computational difficulty of distinguishing products of two primes from those product of three primes), both the requirements (i) and (ii) above are not necessary to the existence of Zero-Knowledge Proof Systems. Instead of (i), it is enough for the prover only to talk and for the verifier only to listen. Instead of (ii), it is enough that both the prover and verifier share a randomly selected string.

We strengthen their result by showing that Non-Interactive Zero-Knowledge Proof Systems exist based on the weaker and well-known assumption that quadratic residuosity is hard."
1988,Optimal Algorithms for Byzantine Agreement.,"We exhibit randomized Byzantine agreement (BA) algorithms achieving optimal running time and fault tolerance against all types of adversaries ever considered in the literature. Our BA algorithms do not require trusted parties, preprocessing, or non-constructive arguments.

Given private communication lines, we show that n processors can reach BA in expected constant time

in a syncronous network if any < n/3 faults occur
in an asynchronous network if any < n/4 faults occur
For both synchronous and asynchronous networks whose lines do not guarantee private communication, we may use cryptography to obtain algorithms optimal both in fault tolerance and running time against computationally bounded adversaries. (Thus, in this setting, we tolerate up to n/3 faults even in an asynchronous network.)"
1987,Non-Interactive Zero-Knowledge Proof Systems.,"Abstract
The intriguing notion of a Zero-Knowledge Proof System has been introduced by Goldwasser, Micali and Rackoff [GMR] and its wide applicability has been demonstrated by Goldreich, Micali and Wigderson [GMW1]-[GMW2].
Based on complexity theoretic assumptions, Zero-Knowledge Proof Systems exist, provided that
(i)
The prover and the verifier are allowed to talk back and forth.
 (ii)
The verifier is allowed to flip coins whose result the prover cannot see.
  Blum, Feldman and Micali [BFM] have recently shown that, based on specific complexity theoretic assumption (the computational difficulty of distinguishing products of two primes from those product of three primes), both the requirements (i) and (ii) above are not necessary to the existence of Zero-Knowledge Proof Systems. Instead of (i), it is enough for the prover only to talk and for the verifier only to listen. Instead of (ii), it is enough that both the prover and verifier share a randomly selected string.
We strengthen their result by showing that Non-Interactive Zero-Knowledge Proof Systems exist based on the weaker and well-known assumption that quadratic residuosity is hard."
1987,How to Play any Mental Game or A Completeness Theorem for Protocols with Honest Majority.,"We present a polynomial-time algorithm that, given as a input the description of a game with incomplete information and any number of players, produces a protocol for playing the game that leaks no partial information, provided the majority of the players is honest.

Our algorithm automatically solves all the multi-party protocol problems addressed in complexity-based cryptography during the last 10 years. It actually is a completeness theorem for the class of distributed protocols with honest majority. Such completeness theorem is optimal in the sense that, if the majority of the players is not honest, some protocol problems have no efficient solution [C]."
1986,How to construct random functions.,"A constructive theory of randomness for functions, based on computational complexity, is developed, and a pseudorandom function generator is presented. This generator is a deterministic polynomial-time algorithm that transforms pairs (g, r), where g is any one-way function and r is a random k-bit string, to polynomial-time computable functions Ér: {1, Ö , 2k} ? {1, Ö , 2k}. These Ér's cannot be distinguished from random functions by any probabilistic polynomial-time algorithm that asks and receives the value of a function at arguments of its choice. The result has applications in cryptography, random constructions, and complexity theory.
"
1986,An O(EV log V) Algorithm for Finding a Maximal Weighted Matching in General Graphs.,"We define two generalized types of a priority queue by allowing some forms of changing the priorities of the elements in the queue. We show that they can be implemented efficiently. Consequently, each operation takes $O(\log n)$ time. We use these generalized priority queues to construct an $O(EV\log V)$ algorithm for finding a maximal weighted matching in general graphs.
"
1986,"How to Prove all NP-Statements in Zero-Knowledge, and a Methodology of Cryptographic Protocol Design.","Abstract
Under the assumption that encryption functions exist, we show that all languages in NP possess zero-knowledge proofs. That is, it is possible to demonstrate that a CNF formula is satisfiable without revealing any other property of the formula. In particular, without yielding neither a satisfying assignment nor weaker properties such as whether there is a satisfying assignment in which x 1 = TRUE, or whether there is a satisfying assignment in which x 1 = x 3 etc.
The above result allows us to prove two fundamental theorems in the field of (two-party and multi-party) cryptographic protocols. These theorems yield automatic and efficient transformations that, given a protocol that is correct with respect to an extremely weak adversary, output a protocol correct in the most adversarial scenario. Thus, these theorems imply powerful methodologies for developing two-party and multi-party cryptographic protocols."
1986,The Notion of Security for Probabilistic Cryptosystems.,"Abstract
Three very different formal definitions of security for public-key cryptosystems have been proposed‚Äîtwo by Goldwasser and Micali and one by Yao. We prove all of them to be equivalent. This equivalence provides evidence that the right formalization of the notion of security has been reached."
1986,Proofs that Yield Nothing But their Validity and a Methodology of Cryptographic Protocol Design (Extended Abstract).,"Abstract:
In this paper we demonstrate the generality and wide applicability of zero-knowledge proofs, a notion introduced by Goldwasser, Micali and Rackoff. These are probabilistic and interactive proofs that, for the members x of a language L, efficiently demonstrate membership in the language without conveying any additional knowledge. So far, zero-knowledge proofs were known only for some number theoretic languages in NP ‚à© Co-NP."
1986,Dynamic deadlock resolution protocols (Extended Abstract).,"Abstract:
The deadlock resolution problem can be informally stated as follows. There exists a set of actions, generated at different times, with some complex and contradictory precedence constraints between their executions. To resolve a deadlock, some of the actions need to be aborted; this enables to execute the remaining ones. This problem naturally arises in the context of distributed systems, e.g. communication networks, distributed operating systems and distributed databases, where actions are generated by many processors, and are not coordinated by a central controller. In this paper, we are concerned with efficient distributed algorithms (protocols) for resolution of dynamic deadlocks. Such resolution protocols must operate on-line without any knowledge of the future and using only local information. The main contribution of the paper is a reduction of the most general dynamic deadlock resolution problem to a conceptually simpler static problem, in which all actions are known a priori. The complexity of our reduction is O (m + n logn) in communication and O (n) in time, where n is the number of actions and m is total number of constraints. Since the static deadlock resolution requires at least Œ©(m + n logn) in communication and Œ©(n) in time, our reduction essentially shows that the resolution of dynamic deadlocks is not any harder than resolution of the static ones. We also show here a simple and optimal algorithm for the static problem, which, together with the above reduction, yields an optimal dynamic deadlock resolution protocol."
1986,Proofs that Release Minimum Knowledge.,n/a
1986,Knowledge and Efficient Computation.,n/a
1985,Byzantine Agreement in Constant Expected Time (and Trusting No One).,"Abstract:
We present a novel cryptographic algorithm for Byzantine agreement in a network with l=O(n) faulty processors and in the most adversarial setting. Our algorithm requires, once and for all, O(t) rounds of preprocessing. Afterwards it allows us to reach each individual Byzantine agreement in constant expected time. Our solution does not make use of any trusted party."
1985,Verifiable Secret Sharing and Achieving Simultaneity in the Presence of Faults (Extended Abstract).,"Verifiable secret sharing is a cryptographic protocol that allows one to break a secret in 11 pieccs and publicly distribute thcln to 11 people so that tile secret is reconstructible given only sufficiently many pieces. 'rhe novelty is that everyone can verify that all received a ""valid"" piece of the secret without having any idea of what the secret is. One application of this tool is the simulation of simultaneous-broadcast networks on semi-synchronous broadcast networks. "
1985,A Fair Protocol for Signing Contracts (Extended Abstract).,"Abstract
Assume that two parties, A and B, want to sign a contract over a communication network, i.e. they want to exchange their ‚Äúcommitments‚Äú to the contract. We consider a contract signing protocol to be fair if, at any stage in its execution, the following hold: the conditional probability that party A obtains B's signature to the contract given that B has obtained A's signature to the contract, is close to 1. (Symmetrically, when switching the roles of A and B).
Contract signing protocols cannot be fair without relying on a trusted third party. We present a fair, cryptographic protocol for signing contracts that makes use of the weakest possible form of a trusted third party (judge). If both A and B are honest, the judge will never be called upon. Otherwise, the judge rules by performing a simple computation, without referring to previous verdicts. Thus, no bookkeeping is required from the judge. Our protocol is fair even if A and B have very different computing powers. Its fairness is proved under the very general cryptographic assumption that functions that are one-way in a weak sense exist. Our protocol is also optimal with respect to the number of messages exchanged."
1985,The Knowledge Complexity of Interactive Proof-Systems (Extended Abstract).,"Usually, a proof of a theorem contains more knowledge than the mere fact that the theorem is true. For instance, to prove that a graph is Hamiltonian it suffices to exhibit a Hamiltonian tour in it; however, this seems to contain more knowledge than the single bit Hamiltonian/non-Hamiltonian.In this paper a computational complexity theory of the ìknowledgeî contained in a proof is developed. Zero-knowledge proofs are defined as those proofs that convey no additional knowledge other than the correctness of the proposition in question. Examples of zero-knowledge proof systems are given for the languages of quadratic residuosity and 'quadratic nonresiduosity. These are the first examples of zero-knowledge proofs for languages not known to be efficiently recognizable."
1984,Probabilistic Encryption.,"Abstract
A new probabilistic model of data encryption is introduced. For this model, under suitable complexity assumptions, it is proved that extracting any information about the cleartext from the cyphertext is hard on the average for an adversary with polynomially bounded computational resources. The proof holds for any message space with any probability distribution. The first implementation of this model is presented. The security of this implementation is proved under the interactability assumptin of deciding Quadratic Residuosity modulo composite numbers whose factorization is unknown."
1984,How to Generate Cryptographically Strong Sequences of Pseudo-Random Bits.,"We give a set of conditions that allow one to generate 50ñ50 unpredictable bits.Based on those conditions, we present a general algorithmic scheme for constructing polynomial-time deterministic algorithms that stretch a short secret random input into a long sequence of unpredictable pseudo-random bits.

We give an implementation of our scheme and exhibit a pseudo-random bit generator for which any efficient strategy for predicting the next output bit with better than 50ñ50 chance is easily transformable to an ìequally efficientî algorithm for solving the discrete logarithm problem. In particular: if the discrete logarithm problem cannot be solved in probabilistic polynomial time, no probabilistic polynomial-time algorithm can guess the next output bit better than by flipping a coin: if ìheadî guess ì0î, if ìtailî guess ì1î
"
1984,On the Cryptographic Applications of Random Functions.,"Abstract
Now that ‚Äúrandom functions‚Äù can be efficiently constructed([GGM]), we discuss some of their possible applications to cryptography:
1)
Distributing unforgable ID numbers which can be locally verified by stations which contain only a small amount of storage.
 2)
Dynamic Hashing: even if the adversary can change the key-distribution depending on the values the hashing function has assigned to the previous keys, still he can not force collisions.
 3)
Constructing deterministic, memoryless authentication schemes which are provably secure against chosen message attack.
 4)
Construction Identity Friend or Foe systems.
 "
1984,"A ""Paradoxical'""Solution to the Signature Problem (Abstract).","Abstract
We present a general signature scheme which uses any pair of trap-door permutations (f0, f1) for which it is infeasible to find any x, y with f0(x) = f1(y). The scheme possesses the novel property of being robust against an adaptive chosen message attack: no adversary who first asks for and then receives sgnatures for messages of his choice (which may depend on previous signatures seen) can later forge the signature of even a singl additional message.
For specific instance of our general scheme, we prove that
(1)
forging signatures is provably equivalent to factoring (i.e., factoring is polynomial-time reducible to forging signatures, and vice versa) while
 (2)
forging an additional signature, after an adaptive chosen message attack is still equivalent to factoring.
  Such scheme is ‚Äúparadoxical‚Äù since the above two properties were believed (and even ‚Äúproven‚Äù in the folklore) to be contradictory.
The new scheme is potentially practical: signing and verifying signatures are reasonably fast, and signatures are not too long."
1984,"A ""Paradoxical"" Solution to the Signature Problem (Extended Abstract).","We present a general signature scheme which uses any pair of trap-door permutations (f0, f1) for which it is infeasible to find any x, y with f0(x) = f1(y). The scheme possesses the novel property of being robust against an adaptive chosen message attack: no adversary who first asks for and then receives sgnatures for messages of his choice (which may depend on previous signatures seen) can later forge the signature of even a singl additional message.

For specific instance of our general scheme, we prove that
(1)
forging signatures is provably equivalent to factoring (i.e., factoring is polynomial-time reducible to forging signatures, and vice versa) while

 
(2)
forging an additional signature, after an adaptive chosen message attack is still equivalent to factoring.

 
Such scheme is ìparadoxicalî since the above two properties were believed (and even ìprovenî in the folklore) to be contradictory.
The new scheme is potentially practical: signing and verifying signatures are reasonably fast, and signatures are not too long."
1984,How to Construct Random Functions (Extended Abstract).,"Abstract:
This paper develops a constructive theory of randomness for functions based on computational complexity. We present a deterministic polynomial-time algorithm that transforms pairs (g,r), where g is any one-way (in a very weak sense) function and r is a random k-bit string, to polynomial-time computable functions f/sub r/:{1,..., 2/sup k} /spl I.oarr/ {1, ..., 2/sup k/}. These f/sub r/'s cannot be distinguished from random functions by any probabilistic polynomial time algorithm that asks and receives the value of a function at arguments of its choice. The result has applications in cryptography, random constructions and complexity theory."
1983,How to Simultaneously Exchange a Secret Bit by Flipping a Symmetrically-Biased Coin.,"Abstract:
We present a cryptographic protocol allowing two mutually distrusting parties, A and B, each having a secret bit, to ""simultaneously"" exchange the values of those bits. It is assumed that initially each party presents a correct encryption of his secret bit to the other party. We develop a new tool to implement our protocol: a slightly biased symmetric coin. The key property of this coin is that from each flip A receives a piece of probabilistic information about B's secret bit which is symmetric to the piece of information B receives about A's secret bit."
1983,Strong Signature Schemes.,"The notion of digital signature based on trapdoor functions has been introduced by Diffie and Hellman[3]. Rivest, Shamir and Adleman[8] gave the first number theoretic implementation of a signature scheme based on a trapdoor function. If f is a trapdoor function and m a message, f?1(m) is the signature of m. The signature can be verified by computing f(f?1(m)) = m. This approach presents the following problems even when f is hard to invert:

1) there may be special message spaces (or subsets of them) that are easy to sign without knowing the trapdoor information

2) it is possible to forge the signature of random numbers; this violates the requirements of many protocols

3) given a polynomial number of signed messages, it may be possible to sign a new one without knowing the trapdoor information.

We solve the above problems by exhibiting two signature schemes for which any strategy of an adversary, who has seen all previously signed messages, that has a moderate success in forging even a single additional signature, is transformable to a fast algorithm for factoring or inverting the RSA function. This provably holds for all message spaces with all possible Probability distributions. Thus, in particular, given the signature of m, forging the signature of m+1 or 2m or 2sm is as hard as factoring. The two signature schemes"
1982,On Signatures and Authentication.,"Abstract
The design of cryptographic protocols using trapdoor and one-way functions has received considerable attention in the past few years [1‚Äì8]. More recently, attention has been paid to provide rigorous correctness proofs based on simple mathematical assumptions, for example, in coin flipping (Blum [1]), mental poker (Goldwasser and Micali [4]). It is perhaps reasonable to speculate at this time that all cryptographic protocols can eventually be designed to be provably secure under simple assumptions, such as factoring large numbers or inverting RSA functions are computationally intractable in the appropriate sense."
1982,How to Generate Cryptographically Strong Sequences of Pseudo Random Bits.,"We give a set of conditions that allow one to generate 50ñ50 unpredictable bits.Based on those conditions, we present a general algorithmic scheme for constructing polynomial-time deterministic algorithms that stretch a short secret random input into a long sequence of unpredictable pseudo-random bits.We give an implementation of our scheme and exhibit a pseudo-random bit generator for which any efficient strategy for predicting the next output bit with better than 50ñ50 chance is easily transformable to an ìequally efficientî algorithm for solving the discrete logarithm problem. In particular: if the discrete logarithm problem cannot be solved in probabilistic polynomial time, no probabilistic polynomial-time algorithm can guess the next output bit better than by flipping a coin: if ìheadî guess ì0î, if ìtailî guess ì1î "
1982,Why and How to Establish a Private Code on a Public Network (Extended Abstract).,"Abstract:
The Diffie and Hellman model of a Public Key Cryptosystem has received much attention as a way to provide secure network communication. In this paper, we show that the original Diffie and Hellman model does not guarantee security against other users in the system. It is shown how users, which are more powerful adversarys than the traditionally considered passive eavesdroppers, can decrypt other users messages, in implementations of Public Key Cryptosystem using the RSA function, the Rabin function and the Goldwasser&Micali scheme. This weakness depends on the bit security of the encryption function. For the RSA (Rabin) function we show that computing, from the cyphertext, specific bits of the cleartext, is polynomially equivalent to inverting the function (factoring). As for many message spaces, this bit can be easily found out by communicating, the system is insecure. We present a modification of the Diffie and Hellman model of a Public-Key Cryptosystem, and one concrete implementation of the modified model. For this implementation, the difficulty of extracting partial information about clear text messages from their encoding, by eavesdroppers, users or by Chosen Cyphertext Attacks is proved equivalent to the computational difficulty of factoring. Such equivalence proof holds in a very strong probabilistic sense and for any message space. No additional assumptions, such as the existence of a perfect signature scheme, or a trusted authentication center, are made."
1982,Priority Queues with Variable Priority and an O(EV log V) Algorithm for Finding a Maximal Weighted Matching in General Graphs.,"Abstract:
We define two generalized types of a priority queue by allowing some forms of changing the priorities of the elements in the queue. We show that they can be implemented efficiently. Consequently, each operation takes O(log n) time. We use these generalized priority queues to construct an O(EV log V) algorithm for finding a maximal weighted matching in general graphs."
1982,Probabilistic Encryption and How to Play Mental Poker Keeping Secret All Partial Information.,"This paper proposes an Encryption Scheme that possess the following property : An adversary, who knows the encryption algorithm and is given the cyphertext, cannot obtain any information about the clear-text.

Any implementation of a Public Key Cryptosystem, as proposed by Diffie and Hellman in [8], should possess this property.

Our Encryption Scheme follows the ideas in the number theoretic implementations of a Public Key Cryptosystem due to Rivest, Shamir and Adleman [13], and Rabin [12]."
1981,Two-Way Deterministic Finite Automata are Exponentially More Succinct Than Sweeping Automata.,n/a
1980,Minimal Forms in lambda-Calculus Computations.,"The notion of a minimal form is defined as an extension of the notion of a normal form in ?-?-calculus and its meaning is discussed in a computational environment. The features of the Knuth-Gross reduction strategy are used to prove that to possess a minimal form, for a generic term, is a semidecidable predicate.
"
1980,An O(sqrt(|v|) |E|) Algorithm for Finding Maximum Matching in General Graphs.,"Abstract:
In this paper we present an 0(‚àö|V|¬∑|E|) algorithm for finding a maximum matching in general graphs. This algorithm works in 'phases'. In each phase a maximal set of disjoint minimum length augmenting paths is found, and the existing matching is increased along these paths. Our contribution consists in devising a special way of handling blossoms, which enables an O(|E|) implementation of a phase. In each phase, the algorithm grows Breadth First Search trees at all unmatched vertices. When it detects the presence of a blossom, it does not 'shrink' the blossom immediately. Instead, it delays the shrinking in such a way that the first augmenting path found is of minimum length. Furthermore, it achieves the effect of shrinking a blossom by a special labeling procedure which enables it to find an augmenting path through a blossom quickly."
