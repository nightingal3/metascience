1994,Towards real-time GOMS: a model of expert behaviour in a highly interactive task.,"We present an analysis of an expert performing a highly interactive computer task. The analysis uses GOMS models, specifying the Goals, Operators, Methods, and Selection rules used by the expert. Two models are presented, one with function-level operators which perform high-level functions in the domain, and one with keystroke-level operators which describe hand movements. For a segment of behaviour in which the expert accomplished about 30 functions in about 30 s, the function-level model predicted the observed behaviour well, while the keystroke-level model predicted only about half of the observed hand movements. These results, including the discrepancy between the models, are discussed.
"
1993,Intelligent control of external software systems.,"This paper focuses on the relatively unexplored set of issues that arises when an intelligent agent attempts to use external software systems (EESs). The issues are illustrated initially in the context of the complex agent-ESS interactions in an engineering design example. Approaching the area from the perspective of artificial intelligence (AI) research, we find that in general, agent-ESS interactions vary widely. We characterize the possible variations in terms of performance capabilities required, skill levels at which performance is exhibited, and knowledge sources from which capabilities can be acquired. We are exploring these variations using Soar as our candidate AI agent; the document briefly describes seven Soar-based projects in early stages of development, in which agent-ESS issues are addressed. We conclude by placing agent-ESS research in the context of other work on software technology, and discuss the research agenda we have set for ourselves in this area.
"
1993,Reflections on the Knowledge Level.,n/a
1993,Applying an Architecture for General Intelligence to Reduce Scheduling Effort.,"A system called Merle-Soar is described which demonstrates how a specific architecture for general intelligence and learning (Soar) can be used to reduce scheduling effort when solving simple scheduling problems. In particular, we describe how Merle-Soar schedules sequences of jobs on a single bottleneck machine in a job shop. The knowledge of dispatching, acquired from examining how a human expert performs the task, is cast as search rules. A study was conducted which examined the extent to which learning could contribute to decreases in scheduling effort; specifically, the contribution of learning within-tasks was exploredóthe change in reasoning effort while solving a particular scheduling problem as knowledge is accumulated from successive trials. The results indicated that dramatic reductions in scheduling effort (in terms of the Soar architecture) were obtained. Knowledge gained early in the scheduling task was subsequently applied later in the task to reduce deliberation, and knowledge gained from one trial successfully reduced deliberation effort in subsequent trials. Additionally, the reduction exhibited the general power law of learning documented in psychological studies of skill acquisition.
"
1992,"Learning 10, 000 Chunks: What's It Like Out There?","This paper describes an initial exploration into large learning systems, i.e., systems that learn a large number of rules. Given the well-known utility problem in learning systems, efficiency questions are a major concern. But the questions are much broader than just efficiency, e.g., will the effectiveness of the learned rules change with scale? This investigation uses a single problem-solving and learning system, Dispatcher-Soar, to begin to get answers to these questions. Dispatcher-Soar has currently learned 10,112 new productions, on top of an initial system of 1,819 productions, so its total size is 11,931 productions. This represents one of the largest production systems in existence, and by far the largest number of rules ever learned by an AI system. This paper presents a variety of data from our experiments with Dispatcher-Soar and raises important questions for large learning systems.
"
1991,A Preliminary Analysis of the Soar Architecture as a Basis for General Intelligence.,"In this article we take a step towards providing an analysis of the Soar architecture as a basis for general intelligence. Included are discussions of the basic assumptions underlying the development of Soar, a description of Soar cast in terms of the theoretical idea of multiple levels of description, an example of Soar performing multi-column subtraction, and three analyses of Soar: its natural tasks, the sources of its power, and its scope and limits
"
1991,The Effectiveness of Task-Level Parallelism for Production Systems.,"Large production systems (rule-based systems) continue to suffer from extremely slow execution which limits their utility in practical applications as well as in research settings. Most investigations in speeding up these systems have focused on match (or knowledge-search) parallelism. Although good speed-ups have been achieved in this process, these investigations have revealed the limitations on the total speed-up available from this source. This limited speed-up is insufficient to alleviate the problem of slow execution in large-scale production system implementations. Such large-scale systems are expected to increase as researchers develop increasingly more competent production systems. In this paper, we focus on task-level parallelism, which is obtained by a high-level decomposition of the production system. Speed-ups obtained from task-level parallelism will multiply with the speed-ups obtained from match parallelism. The vehicle for our investigation of task-level parallelism is SPAM, a high-level vision system, implemented as a production system. SPAM is a mature research system with a typical run requiring between 50,000 to 400,000 production firings and an execution time of the order of 10 to 100 cpu hours. We report very encouraging speed-ups from task-level parallelism in SPAM ó our parallel implementation shows near linear speed-ups of over 12 fold using 14 processors and points the way to substantial (50-100 fold) speed-ups from task-level parallelism. We present a characterization of task-level parallelism in production systems and describe our methodology for selecting and applying a particular approach to parallelize SPAM. Additionally, we report the speed-ups obtained from the use of shared virtual memory (network shared memory) in this implementation. Overall, task-level parallelism has not received much attention in the literature. Our experience illustrates that it is potentially a very important tool for speeding up large-scale production systems1."
1990,The Problem of Expensive Chunks and its Solution by Restricting Expressiveness.,"Soar is an architecture for a system that is intended to be capable of general intelligence. Chunking, a simple experience-based learning mechanism, is Soar's only learning mechanism. Chunking creates new items of information, called chunks, based on the results of problem-solving and stores them in the knowledge base. These chunks are accessed and used in appropriate later situations to avoid the problem-solving required to determine them. It is already well-established that chunking improves performance in Soar when viewed in terms of the subproblems required and the number of steps within a subproblem. However, despite the reduction in number of steps, sometimes there may be a severe degradation in the total run time. This problem arises due toexpensive chunks, i.e., chunks that require a large amount of effort in accessing them from the knowledge base. They pose a major problem for Soar, since in their presence, no guarantees can be given about Soar's performance.
In this article, we establish that expensive chunks exist and analyze their causes. We use this analysis to propose a solution for expensive chunks. The solution is based on the notion of restricting the expressiveness of the representational language to guarantee that the chunks formed will require only a limited amount of accessing effort. We analyze the tradeoffs involved in restricting expressiveness and present some empirical evidence to support our analysis.
"
1990,The Effectiveness of Task-Level Parallelism for High-Level Vision.,"Large production systems (rule-based systems) continue to suffer from extremely slow execution which limits their utility in practical applications as well as in research settings. Most investigations in speeding up these systems have focused on match (or knowledge-search) parallelism. Although good speed-ups have been achieved in this process, these investigations have revealed the limitations on the total speed-up available from this source. This limited speed-up is insufficient to alleviate the problem of slow execution in large-scale production system implementations. Such large-scale systems are expected to increase as researchers develop increasingly more competent production systems. In this paper, we focus on task-level parallelism, which is obtained by a high-level decomposition of the production system. Speed-ups obtained from task-level parallelism will multiply with the speed-ups obtained from match parallelism. The vehicle for our investigation of task-level parallelism is SPAM, a high-level vision system, implemented as a production system. SPAM is a mature research system with a typical run requiring between 50,000 to 400,000 production firings and an execution time of the order of 10 to 100 cpu hours. We report very encouraging speed-ups from task-level parallelism in SPAM ‚Äî our parallel implementation shows near linear speed-ups of over 12 fold using 14 processors and points the way to substantial (50-100 fold) speed-ups from task-level parallelism. We present a characterization of task-level parallelism in production systems and describe our methodology for selecting and applying a particular approach to parallelize SPAM. Additionally, we report the speed-ups obtained from the use of shared virtual memory (network shared memory) in this implementation. Overall, task-level parallelism has not received much attention in the literature. Our experience illustrates that it is potentially a very important tool for speeding up large-scale production systems1."
1989,High-Speed Implementations of Rule-Based Systems.,"Rule-based systems are widely used in artificial intelligence for modeling intelligent behavior and building expert systems. Most rule-based programs, however, are extremely computation intensive and run quite slowly. The slow speed of execution has prohibited the use of rule-based systems in domains requiring high performance and real-time response. In this paper we explore various methods for speeding up the execution of rule-based systems. In particular, we examine the role of parallelism in the high-speed execution of rule-based systems and study the architectural issues in the design of computers for rule-based systems. Our results show that contrary to initial expectations, the speed-up that can be obtained from parallelism is quite limited, only about tenfold. The reasons for the small speed-up are: (1) the small number of rules relevant to each change to data memory; (2) the large variation in the processing requirements of relevant rules; and (3) the small number of changes made to data memory between synchronization steps. Furthermore, we observe that to obtain this limited factor of tenfold speed-up, it is necessary to exploit parallelism at a very fine granularity. We propose that a suitable architecture to exploit such fine-grain parallelism is a shared-memory multiprocessor with 32-64 processors. Using such a multiprocessor, it is possible to obtain execution speeds of about 3800 rule-firings/set. This speed is significantly higher than that obtained by other proposed parallel implementations of rule-based systems."
1989,Cumulating the science of HCI: from s-R compatibility to transcription typing.,"In keeping with our claim that an applied psychology of HCI must be based on cumulative work within a unified framework, we present two extensions of the Model Human Processor. A model of immediate response behavior and stimulus-response (S-R) compatibility is presented and extended to a new domain: transcription typing. Parameters are estimated using one S-R compatibility experiment, used to make a priori predictions in four other S-R compatibility tasks, and then carried over into the area of typing. A model of expert transcription typing is described and its prediction of typing phenomena is demonstrated and summarized."
1989,A Problem Space Approach to Expert System Specification.,"One view of expert system development separates the endeavor into two parts. First, a domain expert, with the aid of a knowledge engineer, articulates a procedure for performing the desired task in some external form. Next, the knowledge engineer operationalizes the external description within some computer language. This paper examines the nature of the processes that operationalize natural task descriptions. We exhibit a language based on a computational model of problem spaces for which these processes are quite simple. We describe the processes in detail, and discuss which aspects of our computational model determine the simplicity of these processes."
1988,Parallel implementation of OPS5 on the encore multiprocessor: Results and analysis.,"Abstract
Until now, most results reported for parallelism in production systems (rulebased systems) have been simulation results-very few real parallel implementations exist. In this paper, we present results from our parallel implementation of OPS5 on the Encore multiprocessor. The implementation exploits very finegrained parallelism to achieve significant speed-ups. For one of the applications, we achieve 12.4 fold speed-up using 13 processes. Our implementation is also distinct from other parallel implementations in that we parallelize a highly optimized C-based implementation of OPS5. Running on a uniprocessor, our C-based implementation is 10‚Äì20 times faster than the standard lisp implementation distributed by Carnegie Mellon University. In addition to presenting the performance numbers, the paper discusses the details of the parallel implementation-the data structures used, the amount of contention observed for shared data structures, and the techniques used to reduce such contention."
1988,"Integrating Multiple Sources of Knowledge Into Designer Soar, an Automatic Algorithm Designer.","Designing algorithms requires diverse knowledge about general problem-solving, algorithm design, implementation techniques, and the application domain. The knowledge can come from a variety of sources, including previous design experience, and the ability to integrate larowledge from such diverse sources appears critical to the success of human algorithm designers. Such integration is feasible in an automatic design system, especially when supported by the general problem-solving and learning mechanisms in the Soar architecture. Our system, Designer-Soar, now designs several simple generate-and-test and divide-and-conquer algorithms. The system already uses several levels of abstraction, generalizes from examples, and learns from experience, transferring knowledge acquired during the design of one algorithm to aid in the design of others.
"
1988,Some Chunks Are Expensive.,"Soar is an attempt to realize a set of hypothesis on the nature of general intelligence within a single system. One central hypothesis is that chunking, Soar's simple experience-based learning mechanism, can form the basis for a general learning mechanism. It is already well established that the addition of chunks improves the performance in Soar a great deal, when viewed in terms of subproblems required and number of steps within a subproblem. But this high level view does not take into account potential offsetting costs that arise from various computational effects. This paper is an investigation into the computational effect of expensive chunks. These chunks add significantly to the time per step by being individually expensive. We decompose the causes of expensive chunks into three components and identify the features of the task environment that give rise to them. We then discuss the implications of the existence of expensive chunks for a complete implementation of Soar."
1988,Parallel OPS5 on the Encore Multimax.,"Until now, most results reported for parallelism in production systems (rule-based systems) have been simulation results -very few real parallel implementations exist. In this paper, we present results from our parallel implementation of OPS5 on the Encore multiprocessor. The implementation exploits very fine-grained parallelism to achieve significant speed-ups. For one of the applications, we achieve 12.4 fold speed-up using 13 processes. Our implementation is also distinct from other parallel implementations in that we parallelize a highly optimized C-based implementation of OPS5. Running on a uniprocessor, our C-based implementation is 10-20 times faster than the standard lisp implementation distributed by Carnegie Mellon University. In addition to presenting the performance numbers, the paper discusses the amount of contention observed for shared data structures, and the techniques used to reduce such contention."
1988,Soar/PSM-E: Investigating Match Parallelism in a Learning Production System.,"Soar is an attempt to realize a set of hypotheses on the nature of general intelligence within a single system. Soar uses a production system (rule based system) to encode its knowledge base. Its learning mechanism, chunking, adds productions continuously to the production system. The process of searching for relevant knowledge, matching, is known to be a performance bottleneck in production systems. PSM-E is a C-based implementation of the OPS5 production system on the Encore Multimax that has achieved significant speedups in matching. In this paper we describe our implementation, Soar/PSM-E, of Soar on the Encore Multimax that is built on top of PSM-E. We first describe the extensions and modifications required to PSM-E in order to support Soar, especially the capability of adding productions at run time as required by chunking. We present the speedups obtained on Soar/PSM-E and discuss some effects of chunking on parallelism. We also analyze the performance of the system and identify the bottlenecks limiting parallelism. Finally, we discuss the work in progress to deal with some of them."
1987,SOAR: An Architecture for General Intelligence.,"The ultimate goal of work in cognitive architecture is to provide the foundation for a system capable of general intelligent behavior. That is, the goal is to provide the underlying structure that would enable a system to perform the full range of cognitive tasks, employ the full range of problem solving methods and representations appropriate for the tasks, and learn about all aspects of the tasks and its performance on them. In this article we present SOAR, an implemented proposal for such an architecture. We describe its organizational principles, the system as currently implemented, and demonstrations of its capabilities.
"
1987,Knowledge Level Learning in Soar.,"In this article we demonstrate how knowledge level learning can be performed within the Soar architecture. That is, we demonstrate how Soar can acquire new knowledge that is not deductively implied by its existing knowledge. This demonstration employs Soarís chunking mechanism-a mechanism which acquires new productions from goal-baaed experience-as its only learning mechanism. Chunking has previously been demonstrated to be a useful symbol level learning mechanism, able to speed up the performance of existing systems, but this is the first demonstration of its ability to perform knowledge level learning. Two simple declarative-memory tasks are employed for this demonstration: recognition and recall.
"
1986,Information Processing Language V on the IBM 650.,n/a
1986,Straightening Out Softening Up: Response to Carroll and Campbell.,"Carroll and Campbell have exercised themselves over a straw man not subscribed to by us. In the process, our position has not been accurately represented. In reply, we restate as clearly as we can the position for which we actually did and do argue. The underlying issue seems to concern the advantages of using technical psychological theories to identify underlying mechanisms in human-computer interaction. We argue that such theories are an important part of a science of human-computer interaction. We argue further that technical theories must be considered in the context of the uses to which they are put. The use of a theory helps determine what is a good approximation, the degree of formalization that is justified, and the appropriate commingling of qualitative and quantitative techniques. Technical theories encourage cumulative progress by abetting the classical scientific heuristic of divide and conquer.
"
1986,Chunking in Soar: The Anatomy of a General Learning Mechanism.,"Abstract
In this article we describe an approach to the construction of a general learning mechanism based on chunking in Soar. Chunking is a learning mechanism that acquires rules from goal-based experience. Soar is a general problem-solving architecture with a rule-based memory. In previous work we have demonstrated how the combination of chunking and Soar could acquire search-control knowledge (strategy acquisition) and operator implementation rules in both search-based puzzle tasks and knowledge-based expert-systems tasks. In this work we examine the anatomy of chunking in Soar and provide a new demonstration of its learning capabilities involving the acquisition and use of macro-operators."
1986,Parallel Algorithms and Architectures for Rule-Based Systems.,"Rule-based systems, on the surface, appear to be capable of exploiting large amounts of parallelism‚Äîit is possible to match each rule to the data memory in parallel. In practice, however, we show that the speed-up from parallelism is quite limited, less than 10-fold. The reasons for the small speed-up are: (1) the small number of rules relevant to each change to data memory; (2) the large variation in the processing required by the relevant rules; and (3) the small number of changes made to data memory between synchronization steps. Furthermore, we observe that to obtain this limited factor of 10-fold speed-up, it is necessary to exploit parallelism at a very fine granularity. We propose that a suitable architecture to exploit such fine-grain parallelism is a bus-based shared-memory multiprocessor with 32-64 processors. Using such a multiprocessor (with individual processors working at 2 MIPS), it is possible to obtain execution speeds of about 3800 rule-firings/sec. This speed is significantly higher than that obtained by other proposed parallel implementations of rule-based systems."
1985,The Prospects for Psychological Science in Human-Computer Interaction.,"This paper discusses the prospects of psychology playing a significant role in the progress of human-computer interaction. In any field, hard science (science that is mathematical or otherwise technical) has a tendency to drive out softer sciences, even if the softer sciences have important contributions to make. It is possible that, as computer science and artificial intelligence contributions to human-computer interaction mature, this could happen to psychology. It is suggested that this trend might be prevented by hardening the applicable psychological science. This approach, however, has been criticized on the grounds that the resulting body of knowledge would be too low level, too limited in scope, too late to affect computer technology, and too difficult to apply. The prospects for overcoming each of these obstacles are analyzed here.
"
1985,R1-Soar: An Experiment in Knowledge-Intensive Programming in a Problem-Solving Architecture.,"Abstract:
This paper presents an experiment in knowledge-intensive programming within a general problem-solving production-system architecture called Soar. In Soar, knowledge is encoded within a set of problem spaces, which yields a system capable of reasoning from first principles. Expertise consists of additional rules that guide complex problem-space searches and substitute for expensive problem-space operators. The resulting system uses both knowledge and search when relevant. Expertise knowledge is acquired either by having it programmed, or by a chunking mechanism that automatically learns new rules reflecting the results implicit in the knowledge of the problem spaces. The approach is demonstrated on the computer-system configuration task, the task performed by the expert system R1."
1984,Introduction to the COMTEX Microfiche Edition of Reports on Artificial Intelligence from Carnegie-Mellon University.,"Originally it was Complex Information Processing. That was the name Herb Simon and I chose in 1956 to describe the area in which we are working. It didn't take long before it became Artificial Intelligence (AI). Coined by John McCarthy, that term has stuck firmly, despite continual grumblings that any other name would be twice as fair (though no grumblings by me; I like the present name). Complex Information processing lives on now only in the title of the CIP Working Papers, a series started by Herb Simon in 1956 and still accumulating entries (to 447). However, from about 1965 much of the work on artificial intelligence that was not related to psychology began to appear in technical reports of the Computer Science Department. These reports, never part of a coherent numbered series until 1978, proliferated in all directions. Starting in the early 1970s (on one can recall exactly when), they did become the subject of a general mailing and thus began to form what everyone thinks of as the CMU Computer Science Technical Reports."
1984,Problem solving techniques for the design of algorithms.,"By studying the problem-solving techniques that people use to design algorithms we can learn something about building systems that automatically derive algorithms or assist human designers. In this paper we present a model of algorithm design based on our analysis of the protocols of two subjects designing three convex hull algorithms. The subjects work mainly in a data-flow problem space in which the objects are representations of partially specified algorithms. A small number of general-purpose operators construct and modify the representations; these operators are adapted to the current problem state by means-ends analysis. The problem space also includes knowledge-rich schemas such as divide and conquer that subjects incorporate into their algorithms. A particularly versatile problem-solving method in this problem space is symbolic execution, which can be used to refine, verify, or explain components of an algorithm. The subjects also work in a task-domain space about geometry. The interplay between problem solving in the two spaces makes possible the process of discovery. We have observed that the time a subject takes to design an algorithm is proportional to the number of components in the algorithm's data-flow representation. Finally, the details of the problem spaces provide a model for building a robust automated system.
"
1984,Initial Assessment of Architectures for Production Systems.,"Although production systems are appropriate for many applications in the artificial intelligence and expert systems areas, there are applications for which they are not fast enough to be used. If they are to be used for very large problems with severe time constraints, speed increases are essential. Recognizing that substantial further increases are not likely to be achieved through software techniques, the PSM project has begun investigating the use of hardware support for production system interpreters. The first task undertaken in the project was to attempt to understand the space of architectural possibilities and the trade-offs involved. This articlc presents the initial findings of the project. Briefly, the preliminary results indicate that the most attractive architecture for production systems is a machine containing a small number of very simple and very fast processors.
"
1984,Towards Chunking as a General Learning Mechanism.,"The power law of practice states that performance on a task improves as a power-law function of the number of times the task has been performed. In this article we describe recent work on a model of this effect. The model, called the chunking theory of learning, is based on the notion of chunking. A limited version of this model has been implemented within the Xaps2 production system architecture. When it is applied to a 1023-choice reaction-time task (encoded as a set of productions), task performance is improved (measured in terms of the number of production system cycles). Moreover, the practice curves are power law in form.
"
1983,An Automatic Algorithm Designer: An Initial Implementation.,"This paper outlines a specification for an algorithm-design system (based on previous work involving protocol analysis) and describes an implementation of the specification that is a combination frame and production system. In the implementation, design occurs in two problem spaces -- one about algorithms and one about the task-domain. The partially worked out algorithms are represented as configurations of data-flow components. A small number of general-purpose operators construct and modify the representations. These operators are adapted to different situations by instantiation and means-ends analysis rules. The data-flow space also includes symbolic and test-case execution rules that drive the component-refinement orocess by exposing both problems and opportunities. A domain space about geometric images supports test,case execution, domain-specific problem solving, recognition and discovery.
"
1983,A Universal Weak Method: Summary of Results.,"The weak methods occur pervasively in AI systems and may form the basic methods for all intelligent systems. The purpose of this paper is to characterize the weak methods and to explain how and why they arise in intelligent systems. We propose an organization, called a universal weak method, that provides functionality of all the weak methods. A universal weak method is an organizational scheme for knowledge that produces the appropriate search behaviour given the available task-domain knowledge. We present a problem solving architecture in which we realize a universal weak method. We also demonstrate the universal weak method with a variety of weak methods on a set of tasks."
1982,Learning by Chunking: Summary of a Task and a Model.,"A gridded electron tube suitable for generating high power at high frequency, has a cathode surface composed of a set of parallel emitting strips separated by non-emitting strips. Massive grid bars are aligned over the non-emitting strips, thereby reducing grid current interception and providing thermal dissipation. Attached to the side of the grid bars facing the cathode is an array of closely spaced fine wires which increase the electric field due to the grid at the emitting surface, and decrease the penetration of field due to the anode, thereby increasing the transconductance and amplification factor, decreasing the transit time of electrons, and improving the uniformity of emission current density."
1982,Naive Algorithm Design Techniques: A Case Study.,"By studying how people design algorithms, we can learn something about building systems that automatically derive algorithms or assist human designers. We present the results of a protocol analysis of one subject designing a convex hull algorithm. We conclude that the subject works mainly in a data flow problem space that includes representations of ambiguous, partially specified algorithms, a variety of search control methods, and a few general purpose operators with a powerful matching process to make the operators applicable to the current problem state. The details of the problem space provide a model for building a robust automated system"
1980,The Keystroke-Level Model for User Performance Time with Interactice Systems.,"A gridded electron tube suitable for generating high power at high frequency, has a cathode surface composed of a set of parallel emitting strips separated by non-emitting strips. Massive grid bars are aligned over the non-emitting strips, thereby reducing grid current interception and providing thermal dissipation. Attached to the side of the grid bars facing the cathode is an array of closely spaced fine wires which increase the electric field due to the grid at the emitting surface, and decrease the penetration of field due to the anode, thereby increasing the transconductance and amplification factor, decreasing the transit time of electrons, and improving the uniformity of emission current density."
1980,Physical Symbol Systems.,"On the occasion of a first conference on Cognitive Science, it seems appropriate to review the basis of common understanding between the various disciplines. In my estimate, the most fundamental contribution so far of artificial intelligence and computer science to the joint enterprise of cognitive science has been the notion of a physical symbol system, i.e., the concept of a broad class of systems capable of having and manipulating symbols, yet realizable in the physical universe. The notion of symbol so defined is internal to this concept, so it becomes a hypothesis that this notion of symbols includes the symbols that we humans use every day of our lives. In this paper we attempt systematically, but plainly, to lay out the nature of physical symbol systems. Such a review is in ways familiar, but not thereby useless. Restatement of fundamentals is an important exercise.
"
1977,Speech Understanding Systems.,"A five-year interdisciplinary effort by speech scientists and computer scientists has demonstrated the feasibility of programming a computer system to ìunderstandî connected speech, i.e., translate it into operational form and respond accordingly. An operational system (HARPY) accepts speech from five speakers, interprets a 1000-word vocabulary, and attains 91 percent sentence accuracy. This Steering Committee summary report describes the project history, problem, goals, and results.
"
1977,An instructable production system: basic design issues.,"The full advantages of the incremental properties of production systems have yet to be exploited on a large scale. A promising vehicle for this is the task of instructing a system to solve problems in a complex domain. For this, it is important to express the instruction in a language similar to natural language and without detailed knowledge of the inner structure of the system. Instruction and close interaction with the system as it behaves are preferred over a longer feedback loop with more independent learning by the system. The domain is initially an abstract job shop. The beginning system has capabilities for solving problems,, processing language, building productions, and interacting with the task environment. All parts of the system are subject to instruction. The main problem-solving strategy, which permeates all four system components, is based on means ends analysis and goal-subgoal search. This is coupled with an explicit representation of control knowledge. The system's behavior so far is restricted to simple environmental manipulations, a number of which must be taught before more complex tasks can be done."
1977,The efficiency of certain production system implementations.,"The obvious method of determining which productions are satisfied on a given cycle involves matching productions, one at a time, against the contents of working memory. The cost of this processing is essentially linear in the product of the number of productions in production memory and the number of assertions in working memory. By augmenting a production system architecture with a mechanism that enables knowledge of similarities among productions to be precomputed and then exploited during a run, it is possible to eliminate the dependency on the size of production memory. If in addition, the architecture is augmented with a mechanism that enables knowledge of the degree to which each production is currently satisfied to be maintained across cycles, then the dependency on the size of working memory can be eliminated as well. After a particular production system architecture, PSG, is described, two sets of mechanisms that increase its efficiency are presented. To determine their effectiveness, two augmented versions of PSG are compared experimentally with each other and with the original version."
1977,Speech Understanding and AI/AI and Speech Understanding.,"This panel will review research in speech understanding and in artificial intelligence from two perspectives: the contributions that AI has made to speech understanding - the resources in AI that have been used in the development of speech understanding systems. the contributions that speech understanding has made to AI - the results of the speech understanding program that have been affected or are likely to affect future AI research. Four topics are identified for major consideration: multiple sources of knowledge which are required; how should they be organized, system control how to manage the complex interactions involved. Language understanding comparisons of text and speech input. Organization of research-creating complex, multisource, knowledge-based systems "
1977,Knowledge Representation.,n/a
1976,Computer Science as Empirical Inquiry: Symbols and Search.,"Computer science is the study of the phenomena surrounding computers. The founders of this society understood this very well when they called themselves the Association for Computing Machinery. The machine‚Äînot just the hardware, but the programmed, living machine‚Äîis the organism we study."
1976,PAS-II: An Interactive Task-Free Version of an Automatic Protocol Analysis System.,"Abstract:
PAS-II, a computer program which represents a generalized version of an automatic protocol system (PAS-I) is described. PAS-II is a task-free, interactive, modular data analysis sis system for inferring the information processes used by a human from his verbal behavior while solving a problem. The output of the program is a problem behavior graph: a description of the subject's changing knowledge state during problem solving. As an example of system operation the PAS-II analysis of a short cryptarithmetic protocol is presented."
1973,PAS-II: An Interactive Task-Free Version of an Automatic Protocol Analysis System.,"Abstract:
PAS-II, a computer program which represents a generalized version of an automatic protocol system (PAS-I) is described. PAS-II is a task-free, interactive, modular data analysis sis system for inferring the information processes used by a human from his verbal behavior while solving a problem. The output of the program is a problem behavior graph: a description of the subject's changing knowledge state during problem solving. As an example of system operation the PAS-II analysis of a short cryptarithmetic protocol is presented."
1971,Protocol Analysis as a Task for Artificial Intelligence.,"We are attempting to automate protocol analysis, which is a form of data analysis in psychology for inferring the information processes used by a human from his verbal behaviour while solving a problem. The paper discusses protocol analysis as a task in artificial intelligence. The discussion is based on (but broader than) our current program, PAS-I, which creates a description of a subject's changing knowledge state from his verbal behaviour."
1971,Possibilities for computer structures 1971.,"What computer structures come into existence in a given epoch depends on the confluence of several factors:
The underlying technology---its speed, cost, reliability, etc.
The structures that have actually been conceived.
The demand for computer systems (in terms of both economics and user influence)."
1971,Protocol Analysis as a Task for Artificial Intelligence.,"We are attempting to automate protocol analysis, which is a form of data analysis in psychology for inferring the information processes used by a human from his verbal behaviour while solving a problem. The paper discusses protocol analysis as a task in artificial intelligence. The discussion is based on (but broader than) our current program, PAS-I, which creates a description of a subject's changing knowledge state from his verbal behaviour."
1971,A Model for Functional Reasoning in Design.,"A model of the design process is developed in two stages, corresponding to the task environment of design and the activity of posing and solving design problems. Use of the model with top-down and bottom-up disciplines is discussed. An example of the design of an object using a semi-automated design system based on the model is presented. Several issues raised by the model's qualitative aspects, its suitability to automated design, and lines for further development are discussed."
1970,The PMS and ISP descriptive systems for computer structures.,"In this paper we propose two notations for describing aspects of computer systems that currently are handled by a melange of informal notations. These two notations emerged as a by-product of our efforts to produce a book on computer structures (Bell and Newell, 1970). Since we feel it is slightly unusual to present notations per se, outside of the context of particular design or analysis efforts that use them, it is appropriate to relate some background history."
1967,Some issues of representation in a general problem solver.,"The research reported here is an investigation into the development of a computer program with general problem solving capabilities. This investigation involved the construction of one such computer program called the General Problem Solver (GPS, although more properly GPS-2-6) which was accomplished by modifying an existing program conceived in 1957 by A. Newell, J. C. Shaw, and H. A. Simon."
1965,QUICKSCRIPT - a SIMSCRIPT: like language for the G-20.,"QUIKSCRIPT is a simulation language based on SIMSCRIPT and programmed entirely in an algebraic language, 20GATE. The QUIKSCRIPT language, its internal implementation, and major differences between QUIKSCRIPT and SIMSCRIPT are presented. This paper is not a programming guide to the language, but rather an attempt to present its flavor. A brief description of SIMSCRIPT is included, as is a sufficient description of 20-GATE to render this material understandable to the reader familiar with algebraic languages"
1963,Documentation of IPL-V.,"IPL-V (Information Processing Language-V) is a programming language for list processing and symbol manipulation. It is the fifth of a series of programming languages that has developed as part of a research effort in artificial intelligence and simulation of cognitive processes. This research started in late 1954 at The RAND Corporation and Carnegie Institute of Technology and has remained centered there, so that these two organizations, or more properly the scientists thereof, can be considered the source of the language. The earlier IPL's were coded for the RAND JOHNNIAC, a unique machine of the Princeton class; IPL-V is the only one which has become a ‚Äúpublic‚Äù language and where the necessary effort has been made to document and standardize the language."
1962,"Learning, Generality and Problem Solving.","This memorandum is a discussion of the concept of learning in the field of artificial intelligence and its intimate relationship to other concepts such as generality and problem solving. It is part of a continuing RAND research effort in these areas. The long range goals of artificial intelligence imply the ability for programs to be truly general purpose, in the sense of being able to acquire from their environment the information necessary to develop successfully in ways not envisioned in detail by their designers. The analysis given here shows that learning is generally viewed as the means to accomplish this. These issues are fundamental not only to the field of pure artificial intelligence, but to the whole attempt to develop more sophisticated information processes, such as in command and control systems. To illustrate some of the questions brought up in the paper, a discussion is given of some very recent work done at RAND on a learning scheme for the General Problem Solver (GPS). More details and specifications of this scheme will be given in a subsequent RAND memorandum. "
1960,An Introduction to Information Processing Language V.,"This paper is an informal introduction to Information Processing Language V (IPL-V), a symbol and list-structure manipulating language presently implemented on the IBM 650, 704 and 709. It contains a discussion of the problem context in which a series of Information Processing Languages has developed and of the basic concepts incorporated in IPL-V. A complete description of the language can be found in the IPL-V Programmer's manual. "
1959,Report on a general problem-solving program.,"This paper reports on a computer program, called GPS-I for General Problem Solving Program I. Construction and investigation of this program is part of a research effort by the authors to understand the information processes that underlie human intellectual, adaptive, and creative abilities. The approach is synthetic - to construct computer programs that can solve problems requiring intelligence and adaptation, and to discover which varieties of these programs can be matched to data on human problem solving. GPS - I grew out of an earlier program, the Logic Theorist, which discovers proofs to theorems in the sentential calculus. GPS-I is an attempt to fit the recorded behavior of college students trying to discover proofs. The purpose of this paper is not to relate the program to human behaviour, but to describe its main characteristics and to assess its capacities as a problem-solving mechanism. The paper will present enough theoretical discussion of problem-solving activity so that the program can be seen as an attempt to advance our basic knowledge of intellectual activity. The program will be assessed from this point of view, rather than whether it offers an economical solution to a significant class of problems. The major features of the program that are worthy of discussion are: the recursive nature of its prolem-solving activity, the separation of problem content from problem-solving technique as a way of increasing the generality of the program, the two general problem-solving techniques that now constitute its repertoire: means-ends analysis, and planning. The memory and program organization used to mechanize the program (this will be noted only briefly, since there will be no space to describe the computer languages (IPL's) used to code GPS-I). Examples will be given of how GPS solves problems in the areas of elementary symbolic logic and elementary algebra."
1958,Chess-Playing Programs and the Problem of Complexity.,"Abstract:
This paper traces the development of digital computer programs that play chess. The work of Shannon, Turing, the Los Alamos group, Bernstein, and the authors is treated in turn. The efforts to program chess provide an indication of current progress in understanding and constructing complex and intelligent mechanisms."
1956,The logic theory machine-A complex information processing system.,"Abstract:
In this paper we describe a complex information processing system, which we call the logic theory machine, that is capable of discovering proofs for theorems in symbolic logic. This system, in contrast to the systematic algorithms that are ordinarily employed in computation, relies heavily on heuristic methods similar to those that have been observed in . human problem solving activity. The specification is written in a formal language, of the nature of a pseudo-code, that is suitable for coding for digital computers. However, the present paper is concerned exclusively with specification of the system, and not with its realization in a computer. The logic theory machine is part of a program of research to understand complex information processing systems by specifying and synthesizing a substantial variety of such systems for empirical study."
