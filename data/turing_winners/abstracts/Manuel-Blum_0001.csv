2017,Towards Human Computable Passwords.,"An interesting challenge for the cryptography community is to design authentication protocols that are so simple that a human can execute them without relying on a fully trusted computer. We propose several candidate authentication protocols for a setting in which the human user can only receive assistance from a semi-trusted computer - a computer that stores information and performs computations correctly but does not provide confidentiality. Our schemes use a semi-trusted computer to store and display public challenges C_i\in[n]^k. The human user memorizes a random secret mapping \sigma:[n]\rightarrow \mathbb{Z}_d and authenticates by computing responses f(\sigma(C_i)) to a sequence of public challenges where f:\mathbb{Z}_d^k\rightarrow \mathbb{Z}_d is a function that is easy for the human to evaluate. We prove that any statistical adversary needs to sample m=\tilde{\Omega}\paren{n^{s(f)}} challenge-response pairs to recover \sigma, for a security parameter s(f) that depends on two key properties of f. Our lower bound generalizes recent results of Feldman et al. [Feldman'15] who proved analogous results for the special case d=2. To obtain our results, we apply the general hypercontractivity theorem [O'Donnell'14] to lower bound the statistical dimension of the distribution over challenge-response pairs induced by f and \sigma. Our statistical dimension lower bounds apply to arbitrary functions f:\mathbb{Z}_d^k\rightarrow \mathbb{Z}_d (not just to functions that are easy for a human to evaluate). As an application, we propose a family of human computable password functions f_{k_1,k_2} in which the user needs to perform 2k_1+2k_2+1 primitive operations (e.g., adding two digits or remembering a secret value \sigma(i)), and we show that s(f) = \min{k_1+1, (k_2+1)/2}. For these schemes, we prove that forging passwords is equivalent to recovering the secret mapping. Thus, our human computable password schemes can maintain strong security guarantees even after an adversary has observed the user login to many different accounts."
2016,Cybernetics: A mathematician of mind.,n/a
2015,Publishable Humanly Usable Secure Password Creation Schemas.,"What can a human compute in his/her head that a powerful adversary cannot infer? To answer this question, we define a model of human computation and a measure of security. Then, motivated by the special case of password creation, we propose a collection of well-defined password-generation methods. We show that our password generation methods are humanly computable and, to a well-defined extent, machine uncrackable. For the proof of security, we posit that password generation methods are public, but that the humanís privately chosen seed is not, and that the adversary will have observed only a few input-output pairs. Besides the application to password generation, our proposed Human Usability Model (HUM) will have other applications."
2013,Naturally Rehearsing Passwords.,"Abstract
We introduce quantitative usability and security models to guide the design of password management schemes ‚Äî systematic strategies to help users create and remember multiple passwords. In the same way that security proofs in cryptography are based on complexity-theoretic assumptions (e.g., hardness of factoring and discrete logarithm), we quantify usability by introducing usability assumptions. In particular, password management relies on assumptions about human memory, e.g., that a user who follows a particular rehearsal schedule will successfully maintain the corresponding memory. These assumptions are informed by research in cognitive science and can be tested empirically. Given rehearsal requirements and a user‚Äôs visitation schedule for each account, we use the total number of extra rehearsals that the user would have to do to remember all of his passwords as a measure of the usability of the password scheme. Our usability model leads us to a key observation: password reuse benefits users not only by reducing the number of passwords that the user has to memorize, but more importantly by increasing the natural rehearsal rate for each password. We also present a security model which accounts for the complexity of password management with multiple accounts and associated threats, including online, offline, and plaintext password leak attacks. Observing that current password management schemes are either insecure or unusable, we present Shared Cues ‚Äî a new scheme in which the underlying secret is strategically shared across accounts to ensure that most rehearsal requirements are satisfied naturally while simultaneously providing strong security. The construction uses the Chinese Remainder Theorem to achieve these competing goals."
2013,GOTCHA password hackers!,"We introduce GOTCHAs (Generating panOptic Turing Tests to Tell Computers and Humans Apart) as a way of preventing automated offline dictionary attacks against user selected passwords. A GOTCHA is a randomized puzzle generation protocol, which involves interaction between a computer and a human. Informally, a GOTCHA should satisfy two key properties: (1) The puzzles are easy for the human to solve. (2) The puzzles are hard for a computer to solve even if it has the random bits used by the computer to generate the final puzzle --- unlike a CAPTCHA [44]. Our main theorem demonstrates that GOTCHAs can be used to mitigate the threat of offline dictionary attacks against passwords by ensuring that a password cracker must receive constant feedback from a human being while mounting an attack. Finally, we provide a candidate construction of GOTCHAs based on Inkblot images. Our construction relies on the usability assumption that users can recognize the phrases that they originally used to describe each Inkblot image --- a much weaker usability assumption than previous password systems based on Inkblots which required users to recall their phrase exactly. We conduct a user study to evaluate the usability of our GOTCHA construction. We also generate a GOTCHA challenge where we encourage artificial intelligence and security researchers to try to crack several passwords protected with our scheme."
2010,Understanding and Inductive Inference.,"Abstract
This talk will be about different kinds of understanding, including especially that provided by deductive and inductive inference. Examples will be drawn from Galileo, Kepler, and Fermat. Applications will be given to the physicist‚Äôs problem of inferring the laws of nature, and to the restricted case of inferring sequences in Sloane‚Äôs Encyclopedia of Integer Sequences.
While there is much theory and understanding of deductive inference, there is relatively little understanding of the process of inductive inference. Inductive inference is about hypothesizing, hopefully inferring, the laws of physics from observation of the data. It is what Kepler did in coming up with his three laws of planetary motion.
Once laws are in place, deductive inference can be used to derive their consequences, which then uncovers much of the understanding in physics.
This talk will define inductive inference, state some of its open problems, and give applications to a relatively simpler but still largely unexplored task, namely, inferring a large natural class of algorithmically generated integer sequences that includes (as of this writing) 20in Sloane‚Äôs Encyclopedia of integer sequences."
2007,Improving Image Search with PHETCH.,"Abstract:
Keyword-based image search engines are hindered by the lack of proper labeling for images in their indices. In many cases the labels do not agree with the contents of the image itself, since images are generally indexed by their filename and the surrounding text in the Web page. Another popular approach to image search, content based image retrieval, suffers from a gap between the available low level data and the semantic needs of user searches. To overcome these problems we suggest human annotation of images with natural language descriptions. To this end we present Phetch, an engaging multiplayer game that allows people to attach accurate explanatory text captions to arbitrary images on the Web. People play the game because it is fun, and as a side effect we collect valuable information that can be applied towards improving image retrieval. Furthermore, the game can also be used for other novel applications."
2006,Peekaboom: a game for locating objects in images.,"We introduce Peekaboom, an entertaining web-based game that can help computers locate objects in images. People play the game because of its entertainment value, and as a side effect of them playing, we collect valuable image metadata, such as which pixels belong to which object in the image. The collected data could be applied towards constructing more accurate computer vision algorithms, which require massive amounts of training and testing data not currently available. Peekaboom has been played by thousands of people, some of whom have spent over 12 hours a day playing, and thus far has generated millions of data points. In addition to its purely utilitarian aspect, Peekaboom is an example of a new, emerging class of games, which not only bring people together for leisure purposes, but also exist to improve artificial intelligence. Such games appeal to a general audience, while providing answers to problems that computers cannot yet solve."
2006,Verbosity: a game for collecting common-sense facts.,"We address the problem of collecting a database of """"common-sense facts"""" using a computer game. Informally, a common-sense fact is a true statement about the world that is known to most humans: """"milk is white,"""" """"touching hot metal hurts,"""" etc. Several efforts have been devoted to collecting common-sense knowledge for the purpose of making computer programs more intelligent. Such efforts, however, have not succeeded in amassing enough data because the manual process of entering these facts is tedious. We therefore introduce Verbosity, a novel interactive system in the form of an enjoyable game. People play Verbosity because it is fun, and as a side effect of them playing, we collect accurate common-sense knowledge. Verbosity is an example of a game that not only brings people together for leisure, but also collects useful data for computer science."
2006,Improving accessibility of the web with a computer game.,"Images on the Web present a major accessibility issue for the visually impaired, mainly because the majority of them do not have proper captions. This paper addresses the problem of attaching proper explanatory text descriptions to arbitrary images on the Web. To this end, we introduce Phetch, an enjoyable computer game that collects explanatory descriptions of images. People play the game because it is fun, and as a side effect of game play we collect valuable information. Given any image from the World Wide Web, Phetch can output a correct annotation for it. The collected data can be applied towards significantly improving Web accessibility. In addition to improving accessibility, Phetch is an example of a new class of games that provide entertainment in exchange for human processing power. In essence, we solve a typical computer vision problem with HCI tools alone."
2004,Telling humans and computers apart automatically.,How lazy cryptographers do AI.
2003,CAPTCHA: Using Hard AI Problems for Security.,"Abstract
We introduce captcha, an automated test that humans can pass, but current computer programs can‚Äôt pass: any program that has high success over a captcha can be used to solve an unsolved Artificial Intelligence (AI) problem. We provide several novel constructions of captchas. Since captchas have many applications in practical security, our approach introduces a new class of hard problems that can be exploited for security purposes. Much like research in cryptography has had a positive impact on algorithms for factoring and discrete log, we hope that the use of hard AI problems for security purposes allows us to advance the field of Artificial Intelligence. We introduce two families of AI problems that can be used to construct captchas and we show that solutions to such problems can be used for steganographic communication. CAPTCHAs based on these AI problem families, then, imply a win-win situation: either the problems remain unsolved and there is a way to differentiate humans from computers, or the problems are solved and there is a way to communicate covertly on some channels."
2001,Secure Human Identification Protocols.,"Abstract
One interesting and important challenge for the cryptologic community is that of providing secure authentication and identification for unassisted humans. There are a range of protocols for secure identification which require various forms of trusted hardware or software, aimed at protecting privacy and financial assets. But how do we verify our identity, securely, when we don‚Äôt have or don‚Äôt trust our smart card, palmtop, or laptop?
In this paper, we provide definitions of what we believe to be reasonable goals for secure human identification. We demonstrate that existing solutions do not meet these reasonable definitions. Finally, we provide solutions which demonstrate the feasibility of the security conditions attached to our definitions, but which are impractical for use by humans."
1997,Software reliability via run-time result-checking.,"We review the field of result-checking, discussing simple checkers and self-correctors. We argue that such checkers could profitably be incorporated in software as an aid to efficient debugging and enhanced reliability. We consider how to modify traditional checking methodologies to make them more appropriate for use in real-time, real-number computer systems. In particular, we suggest that checkers should be allowed to use stored randomness: that is, that they should be allowed to generate, preprocess, and store random bits prior to run-time, and then to use this information repeatedly in a series of run-time checks. In a case study of checking a general real-number linear transformation (e.g., a Fourier Transform), we present a simple checker which uses stored randomness, and a self-corrector which is particularly efficient if stored randomness is employed."
1997,Program Error Detection/Correction: Turning PAC Learning into PERFECT Learning (Abstract).,"Abstract
Computational learning theory is concerned with developing ‚Äúscratch‚Äù a probably approximately correct (PAC) algorithm to solve a given computational problem. Program error detection /correction is concerned with transforming programs ‚Äî such as these ‚Äî which are correct on most instances into programs that are correct on all instances. The two approaches together enable one to generate a perfect program from scratch. The goal of this talk is to describe how this latter error detection/correction of algorithms works, and to encourage its integration into learning theory."
1996,Reflections on the Pentium Bug.,"Abstract:
We review the field of result-checking and suggest that it be extended to a methodology for enforcing hardware/software reliability. We thereby formulate a vision for ""self-monitoring"" hardware/software whose reliability is augmented through embedded suites of run-time correctness checkers. In particular, we suggest that embedded checkers and correctors may be employed to safeguard against arithmetic errors such as that which has bedeviled the Intel Pentium Microprocessor. We specify checkers and correctors suitable for monitoring the multiplication and division functionalities of an arbitrary arithmetic processor and seamlessly correcting erroneous output which may occur for any reason during the lifetime of the chip."
1995,on the Problem of Sorting Burnt Pancakes.,"Abstract
The ‚Äúpancake problem‚Äù is a well-known open combinatorial problem that recently has been shown to have applications to parallel processing. Given a stack of n pancakes in arbitrary order, all of different sizes, the goal is to sort them into the size-ordered configuration having the largest pancake on the bottom and the smallest on top. The allowed sorting operation is a ‚Äúspatula flip‚Äù, in which a spatula is inserted beneath any pancake, and all pancakes above the spatula are lifted and replaced in reverse order. The problem is to bound f(n), the minimum number of flips required in the worst case to sort a stack of n pancakes. Equivalently, we seek bounds on the number of prefix reversals necessary to sort a list of n elements. Bounds of
and
were shown by Gates and Papadimitriou in 1979. In this paper, we consider a traditional variation of the problem in which the pancakes are two sided (one side is ‚Äúburnt‚Äù), and must be sorted to the size-ordered configuration in which every pancake has its burnt side down. Let g(n) be the number of flips required to sort n ‚Äúburnt pancakes‚Äù. We find that
, where the upper bound holds for n ‚©æ 10. We consider the conjecture that the most difficult case for sorting n burnt pancakes is ‚àíIn, the configuration having the pancakes in proper size order, but in which each individual pancake is upside down. We present an algorithm for sorting ‚àíIn in
flips, where c is a small constant, thereby establishing a bound of
under the conjecture. Furthermore, the longstanding upper bound of f(n) is also improved to
under the conjecture."
1995,Designing Programs that Check Their Work.,"A program correctness checker is an algorithm for checking the output of a computation. That is, given a program and an instance on which the program is run, the checker certifies whether the output of the program on that instance is correct. This paper defines the concept of a program checker. It designs program checkers for a few specific and carefully chosen problems in the class FP of functions computable in polynomial time. Problems in FP for which checkers are presented in this paper include Sorting, Matrix Rank and GCD. It also applies methods of modern cryptography, especially the idea of a probabilistic interactive proof, to the design of program checkers for group theoretic computations.Two structural theorems are proven here. One is a characterization of problems that can be checked. The other theorem establishes equivalence classes of problems such that whenever one problem in a class is checkable, all problems in the class are checkable."
1995,Contributions of theoretical computer science.,n/a
1995,Self-Correcting for Function Fields Transcendental Degree.,"Abstract
We use algebraic field extension theory to find self-correctors for a broad class of functions. Many functions whose translations are contained in a function field that is a finite degree extension of a scalar field satisfy polynomial identities that can be transformed into self-correctors. These functions can be efficiently corrected in a way that is simpler and different from how the functions are actually computed. This is an essential feature of program self-correcting. Among the functions for which we present self-correctors are many rational expressions of x,e x , and sin(x) (over the real and complex fields) as weD as many rational expressions of x, g x (g a generator) mapping the integers into a finite field and many rational expressions of x,log h (x) (h a generator) mapping a finite field into the reals.
The new tools presented in this extended abstract will be useful to the theory of program self testing/correcting. Furthermore, they may yield new results in complexity theory. Previous work in the self-testing of polynomials had important applications in the PCP protocols that proved the hardness of approximating max-SNP problems."
1994,Checking the Correctness of Memories.,"Abstract
We extend the notion of program checking to include programs which alter their environment. In particular, we consider programs which store and retrieve data from memory. The model we consider allows the checker a small amount of reliable memory. The checker is presented with a sequence of requests (on-line) to a data structure which must reside in a large but unreliable memory. We view the data structure as being controlled by an adversary. We want the checker to perform each operation in the input sequence using its reliable memory and the unreliable data structure so that any error in the operation of the structure will be detected by the checker with high probability.
We present checkers for various data structures. We prove lower bounds of logn on the amount of reliable memory needed by these checkers wheren is the size of the structure. The lower bounds are information theoretic and apply under various assumptions. We also show time-space tradeoffs for checking random access memories as a generalization of those for coherent functions."
1994,Program Result-Checking: A Theory of Testing Meets a Test of Theory.,"Abstract:
We review the field of result-checking, discussing simple checkers and self-correctors. We argue that such checkers could profitably be incorporated in software as an aid to efficient debugging and reliable functionality. We consider how to modify traditional checking methodologies to make them more appropriate for use in real-time, real-number computer systems. In particular, we suggest that checkers should be allowed to use stored randomness: i.e., that they should be allowed to generate, pre-process, and store random bits prior to run-time, and then to use this information repeatedly in a series of run-time checks. In a case study of checking a general real-number linear transformation (for example, a Fourier Transform), we present a simple checker which uses stored randomness, and a self-corrector which is particularly efficient if stored randomness is allowed.< >"
1994,Matching Nuts and Bolts.,"We describe a procedure which may be helpful to any disorganized carpenter who has a mixed pile of bolts and nuts and wants to find the corresponding pairs of bolts and nuts. The procedure uses our (and the carpenter's) ability to construct efficiently highly expanding graphs. The problem considered is given a collection of n bolts of distinct widths and n nuts such that there is a 1-1 correspondence between the nuts and bolts. The goal is to find for each bolt its corresponding nut by comparing nuts to bolts but not nuts to nuts or bolts to bolts. Our objective is to minimize the number of operations of this kind (as well as the total running time). The problem has a randomized algorithm similar to quicksort. Our main result is an n(log n)^O(1) time deterministic algorithm, based on expander graphs, for matching the bolts and the nuts."
1993,Self-Testing/Correcting with Applications to Numerical Problems.,"Abstract
Suppose someone gives us an extremely fast program P that we can call as a black box to compute a function f. Should we trust that P works correctly? A self-testing/correcting pair for f allows us to: (1) estimate the probability that P(x) ‚â† œÜ(x) when x is randomly chosen; (2) on any input x, compute f(x) correctly as long as P is not too faulty on average. Furthermore, both (1) and (2) take time only slightly more than the original running time of P. We present general techniques for constructing simple to program self-testing/correcting pairs for a variety of numerical functions, including integer multiplication, modular multiplication, matrix multiplication, inverting matrices, computing the determinant of a matrix, computing the rank of a matrix, integer division, modular exponentiation, and polynomial multiplication."
1993,Program Result Checking: A New Approach to Making Programs More Reliable.,"Abstract
Program result checking is concerned with designing programs to check their work. For example, after solving an equation for x, a result-checking program would substitute x back into the equation to make sure that the answer obtained is correct. There are many ways to check results, but there has been no theory to say what constitutes a good check. It is not a good check, for example, to redo a computation without change a second time. Such recomputation may uncover an intermittent hardware fault, but it will not uncover a software fault, and the discovery and elimination of software faults is the principal goal of this work. This talk discusses the concept of result checking, gives several examples, and outlines the basic theory."
1993,Designing Programs to Check Their Work (Abstract).,"Designing Programs to Check Their Work Professor Manuel Blurn Department of EECS UC Berkeley and International Computer Science Institute Berkeley, California Abstract Students, engineers, programmers... are all expected to check their work. Computer programs are not. There are several reasons for this: 1. Computer hardware almost never makes errors -- but that fails to recognize that programmers do! 2. Programs are hard enough to write without having to also write program checkers for them -- but that is the price of increased confidence! 3. There is no clear notion what constitutes a good checker. Indeed, the same students and engi-neers who are cautioned to check their work are rarely informed what it is that makes a proce-dure good for doing so -- but that is just the sort of problem that computer science should be able to solve! In my view, the lack of correctness checks in programs is an oversight. Programs have bugs that could perfectly well be caught by such checks. This talk urges that programs be written to check their work, and outlines a promising and rigorous approach to the study of this fascinating new area."
1993,Checking approximate computations over the reals.,"This paper provides the first systematic investigation of checking approximate numerical computations over subsets of the reals. In most cases, approximate checking is more challenging than exact checking. Problem conditioning, i.e., the measure of sensitivity of the output to slight changes in the input, and the presence of approximate ion parameters foil the direct transformation of many exact checkers to the approximate setting. Furthermore, approximate checking over the reals is complicated by the lack of nice finite field properties such as the existence of a samplable distribution which is invariant under addition or multiplication by a scalar. We overcome the above problems by using such techniques as testing and checking over similar but distinct distributions, using functionsí random and downward self-reducibility properties, and taking advantage of the small variance of the sum of independent identically distributed random variables. We provide approximate checkers for a variety of computations, including matrix multiplication, linear system solution, matrix inversion, and computation of the determinant. We also present an approximate version of Beigelís trick and extend the approximate linear self tester/corrector of [8] and the trigonometric selftester/corrector of [5] to more general computations. "
1992,Towards a Computational Theory of Statistical Tests (Extended Abstract).,"Abstract:
The authors initiate a computational theory of statistical tests. Loosely speaking, an algorithm is a statistical test if it rejects a 'negligible' fraction of strings. A statistical test is universal for a class of algorithms if it rejects all (but finitely many) of the strings rejected by each algorithm in the class. They consider the existence and efficiency of universal statistical tests for various classes of statistical tests. They also consider the relation between ensembles passing statistical tests of particular complexity and ensembles which are indistinguishable from uniform by algorithms of the same complexity. Some results refer to relatively simple statistical tests (e.g. those implemented by counter machines).< >"
1992,Universal Statistical Tests.,n/a
1991,Inductive Inference and Unsolvability.,"It is shown that many different problems have the same degree of unsolvability. Among these problems are:

The Inductive Inference Problem. Infer in the limit an index for a recursive function f presented as f(0), f(1),f(2),Ö.

The Recursive Index Problem. Decide in the limit if i is the index of a total recursive function.

The Zero Nonvariant Problem. Decide in the limit if a recursive function f presented as f(0), f(1), f(2),Ö has value unequal to zero for infinitely many arguments.

Finally, it is shown that these unsolvable problems are strictly easier than the halting problem."
1991,Noninteractive Zero-Knowledge.,"This paper investigates the possibility of disposing of interaction between prover and verifier in a zero-knowledge proof if they share beforehand a short random string.

Without any assumption, it is proven that noninteractive zero-knowledge proofs exist for some number-theoretic languages for which no efficient algorithm is known.

If deciding quadratic residuosity (modulo composite integers whose factorization is not known) is computationally hard, it is shown that the NP-complete language of satisfiability also possesses noninteractive zero-knowledge proofs.
"
1991,Checking the Correctness of Memories.,"Abstract:
The notion of program checking is extended to include programs that alter their environment, in particular, programs that store and retrieve data from memory. The model considered allows the checker a small amount of reliable memory. The checker is presented with a sequence of requests (online) to a data structure which must reside in a large but unreliable memory. The data structure is viewed as being controlled by an adversary. The checker is to perform each operation in the input sequence using its reliable memory and the unreliable data structure so that any error in the operation of the structure will be detected by the checker with high probability. Checkers for various data structures are presented. Lower bounds of log n on the amount of reliable memory needed by these checkers, where n is the size of the structure, are proved.< >"
1991,Program Checking.,n/a
1990,Self-Testing/Correcting with Applications to Numerical Problems.,
1989,Reversing Trains: A Turn of the Century Sorting Problem.,"Abstract
In this paper we study a reversing train puzzle proposed by Sam Loyd near the turn of the century. We concern ourselves with a version of this puzzle described most recently by A. K. Dewdney in Scientific American. There is a train, locomotive and n cars, that must be entirely reversed using only a short spur line attached to the main track. The efficiency of a solution is determined by summing, for all cars, the total distance moved by each car, where distance is measured in car lengths. We present an O(n2 log2 n) algorithm for accomplishing this task."
1989,Program Result Checking against Adaptive Programs and in Cryptographic Settings.,The theory of program result checking introduced in Blum allows one to check that a program P correctly computes the function f on input x The checker may use P s outputs on other inputs to help it check that P x f x In this setting P is always assumed to be a xed program whose output on input x is a function P x We extend the theory to check a program P which returns a result on input x that may depend on previous questions asked of P We call a checker that works for such a program an adaptive checker We consider the case where there is an adaptive program that supposedly computes f running on each of several noninteracting machines We design adaptive checkers that work for a constant number of independent and noninteracting programs We also consider the following cryptographic scenario A user wants to evaluate function f on input x using program P running on another machine As in checking the user does not trust the program to be correct The additional requirement is that the user wants to let the other machine know as little information as possible about x from the questions asked of the program P for example the user may want the program to be able to learn at most the input size as in Abadi Feigenbaum Kilian Beaver Feigenbaum We call a program that satis es the above constraints a private checker As is the case for adaptive checking we consider the case where there is a program that supposedly computes f on each of several noninteracting machines We design private checkers that work for a constant number of independent and noninteracting programs The adaptive and private checkers given are general techniques that work for a variety of numerical problems including integer multiplication modular multiplication matrixmultiplica tion the mod function integer division modular exponentiation and polynomial multiplication
1989,Program Correctness: Can One Test For It?,n/a
1989,Designing Programs That Check Their Work.,"A program correctness checker is an algorithm for checking the output of a computation. This paper defines the concept of a program checker. It designs program checkers for a few specific and carefully chosen problems in the class P of problems solvable in polynomial time. It also applies methods of modern cryptography, especially the idea of a probabilistic interactive proof, to the design of program checkers for group theoretic computations. Finally it characterizes the problems that can be checked."
1988,Proving Security Against Chosen Cyphertext Attacks.,"Abstract
The relevance of zero knowledge to cryptography has become apparent in the recent years. In this paper we advance this theory by showing that interaction in any zero-knowledge proof can be replaced by sharing a common, short, random string. This advance finds immediate application in the construction of the first public-key cryptosystem secure against chosen ciphertext attack.
Our solution, though not yet practical, is of theoretical significance, since the existence of cryptosystems secure against chosen ciphertext attack has been a famous long-standing open problem in the field."
1988,Non-Interactive Zero-Knowledge and Its Applications (Extended Abstract).,"We show that interaction in any zero-knowledge proof can be replaced by sharing a common, short, random string. We use this result to construct the first public-key cryptosystem secure against chosen ciphertext attack."
1987,Generic Oracles and Oracle Classes (Extended Abstract).,"Abstract:
In this paper, we examine various complexity issues relative to an oracle for a generic set in order to determine which are the more ""natural"" conjectures for these issues. Generic oracle results should be viewed as parallels to random oracle results, as in [BG]; the two are in many ways related, but, as we shall exhibit, not equivalent. Looking at computation relative to a generic oracle is in some ways a better reflection of computation without an oracle; for example, whereas adding a random oracle allows a deterministic polynomial-time machine to solve any problem in BPP, adding a generic oracle will not help solve any recursive problem faster than it could be solved without an oracle. Generic sets were first introduced by Cohen as a tool for proving independence results in set theory [Co]. Their recursion theoretic properties have also been explored in depth; for example, see [J] and [Ku2]. Some related work using forcing and/or generic sets as tools in oracle constructions can be found in [Ku3], [Do], [P], and [A-SFH]. However, this is to our knowledge the first knowledge the first thorough examination of complexity relative to a generic Oracle."
1986,Independent unbiased coin flips from a correlated biased source-a finite stae Markov chain.,"Von Neumannís trick for simulating anabsolutely unbiased coin by a biased one is this:
1.
Toss the biased coin twice, getting 00, 01, 10, or 11.

 
2.
If 00 or 11 occur, go back to step 1; else

 
3.
Call 10 aH, 01 aT.

 
Since Pr[H]=Pr[1]Pr[0]=Pr[T], the output is unbiased. Example: 00 10 11 01 01 ?HTT.

Peter Elias gives an algorithm to generate an independent unbiased sequence ofHs andTs that nearly achieves the Entropy of the one-coin source. His algorithm is excellent, but certain difficulties arise in trying to use it (or the original von Neumann scheme) to generate bits in expected linear time from a Markov chain.

In this paper, we return to the original one-coin von Neumann scheme, and show how to extend it to generate an independent unbiased sequence ofHs andTs from any Markov chain in expected linear time. We give a wrong and a right way to do this. Two algorithms A and B use the simple von Neumann trick on every state of the Markov chain. They differ in the time they choose to announce the coin flip. This timing is crucial."
1986,A Simple Unpredictable Pseudo-Random Number Generator.,"Two closely-related pseudo-random sequence generators are presented: The ${1 / P}$generator, with input P a prime, outputs the quotient digits obtained on dividing 1 by P. The $x^2 \bmod N$generator with inputs N, $x_0 $ (where $N = P \cdot Q$ is a product of distinct primes, each congruent to 3 mod 4, and $x_0 $ is a quadratic residue $\bmod N$), outputs $b_0 b_1 b_2 \cdots $ where $b_i = {\operatorname{parity}}(x_i )$ and $x_{i + 1} = x_i^2 \bmod N$.

From short seeds each generator efficiently produces long well-distributed sequences. Moreover, both generators have computationally hard problems at their core. The first generatorís sequences, however, are completely predictable (from any small segment of $2|P| + 1$ consecutive digits one can infer the ìseed,î P, and continue the sequence backwards and forwards), whereas the second, under a certain intractability assumption, is unpredictable in a precise sense. The second generator has additional interesting properties: from knowledge of $x_0 $ and N but notP or Q, one can generate the sequence forwards, but, under the above-mentioned intractability assumption, one can not generate the sequence backwards. From the additional knowledge of P and Q, one can generate the sequence backwards; one can even ìjumpî about from any point in the sequence to any other. Because of these properties, the $x^2 \bmod N$generator promises many interesting applications, e.g., to public-key cryptography. To use these generators in practice, an analysis is needed of various properties of these sequences such as their periods. This analysis is begun here.
"
1984,How to Generate Cryptographically Strong Sequences of Pseudo-Random Bits.,"We give a set of conditions that allow one to generate 50ñ50 unpredictable bits.Based on those conditions, we present a general algorithmic scheme for constructing polynomial-time deterministic algorithms that stretch a short secret random input into a long sequence of unpredictable pseudo-random bits.

We give an implementation of our scheme and exhibit a pseudo-random bit generator for which any efficient strategy for predicting the next output bit with better than 50ñ50 chance is easily transformable to an ìequally efficientî algorithm for solving the discrete logarithm problem. In particular: if the discrete logarithm problem cannot be solved in probabilistic polynomial time, no probabilistic polynomial-time algorithm can guess the next output bit better than by flipping a coin: if ìheadî guess ì0î, if ìtailî guess ì1î
"
1984,An Efficient Probabilistic Public-Key Encryption Scheme Which Hides All Partial Information.,"Abstract
This paper introduces the first probabilistic public-key encryption scheme which combines the following two properties:
(1)
Perfect secrecy with respect to polynomial time eavesdroppers: For all message spaces, no polynomial time bounded passive adversary who is tapping the lines, can compute any partial information about messages from their encodings, unless factoring composite integers is in probabilistic polynomial time.
 (2)
Efficiecy: It compares favorably with the deterministic RSA public-key cryptosystem in both encoding and decoding time and bandwidth expansion.
  The security of the system we propose can also be based on the assumption that the RSA function is intractable, maintaining the same cost for encoding and decoding and the Same data expansion. This implementation may have advantages in practice."
1984,Independent Unbiased Coin Flips From a Correlated Biased Source: a Finite State Markov Chain.,"Abstract:
von Neumann's trick for generating an absolutely unbiased coin from a biased one is this: 1. Toss the biased coin twice, getting 00, 01, 10, or 11. 2. If 00 or 11 occur, go back to step 1; else 3. Call 10 a H, 01 a T. Since p[H] = p[1]*p[0] = p[T], the output is unbiased. Example: 00 10 11 01 01 /spl I.oarr/ H T T. Peter Elias gives an algorithm to generate an independent unbiased sequence of Hs and Ts that nearly achieves the Entropy of the one-coin source. His algorithm is excellent, but certain difficulties arise in trying to use it (or the original von Neumann scheme) to generate bits in expected linear time from a Markov chain. In this paper, we return to the original one-coin von Neumann scheme, and show how to extend it to generate an independent unbiased sequence of Hs and Ts from any Markov chain in expected linear time. We give a right and wrong way to do this. Two algorithms A and B use the simple von Neumann trick on every state of the Markov chain. They differ in the time they choose to announce the coin flip. This timing is crucial."
1983,How to Exchange (Secret) Keys.,"A protocol is presented whereby two adversaries may exchange secrets, though neither trusts the other. The secrets are the prime factors of their publicly announced composite numbers. The two adversaries can exchange their secrets bit by bit, but each fea"
1983,Reducibility Among Protocols.,"Recently considerable attention has been given to designing provably secure cryptographic protocols. Three basic problems that have been studied are: exchange of secrets, contract signing, and certified mail. Several protocols have been proposed for these problems. These solutions are very diverse in nature: they are secure under different assumptions, have different probabilities of cheating, and require different number of message exchanges. Consequently, there is a need to investigate the underlying relationships between these problems. In this paper, we study reducibilities among these problems, i.e. how a protocol for one problem can be transformed into a protocol for another problem, preserving (roughly) the operating conditions.
"
1983,How to Exchange (Secret) Keys (Extended Abstract).,"A protocol is presented whereby two adversaries may exchange secrets, though neither trusts the other. The secrets are the prime factors of their publicly announced composite numbers. The two adversaries can exchange their secrets bit by bit, but each fears the other will cheat by sending ‚Äújunk‚Äùbits. To solve this problem we show how each of the two can prove, for each bit delivered, that the bit is good. Applications are suggested to such electronic business transactions as the signing of contracts and the sending of certified electronic mail."
1982,Coin Flipping by Telephone - A Protocol for Solving Impossible Problems.,"Alice and Bob want to flip a coin by telephone. (They have just divorced, live in different cities, want to decide who gets the car.) Bob would not like to tell Alice HEADS and hear Alice (at the other end of the line) say ""Here goes . . . I'm flipping the coin. . . . You lost!""Coin-flipping in the SPECIAL way done here has a serious purpose. Indeed, it should prove an INDISPENSABLE TOOL of the protocol designer. Whenever a protocol requires one of two adversaries, say Alice, to pick a sequence of bits at random, and whenever it serves Alice's interests best NOT to pick her sequence of bits at random, then coin-flipping (Bob flipping coins to Alice) as defined here achieves the desired goal:1. It GUARANTEES to Bob that Alice will pick her sequence of bits at random. Her bit is 1 if Bob flips heads to her, O otherwise.2. It GUARANTEES to Alice that Bob will not know WHAT sequence of bits he flipped to her.Coin-flipping has already proved useful in solving a number of problems once thought impossible: mental poker, certified mail, and exchange of secrets. It will certainly prove a useful tool in solving other problems as well."
1982,Comparison of Two Pseudo-Random Number Generators.,"Abstract
What do we want from a pseudo-random sequence generator? Ideally, we would like a pseudo-random sequence generator to quickly produce, from short seeds, long sequences (of bits) that appear in every way to be generated by successive flips of a fair coin."
1982,How to Generate Cryptographically Strong Sequences of Pseudo Random Bits.,"We give a set of conditions that allow one to generate 50ñ50 unpredictable bits.Based on those conditions, we present a general algorithmic scheme for constructing polynomial-time deterministic algorithms that stretch a short secret random input into a long sequence of unpredictable pseudo-random bits.We give an implementation of our scheme and exhibit a pseudo-random bit generator for which any efficient strategy for predicting the next output bit with better than 50ñ50 chance is easily transformable to an ìequally efficientî algorithm for solving the discrete logarithm problem. In particular: if the discrete logarithm problem cannot be solved in probabilistic polynomial time, no probabilistic polynomial-time algorithm can guess the next output bit better than by flipping a coin: if ìheadî guess ì0î, if ìtailî guess ì1î"
1981,The Complexity of Testing Whether a Graph is a Superconcentrator.,n/a
1981,Coin Flipping by Telephone.,"Two parties, who do not trust each other, wish to agree on the result of a coin toss while they are conversing by telephone. A new protocol which solves this problem is proposed and compared with previous protocols. "
1980,Equivalence of Free Boolean Graphs can be Decided Probabilistically in Polynomial Time.,n/a
1978,"On the Power of the Compass (or, Why Mazes Are Easier to Search than Graphs).",n/a
1977,On the Capability of Finite Automata in 2 and 3 Dimensional Space.,"The paper describes two algorithms for threading unknown, finite directed Eulerian mazes. Each of these algorithms is performed by a traveling robot whose control is a finite-state automaton. It is assumed that each vertex has a circular list of its outgoing edges. The items of this list are called exits. Each of the algorithms puts in one of the exits of each vertex a scan pebble. These pebbles can be used by a simple robot as traffic signals, which allow it to traverse an Eulerian cycle of the maze. For a directed graph (maze) G(V, E), the simple algorithm performs O(|V | ∑ |E|) edge traversals, while the advanced algorithm traverses every edge three times. Let dout(v) be the out-degree of vertex v. The algorithms use, at each vertex v, a local memory of size O(log dout(v))."
1975,Toward a Mathematical Theory of Inductive Inference.,"Intelligence tests occasionally require the extrapolation of an effective sequence (e.g. 1661, 2552, 3663, ‚Ä¶) that is produced by some easily discernible algorithm. In this paper, we investigate the theoretical capabilities and limitations of a computer to infer such sequences. We design Turing machines that in principle are extremely powerful for this purpose and place upper bounds on the capabilities of machines that would do better."
1974,On Almost Everywhere Complex Recursive Functions.,"Let h be a recursive function. A partial recursive function &psgr; is i.o. (infinitely often) h-complex if every program for &psgr; requires more than h(&khgr;) steps to compute &psgr;(&khgr;) for infinitely many inputs &khgr;. A more stringent notion is that of &psgr; being a.e. (almost everywhere) h-complex: &psgr; is a.e. h-complex if every program for &psgr; requires more than h(&khgr;) steps to compute &psgr;(&khgr;) for all but finitely many inputs &khgr;. These two definitions of h-complex functions do not yield the same theorems. Although it is possible to prove of every i.o. h-complex recursive function that it is i.o. h-complex, it is not possible to prove of every a.e. h-complex recursive function that it is a.e. h-complex. Similarly, recursive functions not i.o. h-complex can be proven to be such, but recursive functions not a.e. h-complex cannot be so proven. The construction of almost everywhere complex recursive functions appears much more difficult than the construction of infinitely often complex recursive functions. There have been found no ‚Äúnatural‚Äù examples of recursive functions requiring more than polynomial time for all but finitely many inputs. It is shown that from a single example of a moderately a.e. complex recursive function, one can obtain a.e. very complex recursive functions."
1973,Time Bounds for Selection.,"The number of comparisons required to select the i-th smallest of n numbers is shown to be at most a linear function of n by analysis of a new selection algorithm‚ÄîPICK. Specifically, no more than 5.4305 n comparisons are ever required. This bound is improved for extreme values of i, and a new lower bound on the requisite number of comparisons is also proved."
1973,On Complexity Properties of Recursively Enumerable Sets.,"An important goal of complexity theory, as we see it, is to characterize those partial recursive functions and recursively enumerable sets having some given complexity properties, and to do so in terms which do not involve the notion of complexity.

As a contribution to this goal, we provide characterizations of the effectively speedable, speedable and levelable [2] sets in purely recursive theoretic terms. We introduce the notion of subcreativeness and show that every program for computing a partial recursive function f can be effectively speeded up on infinitely many integers if and only if the graph of f is subcreative.

In addition, in order to cast some light on the concepts of effectively speedable, speedable and levelable sets we show that all maximal sets are levelable (and hence speedable) but not effectively speedable and we exhibit a set which is not levelable in a very strong sense but yet is effectively speedable."
1973,Inductive Inference: A Recursion Theoretic Approach.,"Abstract:
There are several situations that we are trying more or less to model. One arises from the standard IQ test in which a person is given a finite sequence of integers and asked to produce the next integer in the sequence. Another is provided by the following grossly simplified view of one aspect of physics: Consider a physicist who is trying to find a law to explain a growing body of experimental data. This data is presented as a set of pairs (x,y). Here, x is a description of a particular experiment, e.g., a high energy physics experiment; y is a description of the results obtained, e.g., the particles produced and their respective properties. The law describing these phenomena is essentially an algorithm for computing the function f(x) = y. What an inductive inference machine does is to request and obtain data in the form of pairs (x,y) and then use this to look for an algorithm i that computes (an extension of) f. Other examples arise in grammatical inference, pattern recognition, etc. This paper develops inductive inference along the lines of Solomonoff [9], Gold [4], and Feldman [3]. What is new and distinguishes our work from theirs is our attempt to characterize the set of functions that can be identified by an inductive inference machine. In the process, we have discovered (Theorem 4, part 1) that inductive inference machines can be considerably more powerful than we previously thought possible."
1972,Linear Time Bounds for Median Computations.,"New upper and lower bounds are presented for the maximum number of comparisons, f(i,n), required to select the i-th largest of n numbers. An upper bound is found, by an analysis of a new selection algorithm, to be a linear function of n: f(i,n) ‚â§ 103n/18 < 5.73n, for 1 ‚â§ i ‚â§ n. A lower bound is shown deductively to be: f(i,n) ‚â• n+min(i,n‚àíi+l) + [log2(n)] ‚àí 4, for 2 ‚â§ i ‚â§ n‚àí1, or, for the case of computing medians: f([n/2],n) ‚â• 3n/2 ‚àí 3"
1971,On Effective Procedures for Speeding Up Algorithms.,"This paper is concerned with the nature of speedups. Let f be any recursive function. We show that there is no effective procedure for going from an algorithm for f to a significantly faster algorithm for f. On the other hand, there is an effective proced"
1969,On Effective Procedures for Speeding Up Algorithms.,"This paper is concerned with the nature of speedups. Let f be any recursive function. We show that there is no effective procedure for going from an algorithm for f to a significantly faster algorithm for f. On the other hand, there is an effective procedure for going from any algorithm to a faster algorithm, provided one has a bound on the size of the algorithm that does the computation faster for all inputs. If no bound on the size of the faster algorithm is known in advance, one can still obtain a pseudo speedup: This is a very fast algorithm which computes a variant of the function, one which differs from the original function on a finite number of inputs."
1968,Tape Reversal Complexity Hierarchies.,n/a
1967,On the Size of Machines.,"In this paper, the methods of recursive function theory are used to study the size (or cost or complexity) of machines. A positive result of this study shows that to a remarkable degree, the relative size of two machines is independent of the particular way in which machine size is measured. Another result suggests that in order for programs to be of economical size, the programming language must be powerful enough to compute arbitrary general recursive functions, rather than some restricted subset such as the primitive recursive functions. Finally, a kind of speedup theorem is proved which is curiously independent of whether the measure of complexity be the size or the number of steps taken by the machines that compute the functions."
1967,A Machine-Independent Theory of the Complexity of Recursive Functions.,"The number of steps required to compute a function depends, in general, on the type of computer that is used, on the choice of computer program, and on the input-output code. Nevertheless, the results obtained in this paper are so general as to be nearly independent of these considerations. A function is exhibited that requires an enormous number of steps to be computed, yet has a ‚Äúnearly quickest‚Äù program: Any other program for this function, no matter how ingeniously designed it may be, takes practically as many steps as this nearly quickest program. A different function is exhibited with the property that no matter how fast a program may be for computing this function another program exists for computing the function very much faster."
1967,Automata on a 2-Dimensional Tape.,"Abstract:
This paper explains our approach to the problem of pattern recognition by serial computer. The rudimentary theory of vision presented here lies within the framework of automata theory. Our goal is to classify the types of patterns that can be recognized by an automaton that scans a finite 2-dimensional tape. For example, we would like to know if an automaton can decide whether or not a given pattern on a tape forms a connected region. Although we have solved a number of problems, we have failed to solve this connectedness problem. This paper merely begins to describe the action of automata in higher dimensions. Our goal now is to generalize the theory presented here and make it applicable to a wide variety of pattern-recognizing machines."
