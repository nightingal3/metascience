2019,"Logic, Algebra, and Geometry at the Foundation of Computer Science.","Abstract
This paper shows by examples how the Theory of Programming can be taught to first-year CS undergraduates. The only prerequisite is their High School acquaintance with algebra, geometry, and propositional calculus. The main purpose of teaching the subject is to support practical programming assignments and projects throughout the degree course. The aims would be to increase the studentâ€™s enjoyment of programming, reduce the workload, and increase the prospect of success."
2019,"A Calculus of Space, Time, and Causality: Its Algebra, Geometry, Logic.","Abstract
The calculus formalises human intuition and common sense about space, time, and causality in the natural world. Its intention is to assist in the design and implementation of programs, of programming languages, and of interworking by tool chains that support rational program development.
The theses of this paper are that Concurrent Kleene Algebra (CKA) is the algebra of programming, that the diagrams of the Unified Modeling Language provide its geometry, and that Unifying Theories of Programming (UTP) provides its logic. These theses are illustrated by a formalisation of features of the first concurrent object-oriented language, Simula 67. Each level of the calculus is a conservative extension of its predecessor.
We conclude the paper with an extended section on future research directions for developing and applying UTP, CKA, and our calculus, and on how we propose to implement our algebra, geometry, and logic."
2016,Developments in concurrent Kleene algebra.,"Highlights
â€¢
We define and informally interpret the operators of Concurrent Kleene Algebra (CKA).
â€¢
Programs are sets of traces, which consist of occurrences of primitive actions.
â€¢
Refinement orderings between traces and inequational laws with them lift to programs.
â€¢
Several models of CKA and refinement are related to concurrent programming practice.
â€¢
We detail several directions in which to extend the presented research.
Abstract
This report summarises the background and recent progress in the research of its co-authors. It is aimed at the construction of links between algebraic presentations of the principles of programming and the exploitation of concurrency in modern programming practice. The signature and laws of a Concurrent Kleene Algebra (CKA) largely overlap with those of a Regular Algebra, with the addition of concurrent composition and a few simple laws for it. They are re-interpreted here in application to computer programs. The inclusion relation for regular expressions is re-interpreted as a refinement ordering, which supports a stepwise contractual approach to software system design and to program debugging.
The laws are supported by a hierarchy of models, applicable and adaptable to a range of different purposes and to a range of different programming languages. The algebra is presented in three tiers. The bottom tier defines traces of program execution, represented as sets of events that have occurred in a particular run of a program; the middle tier defines a program as the set of traces of all its possible behaviours. The top tier introduces additional incomputable operators, which are useful for describing the desired or undesired properties of computer program behaviour. The final sections outline directions in which further research is needed."
2016,A Discrete Geometric Model of Concurrent Program Execution.,"Abstract
A trace of the execution of a concurrent object-oriented program can be displayed in two-dimensions as a diagram of a non-metric finite geometry. The actions of a programs are represented by points, its objects and threads by vertical lines, its transactions by horizontal lines, its communications and resource sharing by sloping arrows, and its partial traces by rectangular figures.
We prove informally that the geometry satisfies the laws of Concurrent Kleene Algebra (CKA); these describe and justify the interleaved implementation of multithreaded programs on computer systems with a lesser number of concurrent processors. More familiar forms of semantics (e.g., verification-oriented and operational) can be derived from CKA.
Programs are represented as sets of all their possible traces of execution, and non-determinism is introduced as union of these sets. The geometry is extended to multiple levels of abstraction and granularity; a method call at a higher level can be modelled by a specification of the method body, which is implemented at a lower level.
The final section describes how the axioms and definitions of the geometry have been encoded in the interactive proof tool Isabelle, and reports on progress towards automatic checking of the proofs in the paper."
2015,Exploring an Interface Model for CKA.,"Abstract
Concurrent Kleene Algebras (CKAs) serve to describe general concurrent systems in a unified way at an abstract algebraic level. Recently, a graph-based model for CKA has been defined in which the incoming and outgoing edges of a graph define its input/output interface. The present paper provides a simplification and a significant extension of the original model to cover notions of states, predicates and assertions in the vein of algebraic treatments using modal semirings. Moreover, it uses the extension to set up a variant of the temporal logic
CTL
âˆ—
for the interface model."
2014,The laws of programming unify process calculi.,"Abstract
We survey the well-known algebraic laws of sequential programming, and propose some less familiar laws for concurrent programming. On the basis of these laws, we derive the rules of a number of classical programming and process calculi, for example, those due to Hoare, Milner, and Kahn. The algebraic laws are simpler than each of the calculi derived from it, and they are stronger than all the calculi put together. Conversely, most of the laws are derivable from one or more of the calculi. This suggests that the laws are useful as a presentation of program semantics, and correspond to a widely held common understanding of the meaning of programs. For further evidence, Appendix A describes a realistic and generic model of program behaviour, which has been proved to satisfy the laws."
2014,Developments in Concurrent Kleene Algebra.,"Abstract
This report summarises recent progress in the research of its co-authors towards the construction of links between algebraic presentations of the principles of programming and the exploitation of concurrency in modern programming practice. The research concentrates on the construction of a realistic family of partial order models for Concurrent Kleene Algebra (aka, the Laws of Programming). The main elements of the model are objects and the events in which they engage. Further primitive concepts are traces, errors and failures, and transferrable ownership. In terms of these we can define other concepts which have proved useful in reasoning about concurrent programs, for example causal dependency and independence, sequentiality and concurrency, allocation and disposal, synchrony and asynchrony, sharing and locality, input and output."
2014,Laws of Programming: The Algebraic Unification of Theories of Concurrency.,"Abstract
I began my academic research career in 1968, when I moved from industrial employment as a programmer to the Chair of Computing at the Queenâ€™s University in Belfast. My chosen research goal was to discover an axiomatic basis for computer programming. Originally I wanted to express the axioms as algebraic equations, like those which provide the basis of arithmetic or group theory. But I did not know how. After many intellectual vicissitudes, I have now discovered the simple secret. I would be proud of this discovery, if I were not equally ashamed at taking so long to discover it."
2014,Laws of concurrent programming.,"The talk extends the Laws of Programming [1] by four laws governing concurrent composition of programs. This operator is associative and commutative and distributive through union; and it has the same unit (do nothing) as sequential composition. Furthermore, sequential and concurrent composition distribute through each other, in accordance with an exchange law; this permits an implementation of concurrency by partial interleaving."
2013,Unifying Semantics for Concurrent Programming.,"Abstract
Four well-known methods for presenting semantics of a programming language are: denotational, deductive, operational, and algebraic. This essay presents algebraic laws for the structural features of a class of imperative programming languages which provide both sequential and concurrent composition; and it illustrates the way in which the laws are consistent with the other three semantic presentations of the same language. The exposition combines simplicity with generality by postponing consideration of the possibly more complex basic commands of particular programming languages. The proofs are given only as hints, but they are easily reconstructed, even with the aid of a machine."
2013,Generic Models of the Laws of Programming.,"Abstract
The laws of programming are a collection of judgments about the equality and ordering of computer programs. A model of the laws is a mathematical description of the execution of programs, where the model has been proved to satisfy the laws. A generic model is one that has parameters that can be adjusted to the properties of a range of different programming languages and their differing implementations and differing applications. In this way, a generic model serves as the basis of a unifying theory of programming."
2012,In praise of algebra.,"We survey the well-known algebraic laws of sequential programming, and extend them with some less familiar laws for concurrent programming. We give an algebraic definition of the Hoare triple, and algebraic proofs of all the relevant laws for concurrent separation logic. We give the provable concurrency laws for Milner transitions, for the Back/Morgan refinement calculus, and for Dijkstra’s weakest preconditions. We end with a section in praise of algebra, of which Carroll Morgan is such a master.
"
2012,Net Models for Concurrent Object Behaviour.,"Abstract
The behaviour of an object allocated and used by a computer program consists of a set of events involving the object which occur in and around a computer during execution of the program. Object behaviour can be modelled by an occurrence net (a Petri net without places), in which each event is a transition (drawn as a box), and the arrows between the transitions represent dependency between the events. The total behaviour of the program is just the sum of the behaviours of the objects which it allocates. A program (perhaps expressed as a Petri net with places) is mathematically defined as just the set of all its possible behaviours, in all its possible environments of execution. An object class is similarly defined as the set of all the possible behaviours of all its possible objects, as used in any possible program."
2012,Algebra of concurrent design.,"Abstract:
Summary form only given. I introduce some familiar algebraic laws governing the operators of sequential and concurrent composition of designs. They can be combined with the familiar operators of propositional calculus. The resulting logic seems to apply equally to hardware design and to software design; and perhaps also to the planning of other designs and plans for behaviour that evolves in space and time."
2012,The Laws of Programming Unify Process Calculi.,"Abstract
We survey the well-known algebraic laws of sequential programming, and propose some less familiar laws for concurrent programming. On the basis of these laws, we derive the rules of a number of classical programming and process calculi, for example, those due to Hoare, Milner, and Kahn. The algebra is simpler than each of the calculi derived from it, and stronger than all the calculi put together. We end with a section describing the role of unification in Science and Engineering."
2012,Algebra Unifies Operational Calculi.,"Abstract
We survey the well-known algebraic laws of sequential programming, and propose some less familiar laws for concurrent programming. On the basis of these laws, we derive a general calculus of program execution. The basic judgment of the theory is a quintuple, and we deduce its rules by algebraic reasoning. The general calculus can be specialised to obtain more familiar operational calculi, such as the structural operational semantics of Plotkin, process calculus semantics of Milner, reduction semantics with evaluation contexts of Felleisen and Hieb, and the natural semantics of Kahn. The algebra unifies these calculi, as it is simpler than each calculus derived from it, and stronger than all of them put together."
2011,Concurrent Kleene Algebra and its Foundations.,"Abstract
A Concurrent Kleene Algebra offers two composition operators, related by a weak version of an exchange law: when applied in a trace model of program semantics, one of them stands for sequential execution and the other for concurrent execution of program components. After introducing this motivating concrete application, we investigate its abstract background in terms of a primitive independence relation between the traces. On this basis, we develop a series of richer algebras the richest validates a proof calculus for programs similar to that of a Jones style rely/guarantee calculus. On the basis of this abstract algebra, we finally reconstruct the original trace model, using the notion of atoms from lattice theory."
2011,Recovery Blocks.,"We give a semantics for Brian Randell’s recovery blocks, and show how they can be proved correct. The formalisation is expressed wholly in propositional logic.
"
2011,On Locality and the Exchange Law for Concurrent Processes.,"Abstract
This paper studies algebraic models for concurrency, in light of recent work on Concurrent Kleene Algebra and Separation Logic. It establishes a strong connection between the Concurrency and Frame Rules of Separation Logic and a variant of the exchange law of Category Theory. We investigate two standard models: one uses sets of traces, and the other is state-based, using assertions and weakest preconditions. We relate the latter to standard models of the heap as a partial function. We exploit the power of algebra to unify models and classify their variations."
2010,Fine-grain concurrency.,"I have been interested in concurrent programming since about 1963, when its associated problems contributed to the failure of the largest software project that I have managed. When I moved to an academic career in 1968, I hoped that I could find a solution to the problems by my research. Quite quickly I decided to concentrate on coarse?grained concurrency, that does not allow concurrent processes to share main memory. The only interaction between processes is confined to explicit input and output commands. This simplification led eventually to the exploration of the theory of Communicating Sequential Processes. Since joining Microsoft Research in 1999, I have plucked up courage at last to look at fine?grain concurrency, involving threads that interleave their access to main memory at the fine granularity of single instruction execution. By combining the merits of a number of different theories of concurrency, one can paint a relatively simple picture of a theory for the correct design of concurrent systems. Indeed, pictures are a great help in conveying the basic understanding. This paper presents some on?going directions of research that I have been pursuing with colleagues in Cambridge—both at Microsoft Research and in the University Computing Laboratory. Copyright © 2007 C.A.R. Hoare.
"
2010,CSP is a retract of CCS.,"Abstract
Automata theory provides two ways of defining an automaton: either by its transition system, defining its states and events, or by its language, the set of sequences (traces) of events in which it can engage. For many classes of automaton, these forms of definition have been proved equivalent. For example, there is a well-known isomorphism between regular languages and finite deterministic automata. This paper suggests that for (demonically) the non-deterministic automata (as treated in process algebra), the appropriate link between transition systems and languages may be a retraction rather than an isomorphism.
A pair of automata, defined in the tradition of CCS by their transition systems, may be compared by a pre-ordering based on some kind of simulation or bisimulation, for example, weak, strong, or barbed. Automata defined in the tradition of CSP are naturally ordered by set inclusion of their languages (often called refinement); variations in ordering arise from different choices of basic event, including for example, refusals and divergences. In both cases, we characterise a theory by its underlying transition system and its choice of ordering. Our treatment is therefore wholly semantic, independent of the syntax and definition of operators of the calculus.
We put forward a series of retractions relating the above-mentioned versions of CSP to their corresponding CCS transition models. A retraction is an injection that is (with respect to the chosen ordering) monotonic, increasing and idempotent (up to equivalence). It maps the nodes of a transition system of its source theory to those of a system that has been saturated by additional transitions. Each retraction will be defined by a transition rule, in the style of operational semantics; the proofs use the familiar technique of co-induction, often abbreviated by encoding in the relational calculus.
The aim of this paper is to contribute to unification of theories of reactive system programming. More practical benefits may follow. For example, we justify a method to improve the efficiency of model checking based on simulation. Furthermore, we show how model checking of a transition network fits consistently with theorem-proving tools, which reason directly about specifications and designs that are expressed in terms of sets of sequences of observable events."
2010,Unraveling a Card Trick.,"Abstract
In one version of Gilbreathâ€™s card trick, a deck of cards is arranged as a series of quartets, where each quartet contains a card from each suit and all the quartets feature the same ordering of the suits. For example, the deck could be a repeating sequence of spades, hearts, clubs, and diamonds, in that order, as in the deck below.
{\langle 5\spadesuit\rangle}, {\langle 3\heartsuit\rangle}, {\langle Q\clubsuit\rangle}, {\langle 8\diamondsuit\rangle},
{\langle K\spadesuit\rangle}, {\langle 2\heartsuit\rangle}, {\langle 7\clubsuit\rangle}, {\langle 4\diamondsuit\rangle},
{\langle 8\spadesuit\rangle}, {\langle J\heartsuit\rangle}, {\langle 9\clubsuit\rangle}, {\langle A\diamondsuit\rangle}
The deck is then cut into two (not necessarily equal) half-decks, possibly as {\langle 5\spadesuit\rangle}, {\langle 3\heartsuit\rangle}, {\langle Q\clubsuit\rangle}, {\langle 8\diamondsuit\rangle}, {\langle K\spadesuit\rangle} and {\langle 2\heartsuit\rangle}, {\langle 7\clubsuit\rangle}, {\langle 4\diamondsuit\rangle}, {\langle 8\spadesuit\rangle}, {\langle J\heartsuit\rangle}, {\langle 9\clubsuit\rangle}, {\langle A\diamondsuit\rangle}.
The order of one of the half-decks is then reversed. Either half-deck could be reversed. We can pick the smaller one, i.e., the first one, and reverse it to obtain {\langle K\spadesuit\rangle}, {\langle 8\diamondsuit\rangle}, {\langle Q\clubsuit\rangle}, {\langle 3\heartsuit\rangle}, {\langle 5\spadesuit\rangle}. The two half-decks are then shuffled in a (not necessarily perfect) riffle-shuffle. One such shuffle is shown below, where the underlined cards are drawn from the second half-deck.
{\langle 2\heartsuit\rangle}, {\langle 7\clubsuit\rangle}, \underline{\langle K\spadesuit\rangle}, \underline{\langle 8\diamondsuit\rangle},
{\langle 4\diamondsuit\rangle}, {\langle 8\spadesuit\rangle}, \underline{\langle Q\clubsuit\rangle}, {\langle J\heartsuit\rangle},
{\underline{\langle 3\heartsuit\rangle}}, {\langle 9\clubsuit\rangle}, \underline{\langle 5\spadesuit\rangle}, {\langle A\diamondsuit\rangle}
The quartets in the shuffled deck are displayed to demonstrate that each quartet contains a card from each suit. This turns out to be inevitable no matter how the original deck is cut and the order in which the two decks are shuffled. The principle underlying the card trick can be proved in a number of ways. We present the argument as a series of transformations that demystify the trick and describe its formalization."
2010,"Differential static analysis: opportunities, applications, and challenges.","It is widely believed that program analysis can be more closely targeted to the needs of programmers if the program is accompanied by further redundant documentation. This may include regression test suites, API protocol usage, and code contracts. To this should be added the largest and most redundant text of all: the previous version of the same program. It is the differences between successive versions of a legacy program already in use which occupy most of a programmer's time. Although differential analysis in the form of equivalence checking has been quite successful for hardware designs, it has not received as much attention in the static program analysis community.
This paper briefly summarizes the current state of the art in differential static analysis for software, and suggests a number of promising applications. Although regression test generation has often been thought of as the ultimate goal of differential analysis, we highlight several other applications that can be enabled by differential static analysis. This includes equivalence checking, semantic diffing, differential contract checking, summary validation, invariant discovery and better debugging. We speculate that differential static analysis tools have the potential to be widely deployed on the developer's toolbox despite the fundamental stumbling blocks that limit the adoption of static analysis."
2010,"Testing and Proving, Hand-in-Hand.","Abstract
The motivating ideal of my lifetimeâ€™s scientific research has been that of program correctness, achieved with the aid of mathematical proof. As suggested by Turing, Floyd, and others, the proofs would be based on the decoration of a program with assertions, that would be proved true if ever they were evaluated at the point where they were written in the code. During my long academic career, I regarded program testing as the main rival technology, and feared that improvement in the practice of testing would delay development and application of the superior technology of proof."
2009,The verified software initiative: A manifesto.,n/a
2009,Graphical models of separation logic.,"Abstract
Graphs are used to model control and data flow among events occurring in the execution of a concurrent program. Our treatment of data flow covers both shared storage and external communication. Nevertheless, the laws of Hoare and Jones correctness reasoning remain valid when interpreted in this general model."
2009,Foundations of Concurrent Kleene Algebra.,"Abstract
A Concurrent Kleene Algebra offers two composition operators, one that stands for sequential execution and the other for concurrent execution [10]. In this paper we investigate the abstract background of this law in terms of independence relations on which a concrete trace model of the algebra is based. Moreover, we show the interdependence of the basic properties of such relations and two further laws that are essential in the application of the algebra to a Jones style rely/guarantee calculus. Finally we reconstruct the trace model in a more abstract setting based on the notion of atoms from lattice theory."
2009,Concurrent Kleene Algebra.,"Abstract
A concurrent Kleene algebra offers, next to choice and iteration, operators for sequential and concurrent composition, related by an inequational form of the exchange law. We show applicability of the algebra to a partially-ordered trace model of program execution semantics and demonstrate its usefulness by validating familiar proof rules for sequential programs (Hoare triples) and for concurrent ones (Jonesâ€™s rely/guarantee calculus). This involves an algebraic notion of invariants; for these the exchange inequation strengthens to an equational distributivity law. Most of our reasoning has been checked by computer."
2008,Verification of Fine-grain Concurrent Programs.,"Abstract
Intel has announced that in future each standard computer chip will contain many processors operating concurrently on the same shared memory; their use of memory is interleaved at the fine granularity of individual memory accesses. The speed of the individual processors will never be significantly faster than they are today. Continued increase in performance will therefore depend on the skill of programmers in exploiting the concurrency of this multi-core architecture. In addition, programmers will have to avoid increased risks of race conditions, non-determinism, deadlocks and livelocks. To reduce these risks, we propose a theory of correctness for fine-grain concurrent programs. The approach is just an amalgamation of a number of well-known and well-researched ideas, including flowcharts, Floyd assertions, Petri nets, process algebra, separation logic, critical regions and rely/guarantee reasoning. These ideas are applied in combination to the design of a structured calculus of correctness for fine-grain concurrent programs; it includes the main features of a structured concurrent programming language."
2008,Separation Logic Semantics for Communicating Processes.,"Abstract
This paper explores a unification of the ideas of Concurrent Separation Logic with those of Communicating Sequential Processes. It extends separation logic by an operator for separation in time as well as separation in space. It extends CSP in the direction of the pi-calculus: dynamic change of alphabet is achieved by communication of channel names. Separation is exploited to ensure that each channel still has only two ends. For purposes of exploration, the model is the simplest possible, confined to traces without refusals. The treatment is sufficiently general to facilitate extensions by standard techniques for sharing multiplexed channels and heap state."
2008,"Verified Software: Theories, Tools, Experiments.","Abstract:
The ideal of verified software has long been the goal of research in Computer Science. This paper argues that the time is ripe to embark on a Grand Challenge project to construct a program verifier, based on a sound and complete theory of programming, and evaluated by experimental application to a representative sample of useful computer software."
2008,Fine-Grain Concurrency.,"Abstract
I have been interested in concurrent programming since about 1963, when its associated problems contributed to the failure of the largest software project that I have managed. When I moved to an academic career in 1968, I hoped that I could find a solution to the problems by my research. Quite quickly I decided to concentrate on coarse-grained concurrency, which does not allow concurrent processes to share main memory. The only interaction between processes is confined to explicit input and output commands. This simplification led eventually to the exploration of the theory of Communicating Sequential Processes.
Since joining Microsoft Research in 1999, I have plucked up courage at last to look at fine-grain concurrency, involving threads which interleave their access to main memory at the fine granularity of single instruction execution. By combining the merits of a number of different theories of concurrency, one can paint a relatively simple picture of a theory for the correct design of concurrent systems. Indeed, pictures area great help in conveying the basic understanding. This paper presents some on-going directions of research that I have been pursuing with colleagues in Cambridge â€“ both at Microsoft Research and in the University Computing Laboratory."
2008,Compensable Transactions.,The concept of a compensable transaction has been embodied in modern business workflow languages like BPEL. This article uses the concept of a box-structured Petri net to formalise the definition of a compensable transaction. The standard definitions of structured program connectives are extended to construct longer-running transactions out of shorter fine-grain ones. Floyd-type assertions on the arcs of the net specify the intended properties of the transaction and of its component programs. The correctness of the whole transaction can therefore be proved by local reasoning.
2007,The Ideal of Program Correctness: Third Computer Journal Lecture.,"The ideal of verified software has long been the goal of research in Computer Science. This article argues that the time is ripe to embark on a Grand Challenge project to construct a program verifier, based on a sound and complete theory of programming, and evaluated by experimental application to a large and representative sample of useful computer software.
Computer Science owes its existence to the invention of the stored-program digital computer. It derives continuously renewed inspiration from the constant stream of new computer applications, which are still being opened up by half a century of continuous reduction in the cost of computer chips, and by spectacular increases in their reliability, performance and capacity. The science of programming has made comparable advances with the discovery of...
"
2007,Science and Engineering: A Collusion of Cultures.,"Abstract:
The cultures of science and engineering are diametrically opposed along a number of dimensions: long-term/short-term, idealism/compromise, formality/ intuition, certainty/risk management, perfection/ adequacy, originality/familiarity, generality/specificity, unification/diversity, separation/amalgamation of concerns. You would expect two such radically different cultures to collide. Yet all the technological advances of the modern era result not from their collision but from their collusion-in its original sense of a fruitful interplay of ideas from both cultures. The author illustrates these points by the example of research into program verification and research into dependability of systems. The first of these aims at development and exploitation of a grand unified theory of programming, and therefore shares more the culture of science. The second is based on practical experience of projects in a range of important computer applications, and it shares more the culture of engineering. A collision of cultures would not be unexpected. But the author suggests that the time has come for collusion, and the author suggests how. We need to define an interface across which the cultures can explicitly collaborate. Dependability research can deliver its results in the form of a library of realistic domain models for a variety of important and common computer applications. A domain model is a reusable pattern for many subsequently conceived products or product lines. It includes a mix of informal and formal descriptions of the environment in which the computer system or network is embedded. It concentrates on the interfaces to the computer system, and the likely requirements and preferences of its community of users. The practicing software engineer takes the relevant application domain model as the starting point for a new project or project proposal, and then specializes it to accord with the current environment and current customer requirements. Domain models are most lik...
(View more)"
2007,Fine-grain Concurrency.,"I have been interested in concurrent programming since about 1963, when its associated problems contributed to the failure of the largest software project that I have managed. When I moved to an academic career in 1968, I hoped that I could find a solution to the problems by my research. Quite quickly I decided to concentrate on coarse-grained concurrency, which does not allow concurrent processes to share main memory. The only interaction between processes is confined to explicit input and output commands. This simplification led eventually to the exploration of the theory of Communicating Sequential Processes.
Since joining Microsoft Research in 1999, I have plucked up courage at last to look at fine-grain concurrency, involving threads which interleave their access to main memory at the fine granularity of single instruction execution. By combining the merits of a number of different theories of concurrency, one can paint a relatively simple picture of a theory for the correct design of concurrent systems. Indeed, pictures are a great help in conveying the basic understanding. This paper presents some on-going directions of research that I have been pursuing with colleagues in Cambridge – both at Microsoft Research and in the University Computing Laboratory.
"
2006,Why ever CSP?,"Abstract
The original theoretical model of Communicating Sequential Processes owed its inspiration to the achievements of Milner, Scott and Dijkstra. It was developed at around the time of the publication of Milner's Calculus of Communicating Systems. Why ever did CSP diverge from CCS?
The ESPRIT basic research action 'CONCUR' brought together the proponents of three of the original calculi of concurrency: ACP, CCS and CSP. It was hoped that we would develop a unified calculus, and concur upon its adoption. Why ever did we fail?
I would like to share with you the way in which I thought about these questions twenty five years ago. Since then, many excellent comparative studies have been conducted. As a result, we can now see how such theories can best be unified, without assimilation of their distinctive features or compromise of their distinctive merits."
2006,Intelligent Systems and Formal Methods in Software Engineering.,"Abstract:
Over the last few years, technologies for the formal description, construction, analysis, and validation of software - based mostly on logics and formal reasoning - have matured. We can expect them to complement and partly replace traditional software engineering methods in the future. Formal methods in software engineering are an increasingly important application area for intelligent systems. The field has outgrown the area of academic case studies, and industry is showing serious interest. We convincingly argue that we've reached the point where we can solve the problem of how to formally verify industrial-scale software. We propose program verification as a computer science Grand Challenge. Deductive software verification is a core technology of formal methods. We describe recent dramatic changes in the way it's perceived and used. Another important base technique of formal methods, besides software verification, is synthesizing software that's correct by construction because it's formally derived from its specification. We discuss recent developments and trends in this area. Surprisingly efficient decision procedures for the satisfiability modulo theories problem have recently emerged. We explain these techniques and why they're important for all formal-methods tools. We look at formal methods from an industry perspective. We explain the success of Microsoft Research's SLAM project, which has developed a verification tool for device drivers"
2006,The verified software repository: a step towards the verifying compiler.,"Abstract
The verified software repository is dedicated to a long-term vision of a future in which all computer systems justify the trust that society increasingly places in them. This would be accompanied by a substantial reduction in the current high costs of programming error, incurred during the design, development, testing, installation, maintenance, evolution, and retirement of computer software. An important technical contribution to this vision will be a verifying compiler: a tool-set that automatically proves that a program will always meet its specification, insofar as this has been formalised, without even needing to run it. This has been a challenge for computing research for over 30 years, but the current state of the art now gives grounds for hope that it may be implemented in the foreseeable future. Achievement of the overall vision will depend also on continued progress of research into dependability and software evolution, as envisaged by the UKCRC Grand Challenge project in dependable systems evolution. The verified software repository is a first step towards the realisation of this long-term vision. It will maintain and develop an evolving collection of state-of-the-art tools, together with a representative portfolio of real programs and specifications on which to test, evaluate, and develop the tools. It will contribute initially to the inter-working of tools, and eventually to their integration. It will promote transfer of the relevant technology to industrial tools and into software engineering practice. It will build on the recognised achievements of practical formal development of safety-critical computer applications, and contribute to an international initiative in verified software, covering theory, tools, and experimental validation."
2006,The ideal of verified software.,"This talk is intended as an introduction to this afternoon's discussion on Grand Challenge problems for the ACL2 community. I want to invite you to address the possibility of an even more broadly based challenge, directed towards the scientific ideal of verified software. This is a challenge that will bring together the talents of theorists, tool-builders, and experimental scientists from around the world. It will provide opportunity for collaboration and scientific competition between many research communities, over a period of fifteen years or more. And it has a testable goal---the production and automatic verification of (say) a million of lines of useful code, drawn from many areas of computer application."
2006,The Ideal of Verified Software.,"Abstract
The ideal of verified software has long been the goal of research in Computer Science. This paper argues that the time is ripe to embark on a Grand Challenge project to construct a program verifier, based on a sound and complete theory of programming, and evaluated by experimental application to a large and representative sample of useful computer software."
2006,Proving correctness of highly-concurrent linearisable objects.,"We study a family of implementations for linked lists using fine-grain synchronisation. This approach enables greater concurrency, but correctness is a greater challenge than for classical, coarse-grain synchronisation. Our examples are demonstrative of common design patterns such as lock coupling, optimistic, and lazy synchronisation. Although they are are highly concurrent, we prove that they are linearisable, safe, and they correctly implement a high-level abstraction. Our proofs illustrate the power and applicability of rely-guarantee reasoning, as well of some of its limitations. The examples of the paper establish a benchmark challenge for other reasoning techniques."
2006,CSP Is a Retract of CCS.,"Summary
Theories of concurrency can be distinguished by the set of processes that they model, and by their choice of pre-ordering relation used to compare processes and to prove their correctness. For example, theories based on CCS are often pre-ordered by simulation (or more commonly bisimulation), of which the main varieties are strong or weak or barbed. Theories based on CSP choose as their pre-order a refinement relation, defined as inclusion over sets of observations. The main varieties of observation are just traces, or failures and/or divergences. The processes of the CSP model are restricted to those that satisfy certain naturally arising â€˜healthiness conditionsâ€™. This paper gives a unifying treatment of simulation and refinement, and illustrates it by the familiar varieties of CCS and CSP that are mentioned above.
We consider the variations two at a time. A link between two theories is a function L, which maps the processes of its source theory onto those of its target theory. The image of L defines exactly the set of processes of the target theory. The ordering relation of the target theory is obtained by applying the link L to one or both operands before applying the source theory ordering. We will use the normal transition rules of a structured operational semantics to define a series of linking functions: W for weak simulation, T for trace refinement, R for refusals, D for divergences. We then show that each function is a retraction, in the sense that it is idempotent and decreasing and (in most cases) monotonic in its source ordering. Finally, we show that certain compositions of these functions are also retractions.
The definition of a retraction ensures that (1) the processes of the target theory are a subset of those of the source theory; (2) all ordering theorems of the source theory are preserved in the target theory; (3) the healthiness conditions of the target theory are expressed as fixed-point equivalences of the form p â‰¡ Lp; (4) model-checking the target theory can be optimised, by applying L to only one of the two operands of the ordering. Finally, we show how the separately defined retractions can be composed in a way that preserves these important properties. In other words, the transition systems of several alternative versions of CCS, as well as the main standard versions of CSP, are retracts of the universal transition system that underlies CCS.
The research reported here is a step towards completion of the unfinished business of the original ESPRIT Basic Research Action CONCUR [BRA 3009, 1989-92], which aimed to assimilate the theories and notations of CSP, ACP and CCS. A retraction is a good tool for this purpose, because it precisely codifies the similarities between the theories, and enables them to be used in combination, while preserving their essential and beneficial differences. Such unified families of theories may in due course serve as a rigorous foundation for a comprehensive programming toolset, one that provides reliable assistance at all stages of program design, development, testing and evolution. In this working draft, some of the later sections are incomplete."
2005,Grand Challenges for Computing Research.,"What are the major research challenges that face the world of computing today? Are there any of them that match the grandeur of well-known challenges in other branches of science? This article is a report on an exercise by the Computing Research Community in the UK to answer these questions, and includes a summary of the outcomes of a BCS-sponsored conference held in Newcastle-upon-Tyne from 29 to 31 March this year.
"
2005,Comparing Two Approaches to Compensable Flow Composition.,"Abstract
Web services composition is an emerging paradigm for the integration of long running business processes, attracting the interest of both Industry, in terms of XML-based standards for business description, and Academy, exploiting process description languages. The key challenging aspects to model are orchestration workflows, choreography of exchanged messages, fault handling, and transactional integrity with compensation mechanisms. Few recent proposals attempted to mitigate the explosion of XML-constructs in ad hoc standards by a careful selection of a small set of primitives related to the above aspects. This papers clarifies analogies and differences between two such recent process description languages: one based on interleaving trace semantics and the other on concurrent traces. We take advantage of their comparison to characterise and relate four different coordination policies for compensating parallel processes. Such policies differ on the way in which the abort of a process influences the execution of sibling processes, and whether compensation is distributed or centralised."
2005,Linking Theories of Concurrency.,"Abstract
We construct a Galois connection between the theories that underlie CCS [7] and CSP [4]. It projects the complete transition system for CCS onto exactly the subset that satisfies the healthiness conditions of CSP. The construction applies to several varieties of both calculi: CCS with strong, weak or barbed simulation, and CSP with trace refinement or failures refinement, or failures/divergence. We suggest the challenge of linking other theories of concurrency by Galois connection."
2005,"The Verifying Compiler, a Grand Challenge for Computing Research.","Abstract
The ideas of program verification date back to Turing and von Neumann, who introduced the concept of an assertion as the specification of an interface between parts of a program. The idea of mechanical theorem proving dates back to Leibniz; it has been explored in practice on modern computers by McCarthy, Milner, and many others since. A proposal for â€™a program verifierâ€™, combining these two technologies, was the subject of a Doctoral dissertation by James C. King, submitted at the Carnegie Institute of Technology in 1969.
Early attempts at automatic program verification were premature. But much progress has been made in the last thirty five years, both in hardware capacity and in the software technologies for verification. I suggest that the renewed challenge of an automatic verifying compiler could provide a focus for interaction, cross-fertilisation, advancement and experimental evaluation of all the technologies of interest in this conference.
Perhaps by concerted international effort, we may be able to meet this challenge, only fifty years after it was proposed by Jim King. We only have fifteen years left to do it."
2005,"Verified Software: Theories, Tools, Experiments Vision of a Grand Challenge Project.","Summary
The ideal of correct software has long been the goal of research in Computer Science. We now have a good theoretical understanding of how to describe what programs do, how they do it, and why they work.This understanding has already been applied to the design, development and manual verification of simple programs of moderate size that are used in critical applications.Automatic verification could greatly extend the benefits of this technology.
This paper argues that the time is ripe to embark on an international Grand Challenge project to construct a program verifier that would use logical proof to give an automatic check of the correctness of programs submitted to it.Prototypes for the program verifier will be based on a sound and complete theory of programming; they will be supported by a range of program construction and analysis tools; and the entire toolset will be evaluated and evolve by experimental application to a large and widely representative sample of useful computer programs.The project will provide the scientific basis of a solution for many of the problems of programming error that afflict all builders and users of software today.
This paper starts with an optimistic vision of a possible long-term future of reliable programming. It argues that scientific research will play an essential role in reaching these long-term goals. It suggests that their achievement should be accelerated by a major international research initiative, modelled on a Grand Challenge, with specific measurable goals. The suggested measure is one million lines of verified code.By definition, this consists of executable programs, together with their specifications, designs, assertions, etc., and together with a machine-checked proof that the programs are consistent with this documentation. We anticipate that the project would last more than ten years, consume over one thousand person-years of skilled scientific effort, drawn from all over the world.Each country will contribute only a proportion of the effort, but all the benefits will be shared by all.
The paper concludes with suggestions for exploratory pilot projects to launch the initiative and with a call to volunteers to take the first steps in the project immediately after this conference.A possible first step will be to revise and improve this paper as a generally agreed report of goals and methods of the scientists who wish to engage in the project."
2004,Process Algebra: A Unifying Approach.,"Abstract
Process algebra studies systems that act and react continuously with their environment. It models them by transition graphs, whose nodes represent their states, and whose edges are labelled with the names of events by which they interact with their environment. A trace of the behaviour of a process is recorded as a sequence of observable events in which the process engages. Refinement is defined as the inclusion of all traces of a more refined process in those of the process that it refines. A simulation is a relation that compares states as well as events; by definition, two processes that start in states related by a simulation, and which then engage in the same event, will end in states also related by the same simulation. A bisimulation is defined as a symmetric simulation, and similarity is defined as the weakest of all simulations. In classical automata theory, the transition graphs are deterministic: from a given node, there is at most one edge with a given label; as a result, trace refinement and similarity coincide in meaning.
Research over many years has produced a wide variety of process algebras, distinguished by the manner in which they compare processes, usually by some form of simulation or by some form of refinement. This paper aims to unify the study of process algebras, by maintaining the identity between similarity and trace refinement, even for non-deterministic systems. Obviously, this unifying approach is entirely dependent on prior exploration of the diversity of theories that apply to the unbounded diversity of the real world. The aim of unification is to inspire and co-ordinate the exploration of yet further diversity; in no way does it detract from the value of such exploration."
2004,Towards the Verifying Compiler.,"Abstract
A verifying compiler is one that proves mechanically that a program is correct before allowing it to be run. Correctness of a program is defined by placing assertions at strategic points in the program text, particularly at the interfaces between its components. From recent enquiries among software developers at Microsoft, I have discovered that assertions are already used widely in program development practice. Their main rÃ´le is as test oracles, to detect programming errors as close as possible to their place of occurrence. Further progress in reliable software engineering is currently supported by programmer productivity tools. I conjecture that these will be developed to exploit assertions of various kinds in various ways at all stages in program development. Eventually assertions will be used more widely for their original purpose of establishing important aspects of the correctness of large programs. However, the construction of a fully verifying compiler remains as a long-term challenge for twenty-first century Computing Science."
2004,A Trace Semantics for Long-Running Transactions.,"Abstract
A long-running transaction is an interactive component of a distributed system which must be executed as if it were a single atomic action. In principle, it should not be interrupted or fail in the middle, and it must not be interleaved with other atomic actions of other concurrently executing components of the system. In practice, the illusion of atomicity for a long-running transaction is achieved with the aid of compensation actions supplied by the original programmer: because the transaction is interactive, familiar automatic techniques of check-pointing and rollback are no longer adequate. This paper constructs a model of long-running transactions within the framework of the CSP process algebra, showing how the compensations are orchestrated to achieve the illusion of atomicity. It introduces a method for declaring that a process is a transaction, and for declaring a compensation for it in case it needs to be rolled back after it has committed. The familiar operator of sequential composition is redefined to ensure that all necessary compensations will be called in the right order if a later failure makes this necessary. The techniques are designed to work well in a highly concurrent and distributed setting. In addition we define an angelic choice operation, implemented by speculative execution of alternatives; its judicious use can improve responsiveness of a system in the face of the unpredictable latencies of remote communication. Many of the familiar properties of process algebra are preserved by these new definitions, on reasonable assumptions of the correctness and independence of the programmer-declared compensations."
2004,Stuck-Free Conformance.,"Abstract
We present a novel refinement relation (stuck-free conformance) for CCS processes, which satisfies the substitutability property: If I conforms to S, and P is any environment such that P | S is stuck-free, then P | I is stuck-free. Stuck-freedom is related to the CSP notion of deadlock, but it is more discriminative by taking orphan messages in asynchronous systems into account. We prove that conformance is a precongruence on CCS processes, thereby supporting modular refinement. We distinguish conformance from the related preorders, stable failures refinement in CSP and refusal preorder in CCS. We have implemented conformance checking in a new software model checker, zing, and we report on how we used it to find errors in distributed programs."
2003,Assertions: A Personal Perspective.,"Abstract:
Assertions are Boolean formulas placed in program text at places where their evaluation will always be true. If the assertions are strong enough, they express everything that the programmers on either side of an interface need to know about the program on the other side, even before the code is written. Indeed, assertions can serve as the basis of a formal proof of the correctness of a complete program."
2003,The verifying compiler: A grand challenge for computing research.,"This contribution proposes a set of criteria that distinguish a grand challenge in science or engineering from the many other kinds of short-term or long-term research problems that engage the interest of scientists and engineers. As an example drawn from Computer Science, it revives an old challenge: the construction and application of a verifying compiler that guarantees correctness of a program before running it."
2003,The Verifying Compiler: A Grand Challenge for Computing Research.,"Abstract
I propose a set of criteria which distinguish a grand challenge in science or engineering from the many other kinds of short-term or long-term research problems that engage the interest of scientists and engineers. As an example drawn from Computer Science, I revive an old challenge: the construction and application of a verifying compiler that guarantees correctness of a program before running it."
2003,The Verifying Compiler: A Grand Challenge for Computing Research.,"Abstract
I propose a set of criteria which distinguish a grand challenge in science or engineering from the many other kinds of short-term or long-term research problems that engage the interest of scientists and engineers. As an example drawn from Computer Science, I revive an old challenge: the construction and application of a verifying compiler that guarantees correctness of a program before running it."
2003,The Verifying Compiler: A Grand Challenge for Computing Research.,"Abstract
I propose a set of criteria which distinguish a grand challenge in science or engineering from the many other kinds of short-term or long-term research problems that engage the interest of scientists and engineers.
The primary purpose of the formulation and promulgation of a grand challenge is to contribute to the advancement of some branch of science or engineering. A grand challenge represents a commitment by a significant section of the research community to work together towards a common goal, agreed to be valuable and achievable by a team effort within a predicted timescale. The challenge is formulated by the researchers themselves as a focus for the research that they wish to pursue in any case, and which they believe can be pursued more effectively by advance planning and co-ordination. Unlike other common kinds of research initiative, a grand challenge should not be triggered by hope of short-term economic, commercial, medical, military or social benefits; and its initiation should not wait for political promotion or for prior allocation of special funding. The goals of the challenge should be purely scientific goals of the advancement of skill and of knowledge. It should appeal not only to the curiosity of scientists and to the ambition of engineers; ideally it should appeal also to the imagination of the general public; thereby it may enlarge the general understanding and appreciation of science, and attract new entrants to a rewarding career in scientific research.
As an example drawn from Computer Science, I revive an old challenge: the construction and application of a verifying compiler that guarantees correctness of a program before running it. A verifying compiler uses automated mathematical and logical reasoning methods to check the correctness of the programs that it compiles. The criterion of correctness is specified by types, assertions, and other redundant annotations that are associated with the code of the program, often inferred automatically, and increasingly often supplied by the original programmer. The compiler will work in combination with other program development and testing tools, to achieve any desired degree of confidence in the structural soundness of the system and the total correctness of its more critical components. The only limit to its use will be set by an evaluation of the cost and benefits of accurate and complete formalization of the criterion of correctness for the software."
2003,The Verifying Compiler: A Grand Challange for Computing Research.,"Abstract
I propose a set of criteria which distinguish a grand challenge in science or engineering from the many other kinds of short-term or long-term research problems that engage the interest of scientists and engineers. As an example drawn from Computer Science, I revive an old challenge: the construction and application of a verifying compiler that guarantees correctness of a program before running it."
2002,Assertions in Modern Software Engineering Practice.,"Assertions are Boolean formulas placed in program text at places where their evaluation will always be true. If the assertions are strong enough, they express everything that the programmers on either side of an interface need to know about the program on the other side, even before the code is written. Indeed, assertions can serve as the basis of a formal proof of the correctness of a complete program."
2002,Assertions in Programming: From Scientific Theory to Engineering Practice.,"Abstract
An assertion in a computer program is a logical formula (Boolean expression) which the programmer expects to evaluate to true on every occasion that program control reaches the point at which it is written. Assertions can be used to specify the purpose of a program, and to define the interfaces between its major components. An early proponent of assertions was Alan Turing (1948), who suggested their use in establishing the correctness of large routines. In 1967, Bob Floyd revived the idea as the basis of a verifying compiler that would automatically prove the correctness of the programs that it compiled. After reading his paper, I became a member of a small research school devoted to exploring the idea as a theoretical foundation for a top-down design methodology of program development. I did not expect the research to influence industrial practice until after my retirement from academic life, thirty years ahead. And so it has been.
In this talk, I will describe some of the ways in which assertions are now used in Microsoft programming practice. Mostly they are used as test oracles, to detect the effects of a program error as close as possible to its origin. But they are beginning to be exploited also by program analysis tools and even by compilers for optimisation of code. One purpose that they are never actually used for is to prove the correctness of programs. This story is presented as a case study of the way in which scientific research into ideals of accuracy and correctness can find unexpected application in the essentially softer and more approximative tasks of engineering."
2002,Towards the Verifying Compiler.,"Abstract
A verifying compiler is one that proves automatically that a program is correct before allowing it to be run. Correctness of a program is defined by placing assertions at strategic points in the program text, particularly at the interfaces between its components. From recent enquiries among software developers at Microsoft, I have discovered that assertions are widely used in program development practice. Their main role is as test oracles, to detect programming errors as close as possible to their place of occurrence. Further progress in reliable software engineering is supported by programmer productivity tools that exploit assertions of various kinds in various ways at all stages in program development. The construction and exploitation of a fully verifying compiler remains as a long-term challenge for twenty-first century Computing Science. The results of this research will be of intermediate benefit long before the eventual ideal is reached."
2001,Legacy.,"Abstract
An increasing proportion of the effort of skilled programmers is devoted to servicing the legacy of software. The techniques and tools currently in use to tackle the problem take good advantage of the results of past research into programming theory. I suggest that new generations of tools will be based on concepts and principles developed by basic research of the present and by future research directed at currently outstanding challenges. These points are illustrated by examples drawn from my personal experience. They show that academic research and education can contribute to industrial development and production in an atmosphere of mutual respect for their different allegiances and timescales, and in recognition of convergence of their long-term goals."
2001,Growing Use of Assertions.,n/a
2000,A Hard Act to Follow.,n/a
2000,Unifying theories of healthiness condition.,"Abstract:
A theory of programming starts with a complete Boolean algebra of specifications, and defines healthiness conditions which exclude infeasibility of implementation. These are expressed as algebraic laws useful for transformation and optimisation of designs. Programming notations and languages must be restricted to those preserving all the healthiness conditions. We have explored a wide range of programming paradigms, including nondeterministic, sequential, parallel, logical and probabilistic. In all cases, we have found a single healthiness condition, formalised by constructions due to Karoubi and to Kleisli. The uniformity maintains for all paradigms a single notion of correctness throughout the chain that leads from specification through designs to programs that are proved to meet the original specification."
2000,Algebraic derivation of an operational semantics.,n/a
2000,Legacy Code.,n/a
2000,Assertions.,"Abstract
An assertion is a Boolean formula written in the text of a program, which the programmer asserts will always be true when that part of the program is executed. It species an internal interface between that part of the program that comes before it and all that follows it. In the software industry today, assertions are conditionally compiled in test runs of a program, and help in the detection and diagnosis of errors. Alan Turing first proposed assertions as a means of checking a large routine. They were rediscovered independently by Naur as generalised snapshots, and by Floyd, who used them to assign meanings to programs. Floyd suggested that if the internal assertions were strong enough, they would constitute a formal proof of the correctness of a complete program. In this lecture, I will summarise the subsequent development of the idea, and describe some of its practical impact."
1999,Linking Theories in Probabilistic Programming.,"Abstract
This paper presents a theory of probabilistic programming based on relational calculus through a series of stages; each stage concentrates on a different and smaller class of program, defined by the healthiness conditions of increasing strength. At each stage we show that the notations of the probabilistic language conserve the healthiness conditions of their operands, and that every theory conserves the definition of recursion."
1999,Theories of Programming: Top-Down and Bottom-Up and Meeting in the Middle.,"Abstract
A theory of programming provides a scientific basis for programming practices that lead to predictable delivery of programs of high quality. A topÂ­down theory starts with a specification of the intended behaviour of a program; and a bottomÂ­up theory starts with a description of how the program is executed. The aim of both theories is to prove theorems (often algebraic laws) that will be helpful in the design, development, compilation, testing, optimisation and maintainance of all kinds of program. The most mature theories are those that are presented both in bottomÂ­up and top-down fashion, where essentially the same laws are valid in both presentations."
1999,A Trace Model for Pointers and Objects.,"Abstract
Object-oriented programs [Dahl, Goldberg, Meyer] are notoriously prone to the following kinds of error, which could lead to increasingly severe problems in the presence of tasking
1.
Following a null pointer
 2.
Deletion of an accessible object
 3.
Failure to delete an inaccessible object
 4.
Interference due to equality of pointers
 5.
Inhibition of optimisation due to fear of (4)
  Type disciplines and object classes are a great help in avoiding these errors. Stronger protection may be obtainable with the help of assertions, particularly invariants, which are intended to be true before and after each call of a method that updates the structure of the heap. This note introduces a mathematical model and language for the formulation of assertions about objects and pointers, and suggests that a graphical calculus [Curtis, Lowe] may help in reasoning about program correctness. It deals with both garbage-collected heaps and the other kind. The theory is based on a trace model of graphs, using ideas from process algebra; and our development seeks to exploit this analogy as a unifying principle."
1999,Theories of Programming: Top-Down and Bottom-Up and Meeting in the Middle.,"Abstract
The goal of scientific research is to develop an understanding of the complexity of the world which surrounds us. There is certainly enough complexity out there to justify a wide range of specialist branches of science; and within each branch to require a wide range of investigatory styles and techniques."
1999,Algebra of Logic Programming.,"A declarative programming language has two kinds of semantics. The more abstract helps in reasoning about specifications and correctness, while an operational semantics determines the manner of program execution. A correct program should reconcile its abstract meaning with its concrete interpretation. To help in this, we present a kind of algebraic semantics for logic programming. It lists only those laws that are equally valid for predicate calculus and for the standard depth-first strategy of Prolog. An alternative strategy is breadth-first search, which shares many of the same laws. Both strategies are shown to be special cases of the most general strategy, that for tree searching. The three strategies are defined in the lazy functional language Haskell, so that each law can be proved by standard algebraic reasoning. The laws are an enrichment of the familiar categorical concept of a monad, and the links between such monads are explored. 1 Introduction In an earlier p...
"
1999,A Semantics for Imprecise Exceptions.,"Some modern superscalar microprocessors provide only imprecise exceptions. That is, they do not guarantee to report the same exception that would be encountered by a straightforward sequential execution of the program. In exchange, they offer increased performance or decreased chip area (which amount to much the same thing).This performance/precision tradeoff has not so far been much explored at the programming language level. In this paper we propose a design for imprecise exceptions in the lazy functional programming language Haskell. We discuss several designs, and conclude that imprecision is essential if the language is still to enjoy its current rich algebra of transformations. We sketch a precise semantics for the language extended with exceptions.The paper shows how to extend Haskell with exceptions without crippling the language or its compilers. We do not yet have enough experience of using the new mechanism to know whether it strikes an appropriate balance between expressiveness and performance."
1998,Unifying theories of programming.,"Professional practice in a mature engineering discipline is based on relevant scientific theories, usually expressed in the language of mathematics. A mathematical theory of programming aims to provide a similar basis for specification, design and implementation of computer programs. The theory can be presented in a variety of styles, including 1. Denotational, relating a program to a specification of its observable properties and behaviour 2. Algebraic, providing equations and inequations for comparison, transformation and optimization of designs and programs 3. Operational, describing individual steps of a possible mechanical implementation. This paper presents simple theories of sequential non-deterministic programming in each of these three styles; by deriving each presentation from its predecessor in a cyclic fashion, mutual consistency is assured."
1997,Unifying Theories for Parallel Programming.,"Abstract
The progress of science involves a constant interplay between diversification and unification. Diversification extends the boundaries of science to cover new and wider ranges of phenomena; successful unification reveals that a range of experimentally validated theories are no more than particular cases of some more general principle. The cycle continues when the general principle reveals further directions for experimental investigation. This paper suggests that the time has come to attempt a unifying classification of theories of parallel programming. Ideally, this should provide a common basis for reasoning about specifications and the correctness of designs, for optimising programs by algebraic transformation, and for implementing them in a range of technologies on a variety of machine architectures, to satisfy the needs of a wide range of applications."
1996,Unifying Theories : A Personal Statement.,n/a
1996,The logic of engineering design.,n/a
1996,How Did Software Get So Reliable Without Proof?,"Abstract
By surveying current software engineering practice, this paper reveals that the techniques employed to achieve reliability are little different from those which have proved effective in all other branches of modern engineering: rigorous management of procedures for design inspection and review; quality assurance based on a wide range of targeted tests; continuous evolution by removal of errors from products already in widespread use; and defensive programming, among other forms of deliberate over-engineering. Formal methods and proof play a small direct role in large scale programming; but they do provide a conceptual framework and basic understanding to promote the best of current practice, and point directions for future improvement."
1996,"The Role of Formal Techniques: Past, Current and Future or How Did Software Get so Reliable without Proof? (Extended Abstract).",
1996,Mathematical models for computing science.,"Mathematical equations and other predicates are used in the physical sciences to formalise, describe, and predict the observable behaviour of some isolatable fragment of the real world. Phenomena of interest in computing science can with advantage be formalised as mathematical predicates in the same scientiic way. As a result, concurrency can often be modelled by conjunction 5], non-determinism by disjunction, locality by existential quan-tiication, and correctness by logical implication 7]. This thesis is illustrated by application at a variety of levels of granularity, scale and abstraction: in hardware, by switching, combinational and sequential circuitry 9, 6]; and in programming 4] by the procedural 2], parallel 1], and logical 8] paradigms. The major achievement of modern science is to demonstrate the links between phenomena at diierent levels of abstraction and generality, from quarks, particles, atoms and molecules right through to stars, galaxies, and (more conjecturally) the entire universe. On a less grand scale, the computer scientist has to establish such links in every implementation of higher level concepts in terms of lower 10, 11]. Such links are also formalised as equations or more general predicates, describing the relationships between observations made at diierent levels of abstraction 3]. Their clariication assists in understanding the structure of an entire scientific discipline."
1995,Sequential Calculus.,"Abstract
This paper presents an algebraic calculus like the relational calculus for reasoning about sequential phenomena. It provides a common foundation for several proposed models of concurrent or reactive systems. It is clearly differentiated from the relational calculus by absence of a general converse operation. This permits the treatment of temporal logic within the sequential calculus."
1995,Unification of Theories: A Challenge for Computing Science.,"Abstract
Unification of theories is the long-standing goal of the natural sciences; and modern physics offers a spectacular paradigm of its achievement. The structure of modern mathematics has also been determined by its great unifying theories â€” topology, algebra and the like. The same ideals and goals are shared by researchers and students of theoretical computing science."
1994,Editorial.,n/a
1994,Provably Correct Systems.,"Abstract
The goal of the Provably Correct Systems project (ProCoS) is to develop a mathematical basis for development of embedded, real-time, computer systems. This survey paper introduces the specification languages and verification techniques for four levels of development: Requirements definition and control design; Transformation to a systems architecture with program designs and their transformation to programs; Compilation of real-time programs to conventional processors, and Compilation of programs to hardware."
1994,Hardware and Software: The Closing Gap.,"Abstract
The study of computing science is split at an early stage between the branches dealing separately with hardware and software; and there is a corresponding split in later professional specialisation. This paper explores the essential unity and overlap of the two branches. The basic concepts are those of occam, taken as a simple example of a high-level programming language; its notations may be translated by the laws of programming to the machine code of a conventional machine. Almost identical transformations can produce the networks of gates and flip-flops which constitute a hardware design. These insights are being exploited in hybrid systems, implemented partly in hardware and partly in software. A TRAM-standard printed circuit board called HARP has been constructed for such applications. It links a transputer by shared memory with a commercial Field Programmable Gate Array. Prospects for application are discussed."
1993,Normal Form Approach to Compiler Design.,"Abstract
This paper demonstrates how reduction to normal form can help in the design of a correct compiler for Dijkstra's guarded command language. The compilation strategy is to transform a source program, by a series of algebraic manipulations, into a normal form that describes the behaviour of a stored-program computer. Each transformation eliminates high-level language constructs in favour of lower-level constructs. The correctness of the compiler follows from the correctness of each of the algebraic transformations."
1993,From Algebra to Operational Semantics.,n/a
1993,Algebra and Models.,"Science makes progress by constructing mathematical models, deducing their observable consequences, and testing them by experiment. Successful theoretical models are later taken as the basis for engineering methods and codes of practice for design of reliable and useful products. Models can play a similar central role in the progress and practical application of Computing Science.A model of a computational paradigm starts with choice of a carrier set of potential direct or indirect observations that can be made of a computational process. A particular process is modelled as the subset of observations to which it can give rise. Process composition is modelled by relating observations of a composite process to those of its components. Indirect observations play an essential role in such compositions. Algebraic properties of the composition operators are derived with the aid of the simple theory of sets and relations. Feasibility is checked by a mapping from a more operational model.A model constructed as a family of sets is easily adapted as a calculus of design for total correctness. A specification is given by an arbitrary set containing all observations permitted in the required product. It should be expressed as clearly as possible with the aid of the full power of mathematics and logic. A product meets a specification if its potential observations form a subset of its permitted observations. This principle requires that all envisaged failure modes of a product are modelled as indirect observations, so that their avoidance can be proved. Specifications of components can be composed mathematically by the same operators as the components themselves. This permits top-down proof of correctness of designs even before their implementation begins. Algebraic properties and reasoning are helpful throughout development. Non-determinism is seen as no problem, but rather as a part of the solution."
1992,A Model for Synchronous Switching Circuits and its Theory of Correctness.,"Following Bryant [2], an algorithm is given for translating a switching circuit design into a program that simulates its dynamic behavior. A theory of assertions based on Dijkstra [5] and UNITY [4] is then developed to formalize specifications of hardware circuit designs and to establish their correctness. Both combinational and sequential circuits are taken into account both in N-mos and C-mos; the latter turns out to be much simpler.
"
1992,Programs are Predicates.,"A computer program is identified with the strongest predicate describing every relevant observation that can be made of the behaviour of a computer executing that program. A programming language is a subset of logical and mathematical notations, which is so restricted that products described in the language can be automatically implemented on a computer. The notations enjoy a number of elegant algebraic properties, which can be used for optimizing program efficiency. A specification is a predicate describing all permitted observations of a program, and it may be expressed with greatest clarity by taking advantage of the whole language of logic and mathematics. A program P meets its specification S iff . The proof of this implication may use all the classical methods of mathematics and logic. These points are illustrated by design of a small language that includes assignments, conditions, non-determinism, recursion, input, output, and concurrency. "
1992,Algebra and Models.,"Abstract
Science makes progress by constructing mathematical models, deducing their observable consequences, and testing them by experiment. Successful theoretical models are later taken as the basis for engineering methods and codes of practice for design of reliable and useful products. Models can play a similar central role in the progress and practical application of Computing Science.
A model of a computational paradigm starts with choice of a carrier set of potential direct or indirect observations that can be made of a computational process. A particular process is modelled as the subset of observations to which it can give rise. Process composition is modelled by relating observations of a composite process to those of its components. Indirect observations play an essential role in such compositions. Algebraic properties of the composition operators are derived with the aid of the simple theory of sets and relations. Feasibility is checked by a mapping from a more operational model.
A model constructed as a family of sets is easily adapted as a calculus of design for total correctness. A specification is given by an arbitrary set containing all observations permitted in the required product. It should be expressed as clearly as posssible with the aid of the full power of mathematics and logic. A product meets a specification if its potential observations form a subset of its permitted observations. This principle requires that all envisaged failure modes of a product are modelled, as indirect observations, so that their avoidance can be proved. Specifications of components can be composed mathematically by the same operators as the components themselves. This permits top-down proof of correctness of designs even before their implementation begins. Algebraic properties and reasoning are helpful throughout development. Non-determinism is seen as no problem, but rather as a part of the solution."
1991,The transputer and occam: A personal story.,"The paper tells the story of the development over twenty?five years of my ideas about communicating sequential processes. One of its most subtle and most useful facilities is the guarded choice, which appears as the ALT command in occam. Its subtleties are clarified and its usefulness increased by an understanding of the relevant simple algebraic laws, based on mathematical research conducted at Oxford. The abstractions provided by mathematics are the secret of the versatility of occam, which can be reliably and efficiently implemented by multiprogramming or by multiprocessing or by hardware, or by any combination of these. In conclusion, it is conjectured that the occam programming paradigm will remain the most efficient and most reliable for the general?purpose shared?store multiprocessors of the future.
"
1991,A Calculus of Durations.,"Abstract
The purpose of the calculus of durations is to reason about designs and requirements for time-critical systems, without explicit mention of absolute time. Its distinctive feature is reasoning about integrals of the durations of different states within any given interval. The first section introduces the running example, of leakage in a gas burner. The second section defines and axiomatises the proposed calculus as an extension of interval temporal logic. The third section applies it to the problem described in the introduction. The fourth section briefly surveys alternative calculi."
1991,Pre-Adjunctions in Order Enriched Categories.,"Category theory offers a unified mathematical framework for the study of specifications and programs in a variety of styles, such as procedural, functional and concurrent. One way that these different languages may be treated uniformly is by generalising the definitions of some standard categorical concepts. In this paper we reproduce in the generalised theory analogues of some standard theorems on isomorphism, and outline their applications to programming languages.
"
1991,A Theory for the Derivation of Combinational C-MOS Circuit Designs.,"Abstract
This paper shows how propositional logic may be used to reason about synchronous combinational switching circuits implemented in C-mos. It develops a simple formalism and theory for describing and predicting their behaviour. On this it builds a calculus of design which is driven by proof obligations. The design philosophy for software introduced in [1] is thereby extended to a certain kind of hardware design. No prior knowledge of hardware is assumed of the reader; but useful background, motivation, examples and pictures may be found in [2]. Many of the problems described in that paper have been solved in this one."
1990,Fixed Points of Increasing Functions.,"Abstract
If d and e are increasing functions in a partial order, then the fixed points on their functional composition (d âˆ˜ e) are just the points that are fixed for both d and e. The same is true of a form of parallel composition (fx V gx), using the least upper bound V in the partial order. This fact may be useful in cases when fixed points are computed by iteration, with arbitrary mixture of sequential and parallel composition."
1990,Let's Make Models (Abstract).,n/a
1990,A Theory of Synchrony and Asynchrony.,"Loosely-coupled (asynchronous) data flow networks are often contrasted to tightly-coupled (synchronous) systems. We present CSP as a unified theory for both types of system, and deduce algebraic laws relating them. The theory may be useful in design and implementation of systems from parts which take advantage of both paradigms. "
1990,A Theory of Conjunction and Concurrency.,"Abstract:
Some general conditions under which the specification of a concurrent system can be expressed as the conjunction of specifications for its component processes are explored. A lattice-theoretic fixed-point theorem about increasing functions is proved, and examples of its application in several areas of computing science are given. Some consequences are drawn for the design of concurrent algorithms, high-level programming languages, and fine-grained concurrent computer architectures.< >"
1989,Categorical Semantics for Programming Languages.,n/a
1989,The Varieties of Programming Language.,n/a
1988,The Laws of Occam Programming.,"Abstract
One of the attractive features of occam is the large number of memorable algebraic laws which exist relating programs. We investigate these laws and, by discovering a normal form for WHILE-free programs, show that they completely characterise the language's semantics."
1988,Partial Correctness of C-MOS Switching Circuits: An Exercise in Applied Logic.,"Abstract:
The possibility of extending some of the logical methods that have been recommended for the design of software to the design of hardware, in particular, of synchronous switching circuits implemented in CMOS, is explored. The objective is to design networks that are known by construction. Things that can go wrong with circuits designed in this way are examined. The application of the techniques is discussed.< >"
1987,Laws of Programming.,A complete set of algebraic laws is given for Dijkstra's nondeterministic sequential programming language. Iteration and recursion are explained in terms of Scott's domain theory as fixed points of continuous functionals. A calculus analogous to weakest preconditions is suggested as an aid to deriving programs from their specifications.
1987,An Overview of Some Formal Methods for Program Design.,
1987,Algebraic Specification and Proof of a Distributed Recovery Algorithm.,"An algebraic specification is given of an algorithm for recovery from catastrophe by a deterministic process. A second version of the algorithm also includes check-points. The algorithms are formulated in the notations of Communicating Sequential Processes (Hoare 1985) and the proofs of correctness are conducted wholly by application of algebraic laws (together with the unique fixed point theorem).
"
1987,The Weakest Prespecification.,n/a
1987,Prespecification in Data Refinement.,"Abstract
In data refinement, a concrete data type replaces an abstract data type used in the design of an algorithm or system (Gries and Prins, 1985; Hoare, 1972; Jones, 1980). We present two methods for calculating the weakest specification of each operation on a concrete data type from the specification of the corresponding abstract operation, together with a single simulation relation (Milner, 1980; Park, 1981), which specifies the correspondence between the two types. The methods are proved sound and (jointly) complete for a nondeterministic procedural programming language slightly more powerful than Dijkstra's (1976). Operations (in general, nondeterministic) are represented by relations, and significant use is made of prespecification and postspecification (Hoare and He, Jifeng, 1987)."
1986,Specification-Oriented Semantics for Communicating Processes.,"Summary
A process P satisfies a specification S if every observation we can make of the behaviour of P is allowed by S. We use this idea of process correctness as a starting point for developing a specific form of denotational semantics for processes, called here specification â€” oriented semantics. This approach serves as a uniform framework for generating and relating a series of increasingly sophisticated denotational models for Communicating Processes.
These models differ in the underlying structure of their observations which influences both the number of representable language operators and the induced notion of process correctness. Safety properties are treated by all models; the more sophisticated models also permit proofs of certain liveness properties. An important feature of the models is a special hiding operator which abstracts from internal process activity. This allows large processes to be composed hierarchically from networks of smaller ones in such a way that proofs of the whole are constructed from proofs of its components. We also show the consistency of our denotational models w.r.t. a simple operational semantics based on transitions which make internal process activity explicit."
1986,Data Refinement Refined.,"Abstract
We consider the original work of Hoare and Jones on data refinement in the light of Dijkstra and Smyth's treatment of nondeterminism and of Milner and Park's definition of the simulation of Communicating Systems. Two proof methods are suggested which we hope are simpler and more general than those in current use. They are proved to be individually sufficient for the correctness of refinement and together necessary for it. The proof methods can be employed to derive the weakest specification of an implementation from its abstract specification."
1985,A Couple of Novelties in the Propositional Calculus.,n/a
1985,The Mathematics of Programming.,"Abstract
I hold the principle that the construction of computer programs is a mathematical activity like the solution of differential equations. Programs can be derived from their specifications by mathematical insight calculation and proof, using algebraic laws as simple and elegant as those of elementary arithmetic."
1984,A Theory of Communicating Sequential Processes.,"A mathematical model for communicating sequential processes is given, and a number of its interesting and useful properties are stated and proved. The possibilities of nondetermimsm are fully taken into account."
1984,Programming: Sorcery or Science?,"Abstract:
Professional programming practice should be based on underlying mathematical theories and follow the traditions of better-established engineering disciplines. Success will come through improved education."
1984,Programs as Executable Predicates.,"A computer program is identified with the strongest predicate describing every relevant observation that can be made of the behaviour of a computer executing that program. A programming language is a subset of logical and mathematical notations, which is "
1983,An Axiomatic Basis for Computer Programming (Reprint).,"In this paper an attempt is made to explore the logical foundations of computer programming by use of techniques which were first applied in the study of geometry and have later been extended to other branches of mathematics. This involves the elucidation of sets of axioms and rules of inference which can be used in proofs of the properties of computer programs. Examples are given of such axioms and rules, and a formal proof of a simple theorem is displayed. Finally, it is argued that important advantages, both theoretical and practical, may follow from a pursuance of these topics."
1983,Communicating Sequential Processes (Reprint).,"This paper suggests that input and output are basic primitives of programming and that parallel composition of communicating sequential processes is a fundamental program structuring method. When combined with a development of Dijkstra's guarded command, these concepts are surprisingly versatile. Their use is illustrated by sample solutions of a variety of familiar programming exercises."
1983,A More Complete Model of Communicating Processes.,"Abstract
A previous paper by Hoare gives axioms and proof rules for communicating processes that provide a calculus of total correctness. This paper gives explicit definitions of communicating processes as predicates. The former axioms and proof rules become theorems, proved using the explicit definitions. The defining predicates are more powerful than the proof rules for reasoning about processes, but less often useful for their construction. An implementation of the processes using partial recursive functions is given."
1983,Specification of a simplified Network Service in CSP.,"Abstract
The specification of a simplified Network Service is given in CSP.
Because specifications of such services are notoriously difficult, it is especially pleasing that in this case the problem can be decomposed into several more easily achieved sub-specifications. This decomposition greatly improves the credibility of the overall specification."
1983,Specification-Oriented Semantics for Communicating Processes.,"Abstract
We are aiming at a classification of semantical models for Communicating Processes that will enable us to recommend certain models which are just detailed enough for particular applications. But before such an aim can be fully realised, more sophisticated models of processes should be studied.
For example, we have not considered the notion of state so far. This would allow to add assignment and explicit value passing between processes, thus combining sequential programs with Communicating Processes.
It is also important to ensure that the operators satisfy the usual algebraic laws, for example parallel composition should be associative. And the relationship between specification-oriented denotational semantics used here and the operational semantics used in [12,13,16] should be studied. This requires an explicit concept of divergence. In particular, it is interesting to investigate how the criterion P sat S can be derived systematically from the operational semantics. A significant step in this direction has already been made in [14].
Finally, an explicit syntax for specifications and proof systems for the relation P sat S should be developed. First proposals for such proof systems can be found in [5,9]."
1981,A Calculus of Total Correctness for Communicating Processes.,"Abstract
A process communicates with its environment and with other processes by syncronized output and input on named channels. The current state of a process is defined by the sequences of messages which have passed along each of the channels, and by the sets of messages that may next be passed on each channel. A process satisfies an assertion if the assertion is at all times true of all possible states of the process. We present a calculus for proving that a process satisfies the assertion describing its intended behaviour. The following constructs are axiomatised: output; input; simple recursion; disjoint parallelism; channel renaming, connection and hiding; process chaining; nondeterminism; conditional; alternation; and mutual recursion. The calculus is illustrated by proof of a number of simple buffering protocols."
1981,Partial Correctness of Communicating Sequential Processes.,n/a
1981,Ambiguities and Insecurities in Pascal.,"Ambiguities and insecurities in the programming language Pascal are discussed.
"
1980,A Theory of Nondeterminism.,"Abstract
A construction is described which takes an arbitrary set of machines, an arbitray set of tests, and an arbitrary relation on machines and tests defining which machines pass which tests. It produces a domain of specifications, which is a retract of the lattice of sets of tests (with the subset ordering), and a domain of nondeterministic machines (ndms), which is a retract of the lattice of sets of machines (with the superset ordering). These two domains are isomorphic. Simple conditions ensure that they are Ï‰-algebraic.
Functions on such domains may be defined equivalently either as transformations of ndms (an operational definition) or as transformations of specifications (an axiomatic definition). Conditions for the ""realism"" of such functions are formulated."
1979,"Semantics of Nondeterminism, Concurrency, and Communication.",n/a
1978,Communicating Sequential Processes.,"This paper suggests that input and output are basic primitives of programming and that parallel composition of communicating sequential processes is a fundamental program structuring method. When combined with a development of Dijkstra's guarded command, these concepts are surprisingly versatile. Their use is illustrated by sample solutions of a variety of a familiar programming exercises."
1978,Some Properties of Predicate Transformers.,
1978,Software Engineering: A Keynote Address.,"This paper argues that our recent progress in the development of a sound programming methodology should not lead us to ignore the more difficult aspects of engineering; and that in future we should pay more attention to the quality of our designs, and not just the accuracy of their implementation."
1978,"Semantics of Nondeterminism, Concurrency and Communication (Extended Abstract).",n/a
1977,Fast Fourier Transform Free From Tears.,"Many descriptions of Fast Fourier Transform exist in the literature. Several of these appeal to matrix concepts, such as Kronecker multiplication. This paper shows the essential simplicity of the algorithm and the reasoning behind it. However it deals only with the case when the number of points is an exact power of 2.
"
1977,Ambiguities and Insecurities in Pascal.,"Ambiguities and insecurities in the programming language Pascal are discussed.
"
1976,Parallel Programming: An Axiomatic Approach.,"Abstract
This paper develops some ideas expounded in [1]. It distinguishes a number of ways of using parallelism, including disjoint processes, competition, cooperation, and communication. In each case an axiomatic proof rule is given."
1976,Quasiparallel Programming.,"This paper describes SIMONE, an extension of PASCAL,1 which provides the quasiparallel programming facility of SIMULA 67, but without classes or references. The language is intended to be suitable for the design, testing and simulation of operating system algorithms. It is illustrated by simple examples, suitable as project material in a course on operating systems. A simple, restricted, but efficient implementation is described. It is suggested that the language might be suitable for more general simulation purposes, and an example of a general job shop simulation is given.
"
1975,Matrix Reduction - An Efficient Method.,"The paper describes an efficient method for reduction of the binary matrices which arise in some school time-tabling problems. It is a development of that described by John Lions. It has been generalized and adapted to fit into the complete timetabling process; to use a more compact data representation and more efficient processing techniques; to take fuller advantage of possible available previous knowledge about the matrix. And it is designed as a structured program, which can readily be coded by the reader in the high level or low level programming language of his choice. Practical tests of the method have shown it to be a good basis for a realistic timetabling algorithm."
1975,Recursive data structures.,"The power and convenience of a programming language may be enhanced for certain applications by permitting treelike data structures to be defined by recursion. This paper suggests a pleasing notation by which such structures can be declared and processed; it gives the axioms which specify their properties, and suggests an efficient implementation method. It shows how a recursive data structure may be used to represent another data type, for example, a set. It then discusses two ways in which significant gains in efficiency can be made by selective updating of structures, and gives the relevant proof rules and hints for implementation. The examples show that a certain range of applications in symbol manipulation can be efficiently programmed without introducing the low-level concept of a reference into a high-level programming language.
"
1975,Parallel programming: an axiomatic approach.,"Abstract
This paper develops some ideas expounded in [1]. It distinguishes a number of ways of using parallelism, including disjoint processes, competition, cooperation, communication and ""colluding"". In each case an axiomatic proof rule is given. Some light is thrown on traps or ON conditions. Warning: the program structuring methods described here are not suitable for the construction of operating systems."
1975,Proof of correctness of data representation.,"Abstract
A powerful method of simplifying the proofs of program correctness is suggested; and some new light is shed on the problem of functions with side-effects."
1975,The structure of an operating system.,"Abstract
This paper describes the use of the class and inner concepts of SIMULA 67 to express the multi-level structure of an operating system. A comparison is drawn between compile-time checking and run-time protection."
1974,Consistent and Complementary Formal Theories of the Semantics of Programming Languages.,"Summary
This paper presents a comparative study of different methods for formal description of programming languages. These methods have been applied to a simple but realistically usable programming language; the more abstract definitions have been proved to be consistent relative to the more concrete ones."
1974,Monitors: An Operating System Structuring Concept.,"This paper develops Brinch-Hansen's concept of a monitor as a method of structuring an operating system. It introduces a form of synchronization, describes a possible method of implementation in terms of semaphores and gives a suitable proof rule. Illustrative examples include a single resource scheduler, a bounded buffer, an alarm clock, a buffer pool, a disk head optimizer, and a version of the problem of readers and writers."
1974,Optimization of Store Size for Garbage Collection.,n/a
1973,An Axiomatic Definition of the Programming Language PASCAL.,"The axiomatic definition method proposed in reference [5] is extended and applied to define the meaning of the programming language PASCAL [1]. The whole language is covered with the exception of real arithmetic and go to statements.
"
1973,A Structured Paging System.,"The principles and practices of structured programming have been expounded and illustrated by relatively small examples (Dahl, Dijkstra, Hoare, 1972). Systematic methods for the construction of parallel algorithms have also been suggested (Dijkstra, 1968, a, b). This paper attempts to extend structured programming methods to a program intended to operate in a parallel environment, namely a paging system for the implementation of virtual store. The design decisions are motivated by considerations of cost of effectiveness.
The purpose of a paging system is taken to be the sharing of main and backing store of a computer among a number of users making unpredictable demands upon them; and to do so in such a way that each user will not be concerned whether his information is stored at any given time on main or backing store. For the sake of definiteness, the backing store is taken here to be a sectored drum; but the system could readily be adapted for other devices. Our design does not rely on any particular paging hardware, and it should be implementable in any reasonable combination of hardware and software. Furthermore, it does not presuppose any particular structure of virtual store (linear, two-dimensional, ‘cactus’, etc.) provided to the user program.
"
1973,A General Conservation Law for Queueing Disciplines.,n/a
1972,Program Proving: Jumps and Functions.,"Summary
Proof methods adequate for a wide range of computer programs have been expounded in [1] and [2]. This paper develops a method suitable for programs containing functions, and a certain kind Of jump. The method is illustrated by the proof of a useful and efficient program for table lookup by logarithmic search."
1972,Proof of Correctness of Data Representations.,"Summary
A powerful method of simplifying the proofs of program correctness is suggested; and some new light is shed on the problem of functions with side-effects."
1972,Proof of a structured program: 'the sieve of Eratosthenes'.,"This paper illustrates a method of constructing a program together with its proof. By structuring the program at two levels of abstraction, the proof of the more abstract algorithm may be completely separated from the proof of the concrete representation. In this way, the overall complexity of the proof is kept within more reasonable bounds.
"
1972,Incomputability.,n/a
1972,An axiomatic definition of the programming language PASCAL.,n/a
1971,Proof of a Program: FIND.,"A proof is given of the correctness of the algorithm â€œFind.â€ First, an informal description is given of the purpose of the program and the method used. A systematic technique is described for constructing the program proof during the process of coding it, in such a way as to prevent the intrusion of logical errors. The proof of termination is treated as a separate exercise. Finally, some conclusions relating to general programming methodology are drawn."
1971,Proof of a Recursive Program: Quicksort.,"This paper gives the proof of a useful and non-trivial program, Quicksort (Hoare, 1961). First the general algorithm is described informally; next a rigorous but informal proof of correctness of the coded program is given; finally some formal methods are introduced. Conclusions are drawn on the possibility of enlisting mechanical aid in the proof process.
"
1969,An Axiomatic Basis for Computer Programming.,"In this paper an attempt is made to explore the logical foundations of computer programming by use of techniques which were first applied in the study of geometry and have later been extended to other branches of mathematics. This involves the elucidation of sets of axioms and rules of inference which can be used in proofs of the properties of computer programs. Examples are given of such axioms and rules, and a formal proof of a simple theorem is displayed. Finally, it is argued that important advantage, both theoretical and practical, may follow from a pursuance of these topics."
1968,Data structures in two-level store.,n/a
1966,A contribution to the development of ALGOL.,
1963,The Elliott ALGOL input/output system.,"A description of the method of specifying input and output in ALGOL programs run on the National-Elliott 803 and the Elliott 503 digital computers.
"
1962,Quicksort.,"A description is given of a new method of sorting in the random-access store of a computer. The method compares very favourably with other known methods in speed, in economy of storage, and in ease of programming. Certain refinements of the method, which may be useful in the optimization of inner loops, are described in the second part of the paper.
"
1962,Report on the Elliott ALGOL Translator.,n/a
1961,Algorithm 63: partition.,n/a
1961,Algorithm 65: find.,n/a
1961,Algorithm 64: Quicksort.,n/a
