2019,Massively Parallel Computation of Matching and MIS in Sparse Graphs.,"The Massively Parallel Computation (MPC) model serves as a common abstraction of many modern large-scale parallel computation frameworks and has recently gained a lot of importance, especially in the context of classic graph problems. In this work, we mainly consider maximal matching and maximal independent set problems in the MPC model.
These problems are known to admit efficient MPC algorithms if the space available per machine is near-linear in the number n of nodes. This is not only often significantly more than what we can afford, but also allows for easy if not trivial solutions for sparse graphs---which are common in real-world large-scale graphs. We are, therefore, interested in the low-memory MPC model, where the space per machine is restricted to be strongly sublinear, that is, nŒ¥ for any constant 0 < Œ¥ < 1.
We parametrize our algorithms by the arboricity Œª of the input graph. Our key ingredient is a degree reduction technique that reduces these problems in graphs with arboricity Œª to the corresponding problems in graphs with maximum degree poly(Œª, log n) in O(log2 log n) rounds, giving rise to O(‚àö log Œª ‚ãÖ log log Œª + log 2 log n)-round algorithms.
Our result is particularly interesting for graphs with poly log n arboricity as for such graphs, we get O(log 2 log n)-round algorithms. This covers most natural families of sparse graphs and almost exponentially improves over previous algorithms that all required log Œ©(1) n rounds in this regime of MPC.
Finally, our maximal matching algorithm can be employed to obtain a (1+Œµ)-approximate maximum cardinality matching, a (2+Œµ)-approximate maximum weighted matching, as well as a 2-approximate minimum vertex cover in essentially the same number of rounds."
2017,Random Knockout Tournaments.,"We consider a random knockout tournament among players 1,Ö, n, in which each match involves two players. The match format is specified by the number of matches played in each round, where the constitution of the matches in a round is random. Supposing that there are numbers v1,Ö, vn such that a match between i and j will be won by i with probability vi/(vi + vj), we obtain a lower bound on the tournament win probability for the best player, as well as upper and lower bounds for all of the players. We also obtain additional bounds by considering the best and worst formats for player 1 in the special case v1 > v2 = v3 = Ö = vn.
"
2014,Finding a most biased coin with fewest flips.,"We study the problem of learning a most biased coin among a set of coins by tossing the coins adaptively. The goal is to minimize the number of tosses until we identify a coin whose posterior probability of being most biased is at least 1-Œ¥for a given Œ¥. Under a particular probabilistic model, we give an optimal algorithm, i.e., an algorithm that minimizes the expected number of future tosses. The problem is closely related to finding the best arm in the multi-armed bandit problem using adaptive strategies. Our algorithm employs an optimal adaptive strategy‚Äîa strategy that performs the best possible action at each step after observing the outcomes of all previous coin tosses. Consequently, our algorithm is also optimal for any given starting history of outcomes. To our knowledge, this is the first algorithm that employs an optimal adaptive strategy under a Bayesian setting for this problem. Our proof of optimality employs mathematical tools from the area of Markov games."
2013,The Implicit Hitting Set Approach to Solve Combinatorial Optimization Problems with an Application to Multigenome Alignment.,"We develop a novel framework, the implicit hitting set approach, for solving a class of combinatorial optimization problems. The explicit hitting set problem is as follows: given a set U and a family S of subsets of U, find a minimum-cardinality set that intersects (hits) every set in S. In the implicit hitting set problem (IHSP), the family of subsets S is not explicitly listed (its size is, generally, exponential in terms of the size of U); instead, it is given via a polynomial-time oracle that verifies if a given set H is a hitting set or returns a set in S that is not hit by H. Many NP-hard problems can be straightforwardly formulated as implicit hitting set problems. We show that the implicit hitting set approach is valuable in developing exact and heuristic algorithms for solving this class of combinatorial optimization problems. Specifically, we provide a generic algorithmic strategy, which combines efficient heuristics and exact methods, to solve any IHSP. Given an instance of an IHSP, the proposed algorithmic strategy gives a sequence of feasible solutions and lower bounds on the optimal solution value and ultimately yields an optimal solution. We specialize this algorithmic strategy to solve the multigenome alignment problem and present computational results that illustrate the effectiveness of the implicit hitting set approach.
"
2013,Reconstructing Boolean Models of Signaling.,"Since the first emergence of proteinñprotein interaction networks more than a decade ago, they have been viewed as static scaffolds of the signalingñregulatory events taking place in cells, and their analysis has been mainly confined to topological aspects. Recently, functional models of these networks have been suggested, ranging from Boolean to constraint-based methods. However, learning such models from large-scale data remains a formidable task, and most modeling approaches rely on extensive human curation. Here we provide a generic approach to learning Boolean models automatically from data. We apply our approach to growth and inflammatory signaling systems in humans and show how the learning phase can improve the fit of the model to experimental data, remove spurious interactions, and lead to better understanding of the system at hand.
"
2012,Comparing Pedigree Graphs.,"Pedigree graphs, or family trees, are typically constructed by an expensive process of examining genealogical records to determine which pairs of individuals are parent and child. New methods to automate this process take as input genetic data from a set of extant individuals and reconstruct ancestral individuals. There is a great need to evaluate the quality of these methods by comparing the estimated pedigree to the true pedigree. In this article, we consider two main pedigree comparison problems. The first is the pedigree isomorphism problem, for which we present a linear-time algorithm for leaf-labeled pedigrees. The second is the pedigree edit distance problem, for which we present (1) several algorithms that are fast and exact in various special cases, and (2) a general, randomized heuristic algorithm. In the negative direction, we first prove that the pedigree isomorphism problem is as hard as the general graph isomorphism problem, and that the sub-pedigree isomorphism problem is NP-hard. We then show that the pedigree edit distance problem is APX-hard in general and NP-hard on leaf-labeled pedigrees. We use simulated pedigrees to compare our edit-distance algorithms to each other as well as to a branch-and-bound algorithm that always finds an optimal solution.
"
2012,Algorithms to Detect Multiprotein Modularity Conserved during Evolution.,"Abstract:
Detecting essential multiprotein modules that change infrequently during evolution is a challenging algorithmic task that is important for understanding the structure, function, and evolution of the biological cell. In this paper, we define a measure of modularity for interactomes and present a linear-time algorithm, Produles, for detecting multiprotein modularity conserved during evolution that improves on the running time of previous algorithms for related problems and offers desirable theoretical guarantees. We present a biologically motivated graph theoretic set of evaluation measures complementary to previous evaluation measures, demonstrate that Produles exhibits good performance by all measures, and describe certain recurrent anomalies in the performance of previous algorithms that are not detected by previous measures. Consideration of the newly defined measures and algorithm performance on these measures leads to useful insights on the nature of interactomics data and the goals of previous and current algorithms. Through randomization experiments, we demonstrate that conserved modularity is a defining characteristic of interactomes. Computational experiments on current experimentally derived interactomes for Homo sapiens and Drosophila melanogaster, combining results across algorithms, show that nearly 10 percent of current interactome proteins participate in multiprotein modules with good evidence in the protein interaction data of being conserved between human and Drosophila."
2012,An Algorithmic View of the Universe.,"In the years since Alan Turing, and following his lead, computer scientists advanced their understanding of computational phenomena by developing a very specialized, original and penetrating way of rigorous thinking. Now it turns out that this ""algorithmic"" way of thinking can be applied productively to the study of important phenomena outside computation proper (examples: the cell, the brain, the market, the universe, indeed mathematical truth itself). This development is an exquisite unintended consequence of the fact that there is latent computation underlying each of these phenomena, or the ways in which science studies them."
2012,Algorithmic methodologies for ultra-efficient inexact architectures for sustaining technology scaling.,"Owing to a growing desire to reduce energy consumption and widely anticipated hurdles to the continued technology scaling promised by Moore's law, techniques and technologies such as inexact circuits and probabilistic CMOS (PCMOS) have gained prominence. These radical approaches trade accuracy at the hardware level for significant gains in energy consumption, area, and speed. While holding great promise, their ability to influence the broader milieu of computing is limited due to two shortcomings. First, they were mostly based on ad-hoc hand designs and did not consider algorithmically well-characterized automated design methodologies. Also, existing design approaches were limited to particular layers of abstraction such as physical, architectural and algorithmic or more broadly software. However, it is well-known that significant gains can be achieved by optimizing across the layers. To respond to this need, in this paper, we present an algorithmically well-founded cross-layer co-design framework (CCF) for automatically designing inexact hardware in the form of datapath elements. Specifically adders and multipliers, and show that significant associated gains can be achieved in terms of energy, area, and delay or speed. Our algorithms can achieve these gains with adding any additional hardware overhead. The proposed CCF framework embodies a symbiotic relationship between architecture and logic-layer design through the technique of probabilistic pruning combined with the novel confined voltage scaling technique introduced in this paper, applied at the physical layer. A second drawback of the state of the art with inexact design is the lack of physical evidence established through measuring fabricated ICs that the gains and other benefits that can be achieved are valid. Again, in this paper, we have addressed this shortcoming by using CCF to fabricate a prototype chip implementing inexact data-path elements; a range of 64-bit integer adders whose outputs can be erroneous. Through physical measurements of our prototype chip wherein the inexact adders admit expected relative error magnitudes of 10% or less, we have found that cumulative gains over comparable and fully accurate chips, quantified through the area-delay-energy product, can be a multiplicative factor of 15 or more. As evidence of the utility of these results, we demonstrate that despite admitting error while achieving gains, images processed using the FFT algorithm implemented using our inexact adders are visually discernible."
2012,Reconstructing Boolean Models of Signaling.,"Abstract
Since the first emergence of protein-protein interaction networks, more than a decade ago, they have been viewed as static scaffolds of the signaling-regulatory events taking place in the cell and their analysis has been mainly confined to topological aspects. Recently, functional models of these networks have been suggested, ranging from Boolean to constraint-based ones. However, learning such models from large-scale data remains a formidable task and most modeling approaches rely on extensive human curation. Here we provide a generic approach to learning Boolean models automatically from data. We apply our approach to growth and inflammatory signaling systems in human and show how the learning phase can improve the fit of the model to experimental data, remove spurious interactions and lead to better understanding of the system at hand."
2012,Theory of Computation as an Enabling Tool for the Sciences.,"Abstract
Researchers in the theory of computation are increasingly adopting a computational worldview that is radiating out to a wide circle of scientific and technological fields, recognizing that central phenomena of these fields are often computational in nature. Over the past decade we have applied this viewpoint to physics, molecular biology and economics. Connections are also developing to evolutionary biology, machine learning, social choice, social network analysis, nanotechnology, cognitive science and astronomy. To maximize the effectiveness of this outreach to the sciences, the theory of computation must join forces with the fields of massive data analysis and combinatorial optimization."
2011,Pedigree Reconstruction Using Identity by Descent.,"Can we find the family trees, or pedigrees, that relate the haplotypes of a group of individuals? Collecting the genealogical information for how individuals are related is a very time-consuming and expensive process. Methods for automating the construction of pedigrees could stream-line this process. While constructing single-generation families is relatively easy given whole genome data, reconstructing multi-generational, possibly inbred, pedigrees is much more challenging. This article addresses the important question of reconstructing monogamous, regular pedigrees, where pedigrees are regular when individuals mate only with other individuals at the same generation. This article introduces two multi-generational pedigree reconstruction methods: one for inbreeding relationships and one for outbreeding relationships. In contrast to previous methods that focused on the independent estimation of relationship distances between every pair of typed individuals, here we present methods that aim at the reconstruction of the entire pedigree. We show that both our methods out-perform the state-of-the-art and that the outbreeding method is capable of reconstructing pedigrees at least six generations back in time with high accuracy. The two programs are available at http://cop.icsi.berkeley.edu/cop/.
"
2011,Heuristic algorithms in computational molecular biology.,"Abstract
In this paper we develop a framework for designing and validating heuristic algorithms for NP-hard problems arising in computational biology and other application areas. We introduce two areas of current research in which we are applying the framework: implicit hitting set problems and analysis of protein‚Äìprotein interaction networks, with emphasis on a specific problem in each area: multi-genome alignment and colorful connected graph detection."
2011,Understanding Science Through the Computational Lens.,"Abstract
This article explores the changing nature of the interaction between computer science and the natural and social sciences. After briefly tracing the history of scientific computation, the article presents the concept of computational lens, a metaphor for a new relationship that is emerging between the world of computation and the world of the sciences. Our main thesis is that, in many scientific fields, the processes being studied can be viewed as computational in nature, in the sense that the processes perform dynamic transformations on information represented as digital data. Viewing natural or engineered systems through the lens of their computational requirements or capabilities provides new insights and ways of thinking. A number of examples are discussed in support of this thesis. The examples are from various fields, including quantum computing, statistical physics, the World Wide Web and the Internet, mathematics, and computational molecular biology."
2011,Sorting and Selection in Posets.,"Classical problems of sorting and searching assume an underlying linear ordering of the objects being compared. In this paper, we study these problems in the context of partially ordered sets, in which some pairs of objects are incomparable. This generalization is interesting from a combinatorial perspective, and it has immediate applications in ranking scenarios where there is no underlying linear ordering, e.g., conference submissions. It also has applications in reconstructing certain types of networks, including biological networks. Our results represent significant progress over previous results from two decades ago by Faigle and Tur·n. In particular, we present the first algorithm that sorts a width-w poset of size n with query complexity $O(n(w+\log n))$ and prove that this query complexity is asymptotically optimal. We also describe a variant of Mergesort with query complexity $O(wn\log\frac{n}{w})$ and total complexity $O(w^{2}n\log\frac{n}{w})$; an algorithm with the same query complexity was given by Faigle and Tur·n, but no efficient implementation of that algorithm is known. Both our sorting algorithms can be applied with negligible overhead to the more general problem of reconstructing transitive relations. We also consider two related problems: finding the minimal elements, and its generalization to finding the bottom k ìlevels,î called the k-selection problem. We give efficient deterministic and randomized algorithms for finding the minimal elements with query complexity and total complexity $O(wn)$. We provide matching lower bounds for the query complexity up to a factor of 2 and generalize the results to the k-selection problem. Finally, we present efficient algorithms for computing a linear extension of a poset and computing the heights of all elements.
"
2011,Algorithms to Detect Multiprotein Modularity Conserved during Evolution.,"Abstract
Detecting essential multiprotein modules that change infrequently during evolution is a challenging algorithmic task that is important for understanding the structure, function, and evolution of the biological cell. In this paper, we present a linear-time algorithm, Produles, that improves on the running time of previous algorithms. We present a biologically motivated graph theoretic set of algorithm goals complementary to previous evaluation measures, demonstrate that Produles attains these goals more comprehensively than previous algorithms, and exhibit certain recurrent anomalies in the performance of previous algorithms that are not detected by previous measures."
2011,Pedigree Reconstruction Using Identity by Descent.,"Abstract
Can we find the family trees, or pedigrees, that relate the haplotypes of a group of individuals? Collecting the genealogical information for how individuals are related is a very time-consuming and expensive process. Methods for automating the construction of pedigrees could stream-line this process. While constructing single-generation families is relatively easy given whole genome data, reconstructing multi-generational, possibly inbred, pedigrees is much more challenging.
This paper addresses the important question of reconstructing monogamous, regular pedigrees, where pedigrees are regular when individuals mate only with other individuals at the same generation. This paper introduces two multi-generational pedigree reconstruction methods: one for inbreeding relationships and one for outbreeding relationships. In contrast to previous methods that focused on the independent estimation of relationship distances between every pair of typed individuals, here we present methods that aim at the reconstruction of the entire pedigree. We show that both our methods out-perform the state-of-the-art and that the outbreeding method is capable of reconstructing pedigrees at least six generations back in time with high accuracy.
The two programs are available at http://cop.icsi.berkeley.edu/ cop/"
2011,Algorithms for Implicit Hitting Set Problems.,"A hitting set for a collection of sets is a set that has a nonempty intersection with each set in the collection; the hitting set problem is to find a hitting set of minimum cardinality. Motivated by instances of the hitting set problem where the number of sets to be hit is large, we introduce the notion of implicit hitting set problems. In an implicit hitting set problem the collection of sets to be hit is typically too large to list explicitly; instead, an oracle is provided which, given a set H, either determines that H is a hitting set or returns a set that H does not hit. We show a number of examples of classic implicit hitting set problems, and give a generic algorithm for solving such problems optimally. The main contribution of this paper is to show that this framework is valuable in developing approximation algorithms. We illustrate this methodology by presenting a simple on-line algorithm for the minimum feedback vertex set problem on random graphs. In particular our algorithm gives a feedback vertex set of size nñ(1/p) log np(1 ? o(1)) with probability at least 3/4 for the random graph Gn,p (the smallest feedback vertex set is of size n ? (2/p) log np(1 + o(1))). We also consider a planted model for the feedback vertex set in directed random graphs. Here we show that a hitting set for a polynomial-sized subset of cycles is a hitting set for the planted random graph and this allows us to exactly recover the planted feedback vertex set.
"
2010,Topology-Free Querying of Protein Interaction Networks.,"In the network querying problem, one is given a protein complex or pathway of species A and a proteinñprotein interaction network of species B; the goal is to identify subnetworks of B that are similar to the query in terms of sequence, topology, or both. Existing approaches mostly depend on knowledge of the interaction topology of the query in the network of species A; however, in practice, this topology is often not known. To address this problem, we develop a topology-free querying algorithm, which we call Torque. Given a query, represented as a set of proteins, Torque seeks a matching set of proteins that are sequence-similar to the query proteins and span a connected region of the network, while allowing both insertions and deletions. The algorithm uses alternatively dynamic programming and integer linear programming for the search task. We test Torque with queries from yeast, fly, and human, where we compare it to the QNet topology-based approach, and with queries from less studied species, where only topology-free algorithms apply. Torque detects many more matches than QNet, while giving results that are highly functionally coherent.
"
2010,Haplotype Inference in Complex Pedigrees.,"Despite the desirable information contained in complex pedigree data sets, analysis methods struggle to efficiently process these data. The attractiveness of pedigree data is their power for detecting rare variants, particularly in comparison with studies of unrelated individuals. In addition, rather than assuming individuals in a study are unrelated, knowledge of their relationships can avoid spurious results due to confounding population structure effects. However, a major challenge for applying pedigree methods is difficulty in handling complex pedigrees having multiple founding lineages, inbreeding, and half-sibling relationships.
A key ingredient in association studies is imputation and inference of haplotypes from genotype data. Existing haplotype inference methods either do not efficiently scale to complex pedigrees or are of limited accuracy. In this article, we present algorithms for efficient haplotype inference and imputation in complex pedigrees. Our method, PhyloPed, leverages the perfect phylogeny model, resulting in an efficient method with high accuracy. PhyloPed effectively combines the founder haplotype information from different lineages and is immune to inaccuracies in prior information about the founders. In addition, we demonstrate that inference of missing data, using PhyloPed, can substantially improve disease association. 
"
2010,Implicit Hitting Set Problems and Multi-genome Alignment.,"Abstract
Let U be a finite set and S a family of subsets of U. Define a hitting set as a subset of U that intersects every element of S. The optimal hitting set problem is: given a positive weight for each element of U, find a hitting set of minimum total weight. This problem is equivalent to the classic weighted set cover problem.We consider the optimal hitting set problem in the case where the set system S is not explicitly given, but there is an oracle that will supply members of S satisfying certain conditions; for example, we might ask the oracle for a minimum-cardinality set in S that is disjoint from a given set Q. The problems of finding a minimum feedback arc set or minimum feedback vertex set in a digraph are examples of implicit hitting set problems. Our interest is in the number of oracle queries required to find an optimal hitting set. After presenting some generic algorithms for this problem we focus on our computational experience with an implicit hitting set problem related to multi-genome alignment in genomics. This is joint work with Erick Moreno Centeno."
2009,On the Price of Heterogeneity in Parallel Systems.,"Suppose we have a parallel or distributed system whose nodes have limited capacities, such as processing speed, bandwidth, memory, or disk space. How does the performance of the system depend on the amount of heterogeneity of its capacity distribution? We propose a general framework to quantify the worst-case effect of increasing heterogeneity in models of parallel systems. Given a cost function g(C,W) representing the systemís performance as a function of its nodesí capacities C and workload W (such as the makespan of an optimum schedule of jobs W on machines C), we say that g has price of heterogeneity ? when for any workload, cost cannot increase by more than a factor ? if node capacities become arbitrarily more heterogeneous. The price of heterogeneity also upper bounds the ìvalue of parallelismî: the maximum benefit obtained by increasing parallelism at the expense of decreasing processor speed. We give constant or logarithmic bounds on the price of heterogeneity of several well-known job scheduling and graph degree/diameter problems, indicating that in many cases, increasing heterogeneity can never be much of a disadvantage.
"
2009,Torque: topology-free querying of protein interaction networks.,"TORQUE is a tool for cross-species querying of proteinñprotein interaction networks. It aims to answer the following question: given a set of proteins constituting a known complex or a pathway in one species, can a similar complex or pathway be found in the protein network of another species? To this end, TORQUE seeks a matching set of proteins that are sequence similar to the query proteins and span a connected region of the target network, while allowing for both insertions and deletions. Unlike existing approaches, TORQUE does not require knowledge of the interconnections among the query proteins. It can handle large queries of up to 25 proteins. The TORQUE web server is freely available for use at http://www.cs.tau.ac.il/?bnet/torque.html.
"
2009,Topology-Free Querying of Protein Interaction Networks.,"Abstract
In the network querying problem, one is given a protein complex or pathway of species A and a protein‚Äìprotein interaction network of species B; the goal is to identify subnetworks of B that are similar to the query. Existing approaches mostly depend on knowledge of the interaction topology of the query in the network of species A; however, in practice, this topology is often not known. To combat this problem, we develop a topology-free querying algorithm, which we call Torque. Given a query, represented as a set of proteins, Torque seeks a matching set of proteins that are sequence-similar to the query proteins and span a connected region of the network, while allowing both insertions and deletions. The algorithm uses alternatively dynamic programming and integer linear programming for the search task. We test Torque with queries from yeast, fly, and human, where we compare it to the QNet topology-based approach, and with queries from less studied species, where only topology-free algorithms apply. Torque detects many more matches than QNet, while in both cases giving results that are highly functionally coherent."
2009,Haplotype Inference in Complex Pedigrees.,"Abstract
Despite the desirable information contained in complex pedigree datasets, analysis methods struggle to efficiently process these datasets. The attractiveness of pedigree data sets is their power for detecting rare variants, particularly in comparison with studies of unrelated individuals. In addition, rather than assuming individuals in a study are unrelated, knowledge of their relationships can avoid spurious results due to confounding population structure effects. However, a major challenge for the applicability of pedigree methods is the ability handle complex pedigrees, having multiple founding lineages, inbreeding, and half-sibling relationships.
A key ingredient in association studies is imputation and inference of haplotypes from genotype data. Existing haplotype inference methods either do not efficiently scales to complex pedigrees or their accuracy is limited. In this paper, we present algorithms for efficient haplotype inference and imputation in complex pedigrees. Our method, PhyloPed, leverages the perfect phylogeny model, resulting in an efficient method with high accuracy. In addition, PhyloPed effectively combines the founder haplotype information from different lineages and is immune to inaccuracies in prior information about the founders."
2009,Sorting and selection in posets.,"Classical problems of sorting and searching assume an underlying linear ordering of the objects being compared. In this paper, we study these problems in the context of partially ordered sets, in which some pairs of objects are incomparable. This generalization is interesting from a combinatorial perspective, and it has immediate applications in ranking scenarios where there is no underlying linear ordering, e.g., conference submissions. It also has applications in reconstructing certain types of networks, including biological networks.
Our results represent significant progress over previous results from two decades ago by Faigle and Tur√°n. In particular, we present the first algorithm that sorts a width-w poset of size n with optimal query complexity O(n(w + log n)). We also describe a variant of Mergesort with query complexity O(wn log n/w) and total complexity O(w2n log n/w); an algorithm with the same query complexity was given by Faigle and Tur√°n, but no efficient implementation of that algorithm is known. Both our sorting algorithms can be applied with negligible overhead to the more general problem of reconstructing transitive relations.
We also consider two related problems: finding the minimal elements, and its generalization to finding the bottom k ""levels"", called the k-selection problem. We give efficient deterministic and randomized algorithms for finding the minimal elements with O(wn) query and total complexity. We provide matching lower bounds for the query complexity up to a factor of 2 and generalize the results to the k-selection problem. Finally, we present efficient algorithms for computing a linear extension of a poset and computing the heights of all elements."
2008,George Dantzig's impact on the theory of computation.,"Abstract
George Dantzig created the simplex algorithm for linear programming, perhaps the most important algorithm developed in the 20th century. This paper traces a single historical thread: Dantzig‚Äôs work on linear programming and its application and extension to combinatorial optimization, and the investigations it has stimulated about the performance of the simplex algorithm and the intrinsic complexity of linear programming and combinatorial optimization."
2008,Probabilistic Analysis of Linear Programming Decoding.,"Abstract:
We initiate the probabilistic analysis of linear programming (LP) decoding of low-density parity-check (LDPC) codes. Specifically, we show that for a random LDPC code ensemble, the linear programming decoder of Feldman succeeds in correcting a constant fraction of errors with high probability. The fraction of correctable errors guaranteed by our analysis surpasses previous nonasymptotic results for LDPC codes, and in particular, exceeds the best previous finite-length result on LP decoding by a factor greater than ten. This improvement stems in part from our analysis of probabilistic bit-flipping channels, as opposed to adversarial channels. At the core of our analysis is a novel combinatorial characterization of LP decoding success, based on the notion of a flow on the Tanner graph of the code. An interesting by-product of our analysis is to establish the existence of ldquoprobabilistic expansionrdquo in random bipartite graphs, in which one requires only that almost every (as opposed to every) set of a certain size expands, for sets much larger than in the classical worst case setting."
2008,Computer Science as a Lens on the Sciences.,"Abstract:
Summary form only given. This article trace the growing influence of fundamental ideas from computer science on the nature of research in a number of scientific fields. There is a growing awareness that information processing lies at the heart of the processes studied in fields as diverse as quantum mechanics, statistical physics, nanotechnology, neuroscience, linguistics, economics and sociology. Increasingly, mathematical models in these fields are expressed in algorithmic languages and describe algorithmic processes. The author describe connections between quantum computing and the foundations of quantum mechanics, and between statistical mechanics and phase transitions in computation. The author indicates how the growth of the Web has created new phenomena to be investigated by sociologists and economists, spurring new developments in computational game theory and the study of social networks. The author then focus on computational molecular biology, where the view of living cells as complex information processing systems has become the dominant paradigm."
2008,Detecting Disease-Specific Dysregulated Pathways Via Analysis of Clinical Expression Profiles.,"Abstract
We present a method for identifying connected gene subnetworks significantly enriched for genes that are dysregulated in specimens of a disease. These subnetworks provide a signature of the disease potentially useful for diagnosis, pinpoint possible pathways affected by the disease, and suggest targets for drug intervention. Our method uses microarray gene expression profiles derived in clinical case-control studies to identify genes significantly dysregulated in disease specimens, combined with protein interaction data to identify connected sets of genes. Our core algorithm searches for minimal connected subnetworks in which the number of dysregulated genes in each diseased sample exceeds a given threshold. We have applied the method in a study of Huntington‚Äôs disease caudate nucleus expression profiles and in a meta-analysis of breast cancer studies. In both cases the results were statistically significant and appeared to home in on compact pathways enriched with hallmarks of the diseases."
2008,Linked decompositions of networks and the power of choice in Polya urns.,"A linked decomposition of a graph with n nodes is a set of subgraphs covering the n nodes such that all pairs of subgraphs intersect; we seek linked decompositions such that all subgraphs have about ‚àön vertices, logarithmic diameter, and each vertex of the graph belongs to either one or two subgraphs. A linked decomposition enables many control and management functions to be implemented locally, such as resource sharing, maintenance of distributed directory structures, deadlock-free routing, failure recovery and load balancing, without requiring any node to maintain information about the state of the network outside the subgraphs to which it belongs. Linked decompositions also enable efficient routing, schemes with small routing tables, which we describe in Section 5. Our main contribution is to show that ""Internet-like graphs"" (e.g. the preferential attachment model proposed by Barabasi et al. [10] and other similar models) have linked decompositions with the parameters described above with high probability; moreover, our experiments show that the Internet topology itself can be so decomposed. Our proof proceeds by analyzing a novel process, which we call Polya urns with the power of choice, which may be of great independent interest. In this new process, we start with n nonempty bins containing O(n) balls total, and each arriving ball is placed in the least loaded of m bins, drawn independently at random with probability proportional to load. Our analysis shows that in our new process, with high probability the bin loads become roughly balanced some time before O(n2+Œµ) further balls have arrived and stay roughly balanced, regardless of how the initial O(n) balls were distributed, where Œµ > 0 can be arbitrarily small, provided m is large enough."
2007,HAPLOPOOL: improving haplotype frequency estimation through DNA pools and phylogenetic modeling.,"Motivation: The search for genetic variants that are linked to complex diseases such as cancer, Parkinson's;, or Alzheimer's; disease, may lead to better treatments. Since haplotypes can serve as proxies for hidden variants, one method of finding the linked variants is to look for case-control associations between the haplotypes and disease. Finding these associations requires a high-quality estimation of the haplotype frequencies in the population. To this end, we present, HAPLOPOOL, a method of estimating haplotype frequencies from blocks of consecutive SNPs.

Results: HAPLOPOOL leverages the efficiency of DNA pools and estimates the population haplotype frequencies from pools of disjoint sets, each containing two or three unrelated individuals. We study the trade-off between pooling efficiency and accuracy of haplotype frequency estimates. For a fixed genotyping budget, HAPLOPOOL performs favorably on pools of two individuals as compared with a state-of-the-art non-pooled phasing method, PHASE. Of independent interest, HAPLOPOOL can be used to phase non-pooled genotype data with an accuracy approaching that of PHASE.

We compared our algorithm to three programs that estimate haplotype frequencies from pooled data. HAPLOPOOL is an order of magnitude more efficient (at least six times faster), and considerably more accurate than previous methods. In contrast to previous methods, HAPLOPOOL performs well with missing data, genotyping errors and long haplotype blocks (of between 5 and 25 SNPs)."
2007,Comparing Protein Interaction Networks via a Graph Match-and-Split Algorithm.,"We present a method that compares the protein interaction networks of two species to detect functionally similar (conserved) protein modules between them. The method is based on an algorithm we developed to identify matching subgraphs between two graphs. Unlike previous network comparison methods, our algorithm has provable guarantees on correctness and efficiency. Our algorithm framework also admits quite general criteria that define when two subgraphs match and constitute a conserved module. We apply our method to pairwise comparisons of the yeast protein network with the human, fruit fly and nematode worm protein networks, using a lenient criterion based on connectedness and matching edges, coupled with a clustering heuristic. In evaluations of the detected conserved modules against reference yeast protein complexes, our method performs competitively with and sometimes better than two previous network comparison methods. Further, under some conditions (proper homolog and species selection), our method performs better than a popular single-species clustering method. Beyond these evaluations, we discuss the biology of a couple of conserved modules detected by our method. We demonstrate the utility of network comparison for transferring annotations from yeast proteins to human ones, and validate the predicted annotations. Supplemental text is available at www.cs.berkeley.edu/?nmani/M-and-S/supplement.pdf.
"
2007,Computer Science as a Lens on the Sciences: The Example of Computational Molecular Biology.,"Abstract:
With the wide application of green fluorescent protein (GFP) in the study of live cells, there is a surging need for the computer-aided analysis on the huge amount of im- age sequence data acquired by the advanced microscopy devices. One of such tasks is the motility analysis of the multiple subcellular structures. In this paper, an algorithm using sequential Monte Carlo (SMC) method for multiple interacting object tracking is proposed. First, marker resid- ual image is applied to detect individual subcellular struc- ture automatically, and to represent all the objects together using the joint state. Then the interaction between ob- jects in the 2D plane is modeled by augmenting an extra dimension and evaluating the overlapping relationship in the 3D space. Finally, the distribution of the dimension varying joint state is sampled efficiently by Reversible jump Markov chain Monte Carlo (RJMCMC) algorithm with a novel height swap move. The experimental results show that our method is promising."
2007,Streaming Algorithms for Selection and Approximate Sorting.,"Abstract
Companies such as Google, Yahoo and Microsoft maintain extremely large data repositories within which searches are frequently conducted. In an article entitled ‚ÄúData-Intensive Supercomputing: The case for DISC‚Äù Randal Bryant describes such data repositories and suggests an agenda for appying them more broadly to massive data set problems of importance to the scientific community and society in general."
2007,Balancing traffic load in wireless networks with curveball routing.,"We address the problem of balancing the traffic load in multi-hop wireless networks. We consider a point-to-point communicating network with a uniform distribution of source-sink pairs. When routing along shortest paths, the nodes that are centrally located forward a disproportionate amount of traffic. This translates into increased congestion and energy consumption. However, the maximum load can be decreased if the packets follow curved paths. We show that the optimum such routing scheme can be expressed in terms of geometric optics and computed by linear programming. We then propose a practical solution, which we call Curveball Routing which achieves results not much worse than the optimum.
We evaluate our solution at three levels of fidelity: a Java high-level simulator, the ns2 simulator, and the Intel Mirage Sensor Network Testbed. Simulation results using the high-level simulator show that our solution successfully avoids the crowded center of the network, and reduces the maximum load by up to 40%. At the same time, the increase of the expected path length is minimal, i.e., only 8% on average. Simulation results using the ns2 simulator show that our solution can increase throughput on moderately loaded networks by up to 15%, while testbed results show a reduction in peak energy usage by up to 25%. Our prototype suggests that our solution is easily deployable."
2007,Probabilistic analysis of linear programming decoding.,"We initiate the probabilistic analysis of linear programming (LP) decoding of low-density parity-check (LDPC) codes. Specifically, we show that for a random LDPC code ensemble, the linear programming decoder of Feld-man et al. succeeds in correcting a constant fraction of errors with high probability. The fraction of correctable errors guaranteed by our analysis surpasses all prior non-asymptotic results for LDPC codes, and in particular exceeds the best previous finite-length result on LP decoding by a factor greater than ten. This improvement stems in part from our analysis of probabilistic bit-flipping channels, as opposed to adversarial channels. At the core of our analysis is a novel combinatorial characterization of LP decoding success, based on the notion of a generalized matching. An interesting by-product of our analysis is to establish the existence of ""almost expansion"" in random bipartite graphs, in which one requires only that almost every (as opposed to every) set of a certain size expands, with expansion coefficients much larger than the classical case."
2007,Noisy binary search and its applications.,"We study a noisy version of the classic binary search problem of inserting an element into its proper place within an ordered sequence by comparing it with elements of the sequence. In the noisy version we can not compare elements directly. Instead we are given a coin corresponding to each element of the sequence, such that as one goes through the ordered sequence the probability of observing heads when tossing the corresponding coin increases. We design online algorithms which adaptively choose a sequence of experiments, each consisting of tossing a single coin, with the goal of identifying the highest-numbered coin in the ordered sequence whose heads probability is less than some specified target value. Possible applications of such algorithms include investment planning, sponsored search advertising, admission control in queueing networks, college admissions, and admitting new members into an organization ranked by ability, such as a tennis ladder."
2007,Computer Science as a Lens on the Sciences: .,"Abstract:
This talk will trace the growing influence of fundamental ideas from computer science on the nature of research in a number of scientific fields. There is a growing awareness that information processing lies at the heart of the processes studied in fields as diverse as quantum mechanics, statistical physics, nanotechnology, neuroscience, linguistics, economics and sociology. Increasingly, mathematical models in these fields are expressed in algorithmic languages and describe algorithmic processes. The speaker will briefly describe connections between quantum computing and the foundations of quantum mechanics, and between statistical mechanics and phase transitions in computation. He will indicate how the growth of the Web has created new phenomena to be investigated by sociologists and economists. He will then focus in greater detail on computational molecular biology, where the view of living cells as complex information processing systems has become the dominant paradigm, and will discuss specific algorithmic problems arising in the sequencing of genomes, the comparative analysis of the resulting genomic sequences,the modeling of networks of interacting proteins, and the associations between genetic variation and disease."
2006,Efficient Algorithms for Detecting Signaling Pathways in Protein Interaction Networks.,"The interpretation of large-scale protein network data depends on our ability to identify significant substructures in the data, a computationally intensive task. Here we adapt and extend efficient techniques for finding paths and trees in graphs to the problem of identifying pathways in protein interaction networks. We present linear-time algorithms for finding paths and trees in networks under several biologically motivated constraints. We apply our methodology to search for protein pathways in the yeast proteinñprotein interaction network. We demonstrate that our algorithm is capable of reconstructing known signaling pathways and identifying functionally enriched paths and trees in an unsupervised manner. The algorithm is very efficient, computing optimal paths of length 8 within minutes and paths of length 10 in about three hours.
"
2006,Load balancing in dynamic structured peer-to-peer systems.,"Abstract
Most P2P systems that provide a DHT abstraction distribute objects randomly among ‚Äúpeer nodes‚Äù in a way that results in some nodes having Œò(log N) times as many objects as the average node. Further imbalance may result due to nonuniform distribution of objects in the identifier space and a high degree of heterogeneity in object loads and node capacities. Additionally, a node's load may vary greatly over time since the system can experience continuous insertions and deletions of objects, skewed object arrival patterns, and continuous arrival and departure of nodes.
In this paper, we propose an algorithm for load balancing in such heterogeneous, dynamic P2P systems. Our simulation results show that in the face of rapid arrivals and departures of objects of widely varying load, our algorithm improves load balance by more than an order of magnitude for system utilizations as high as 80% while incurring an overhead of only about 6%. We also show that our distributed algorithm performs only negligibly worse than a similar centralized algorithm, and that node heterogeneity helps, not hurts, the scalability of our algorithm. Although many of these results are dependent on the workload, we believe the efficiency and performance improvement demonstrated over the case of no load balancing shows that our technique holds promise for deployed systems."
2006,Reconstructing Chain Functions in Genetic Networks.,"The following problems arise in the analysis of biological networks: We have a boolean function of n variables, each of which has some default value. An experiment fixes the values of any subset of the variables, the remaining variables assume their default values, and the function value is the result of the experiment. How many experiments are needed to determine (reconstruct) the function? How many experiments that involve fixing at most q values are needed? What are the answers to these questions when an unknown subset of the variables are actually involved in the function? In the biological context, the variables are genes and the values are gene expression intensities. An experiment measures the gene levels under conditions that perturb the values of a subset of the genes. The goal is to reconstruct the particular logic (regulation function) by which a subset of the genes together regulate one target gene, using few experiments that involve minor perturbations. We study these questions under the assumption that all functions belong to a biologically motivated set of so?called chain functions. We give optimal reconstruction schemes for several scenarios and show their application in reconstructing the regulation of galactose utilization in yeast.

"
2006,Fair Bandwidth Allocation Without Per-Flow State.,"Abstract
A fundamental goal of Internet congestion control is to allocate limited bandwidth fairly to competing flows. Such flow control involves an interplay between the behavior of routers and the behavior of end hosts. Routers must decide which packets to drop when their output links become congested. End hosts must decide how to moderate their packet transmissions in response to feedback in the form of acknowledgements of packet delivery (acks). Typically this is done according to the TCP protocol, in which a host maintains a window (the number of packets that have been sent but not yet acknowledged) that is increased when an ack is received and decreased when a drop is detected.
Often the selection of packets to be dropped at a router depends on the order of their arrivals at the router but not on the flows to which the packets belong. An exception occurs when packets are stratified according to their quality of service guarantee; in this case packets at higher strata are given priority, but within a stratum the packets from different flows receive the same treatment. A number of methods have been proposed to ensure fairness by selectively dropping packets from flows that are receiving more than their fair share of bandwidth. The most effective known algorithms for detecting and selectively dropping high-rate flows at a router are based on random hashing or random sampling of packets and give only probabilistic guarantees. The known deterministic algorithms either require excessive storage, require packets to carry accurate estimates of the rates of their flows, assume some special properties of the stream of arriving packets, or fail to guarantee fairness. In a simplified theoretical setting we show that the detection and selective dropping of high-rate flows can be accomplished deterministically without any of these defects. This result belies the conventional wisdom that per-flow state is required to guarantee fairness.
Given an arriving stream of packets, each labeled with the name of its flow, our algorithm drops packets selectively upon arrival so as to guarantee that, in every consecutive subsequence of the stream of surviving packets, no flow has significantly more than its fair share of the packets. The main results of the paper are tight bounds on the worst-case storage requirement of this algorithm. The bounds demonstrate that the storage and computation required to guarantee fairness are easily within the capabilites of conventional routers.
It is important to acknowedge the limitations of this work. We have formulated the achievement of fairness at a router in terms of local information on the stream of arriving packets at that router. The implications of such a locally optimal policy on the global stability of the Internet would require analyzing the Internet as a complex dynamical system involving interactions among routers and end hosts, of which some will be TCP-compliant and some will not. In work not reported here we have made an initial simulation study of this complex process, but such a study is outside the scope of the present paper."
2006,Optimal Flow Distribution Among Multiple Channels with Unknown Capacities .,"Abstract
Consider a simple network flow problem in which a flow of value D must be split among n channels directed from a source to a sink. The initially unknown channel capacities can be probed by attempting to send a flow of at most D units through the network. If the flow is not feasible, we are told on which channels the capacity was exceeded (binary feedback) and possibly also how many units of flow were successfully sent on these channels (throughput feedback). For throughput feedback we present optimal protocols for minimizing the number of rounds needed to find a feasible flow and for minimizing the total amount of wasted flow. For binary feedback we present an asymptotically optimal protocol."
2006,On the price of heterogeneity in parallel systems.,"Suppose we have a parallel or distributed system whose nodes have limited capacities, such as processing speed, bandwidth, memory, or disk space. How does the performance of the system depend on the amount of heterogeneity of its capacity distribution? We propose a general framework to quantify the worst-case effect of increasing heterogeneity in models of parallel systems. Given a cost function g(C,W) representing the system's performance as a function of its nodes' capacities C and workload W (such as the completion time of an optimum schedule of jobs W on machines C), we say that g has price of heterogeneity Œ± when for any workload, cost cannot increase by more than a factor Œ± if node capacities become arbitrarily more heterogeneous. We give constant bounds on the price of heterogeneity of several well-known job scheduling and graph degree/diameter problems, indicating that increasing heterogeneity can never be much of a disadvantage. On the other hand, with the introduction of timing constraints such as release times or precedence constraints on the jobs, the dependence on node capacities becomes more complex, so that increasing heterogeneity may be quite detrimental."
2005,Optimal flow distribution among multiple channels with unknown capacities.,"Consider a simple network flow problem in which a flow of value D must be split among n channels directed from a source to a sink. The initially unknown channel capacities can be probed by attempting to send a flow of at most D units through the network. If the flow is not feasible, we are told on which channels the capacity was exceeded (binary feedback) and possibly also how many units of flow were successfully sent on these channels (throughput feedback). For throughput feedback we present optimal protocols for minimizing the number of rounds needed to find a feasible flow and for minimizing the total amount of wasted flow. For binary feedback we present an asymptotically optimal protocol.
"
2005,Identification of Protein Complexes by Comparative Analysis of Yeast and Bacterial Protein Interaction Data.,"Mounting evidence shows that many protein complexes are conserved in evolution. Here we use conservation to find complexes that are common to the yeast S. cerevisiae and the bacteria H. pylori. Our analysis combines protein interaction data that are available for each of the two species and orthology information based on protein sequence comparison. We develop a detailed probabilistic model for protein complexes in a single species and a model for the conservation of complexes between two species. Using these models, one can recast the question of finding conserved complexes as a problem of searching for heavy subgraphs in an edge- and node-weighted graph, whose nodes are orthologous protein pairs. We tested this approach on the data currently available for yeast and bacteria and detected 11 significantly conserved complexes. Several of these complexes match very well with prior experimental knowledge on complexes in yeast only and serve for validation of our methodology. The complexes suggest new functions for a variety of uncharacterized proteins. By identifying a conserved complex whose yeast proteins function predominantly in the nuclear pore complex, we propose that the corresponding bacterial proteins function as a coherent cellular membrane transport system. We also compare our results to two alternative methods for detecting complexes and demonstrate that our methodology obtains a much higher specificity.
"
2005,The minimum-entropy set cover problem.,"Abstract
We consider the minimum entropy principle for learning data generated by a random source and observed with random noise.
In our setting we have a sequence of observations of objects drawn uniformly at random from a population. Each object in the population belongs to one class. We perform an observation for each object which determines that it belongs to one of a given set of classes. Given these observations, we are interested in assigning the most likely class to each of the objects.
This scenario is a very natural one that appears in many real life situations. We show that under reasonable assumptions finding the most likely assignment is equivalent to the following variant of the set cover problem. Given a universe U and a collection
of subsets of U, we wish to find an assignment
such that
and the entropy of the distribution defined by the values
is minimized.
We show that this problem is NP-hard and that the greedy algorithm for set cover s with an additive constant error with respect to the optimal cover. This sheds a new light on the behavior of the greedy set cover algorithm. We further enhance the greedy algorithm and show that the problem admits a polynomial time approximation scheme (PTAS).
Finally, we demonstrate how this model and the greedy algorithm can be useful in real life scenarios, and in particular, in problems arising naturally in computational biology."
2005,Verification decoding of raptor codes.,"Abstract:
In this paper we extend the double verification algorithm of Luby and Mitzenmacher to the class of Raptor codes, analyze it, and design Raptor codes that perform very well with respect to this algorithm"
2005,Efficient Algorithms for Detecting Signaling Pathways in Protein Interaction Networks.,"Abstract
The interpretation of large-scale protein network data depends on our ability to identify significant sub-structures in the data, a computationally intensive task. Here we adapt and extend efficient techniques for finding paths in graphs to the problem of identifying pathways in protein interaction networks. We present linear-time algorithms for finding paths in networks under several biologically-motivated constraints. We apply our methodology to search for protein pathways in the yeast protein-protein interaction network. We demonstrate that our algorithm is capable of reconstructing known signaling pathways and identifying functionally enriched paths in an unsupervised manner. The algorithm is very efficient, computing optimal paths of length 8 within minutes and paths of length 10 in less than two hours."
2004,Logos: a Modular Bayesian Model for de Novo Motif Detection.,"The complexity of the global organization and internal structure of motifs in higher eukaryotic organisms raises significant challenges for motif detection techniques. To achieve successful de novo motif detection, it is necessary to model the complex dependencies within and among motifs and to incorporate biological prior knowledge. In this paper, we present LOGOS, an integrated LOcal and GlObal motif Sequence model for biopolymer sequences, which provides a principled framework for developing, modularizing, extending and computing expressive motif models for complex biopolymer sequence analysis. LOGOS consists of two interacting submodels: HMDM, a local alignment model capturing biological prior knowledge and positional dependency within the motif local structure; and HMM, a global motif distribution model modeling frequencies and dependencies of motif occurrences. Model parameters can be fit using training motifs within an empirical Bayesian framework. A variational EM algorithm is developed for de novo motif detection. LOGOS improves over existing models that ignore biological priors and dependencies in motif structures and motif occurrences, and demonstrates superior performance on both semi-realistic test data and cis-regulatory sequences from yeast and Drosophila genomes with regard to sensitivity, specificity, flexibility and extensibility.
"
2004,Towards Optimally Multiplexed Applications of Universal Arrays.,"We study a design and optimization problem that occurs, for example, when single nucleotide polymorphisms (SNPs) are to be genotyped using a universal DNA tag array. The problem of optimizing the universal array to avoid disruptive cross-hybridization between universal components of the system was addressed in previous work. Cross-hybridization can, however, also occur assay specifically, due to unwanted complementarity involving assay-specific components. Here we examine the problem of identifying the most economic experimental configuration of the assay-specific components that avoids cross-hybridization. Our formalization translates this problem into the problem of covering the vertices of one side of a bipartite graph by a minimum number of balanced subgraphs of maximum degree 1. We show that the general problem is NP-complete. However, in the real biological setting, the vertices that need to be covered have degrees bounded by d. We exploit this restriction and develop an O(d)-approximation algorithm for the problem. We also give an O(d)-approximation for a variant of the problem in which the covering subgraphs are required to be vertex disjoint. In addition, we propose a stochastic model for the input data and use it to prove a lower bound on the cover size. We complement our theoretical analysis by implementing two heuristic approaches and testing their performance on synthetic data as well as on simulated SNP data.
"
2004,MotifPrototyper: A Bayesian profile model for motif families.,"In this article, we address the problem of modeling generic features of structurally but not textually related DNA motifs, that is, motifs whose consensus sequences are entirely different but nevertheless share ìmetasequence featuresî reflecting similarities in the DNA-binding domains of their associated protein recognizers. We present MotifPrototyper, a profile Bayesian model that can capture structural properties typical of particular families of motifs. Each family corresponds to transcription regulatory proteins with similar types of structural signatures in their DNA-binding domains. We show how to train MotifPrototypers from biologically identified motifs categorized according to the TRANSFAC categorization of transcription factors and present empirical results of motif classification, motif parameter estimation, and de novo motif detection by using the learned profile models.
"
2004,The Minimum-Entropy Set Cover Problem.,"Abstract
We consider the minimum entropy principle for learning data generated by a random source and observed with random noise.
In our setting we have a sequence of observations of objects drawn uniformly at random from a population. Each object in the population belongs to one class. We perform an observation for each object which determines that it belongs to one of a given set of classes. Given these observations, we are interested in assigning the most likely class to each of the objects.
This scenario is a very natural one that appears in many real life situations. We show that under reasonable assumptions finding the most likely assignment is equivalent to the following variant of the set cover problem. Given a universe U and a collection
S=(
S
1
,‚Ä¶,
S
m
)
S
of subsets of U, we wish to find an assignment
f:U‚ÜíS
f
such that u ‚àà f(u) and the entropy of the distribution defined by the values |f ‚àí‚Äâ‚àí‚Äâ1(S i )| is minimized.
We show that this problem is NP-hard and that the greedy algorithm for set cover finds a cover with an additive constant error with respect to the optimal cover. This sheds a new light on the behavior of the greedy set cover algorithm. We further enhance the greedy algorithm and show that the problem admits a polynomial time approximation scheme (PTAS).
Finally, we demonstrate how this model and the greedy algorithm can be useful in real life scenarios, and in particular, in problems arising naturally in computational biology."
2004,Load Balancing in Dynamic Structured P2P Systems.,"Abstract:
Most P2P systems that provide a DHT abstraction distribute objects randomly among ""peer nodes"" in a way that results in some nodes having /spl theta/(log N) times as many objects as the average node. Further imbalance may result due to non-uniform distribution of objects in the identifier space and a high degree of heterogeneity in object loads and node capacities. Additionally, a node's load may vary greatly over time since the system can be expected to experience continuous insertions and deletions of objects, skewed object arrival patterns, and continuous arrival and departure of nodes. We propose an algorithm for load balancing in such heterogeneous, dynamic P2P systems. Our simulation results show that in the face of rapid arrivals and departures of objects of widely varying load, our algorithm achieves load balancing for system utilizations as high as 90% while moving only about 8% of the load that arrives into the system. Similarly, in a dynamic system where nodes arrive and depart, our algorithm moves less than 60% of the load the underlying DHT moves due to node arrivals and departures. Finally, we show that our distributed algorithm performs only negligibly worse than a similar centralized algorithm, and that node heterogeneity helps, not hurts, the scalability of our algorithm."
2004,Finite length analysis of LT codes.,"Abstract:
This paper provides an efficient method for analyzing the error probability of the belief propagation (BP) decoder applied to LT Codes. Each output symbol is generated independently by sampling from a distribution and adding the input symbols corresponding to the support of the sampled vector."
2004,Global Synchronization in Sensornets.,"Abstract
Time synchronization is necessary in many distributed systems, but achieving synchronization in sensornets, which combine stringent precision requirements with severe resource constraints, is particularly challenging. This challenge has been met by the recent Reference-Broadcast Synchronization (RBS) proposal, which provides on-demand pairwise synchronization with low overhead and high precision. In this paper we introduce a model of the basic RBS synchronization paradigm. Within the context of this model we characterize the optimally precise clock synchronization algorithm and establish its global consistency. In the course of this analysis we point out unexpected connections between optimal clock synchronization, random walks, and resistive networks, and present a polynomial-time approximation scheme for the problem of calculating the effective resistance in a network based on min-cost flow. We also sketch a polynomial-time algorithm for finding a schedule of data acquisition giving the optimal trade-off between energy consumption and precision of clock synchronization. We also discuss synchronization in the presence of clock skews. In ongoing work we are adapting our synchronization algorithm for execution in a network of seismic sensors that requires global clock consistency."
2004,Reconstructing Chain Functions in Genetic Networks.,"Deciphering the mechanisms that control gene expression in the cell is a fundamental question in molecular biology. This task is complicated by the large number of possible regulation relations in the cell, and the relatively small amount of available experimental data. Recently, a new class of regulation functions called chain functions was suggested. Many signal transduction pathways can be accurately modeled by chain functions, and the restriction to chain functions greatly reduces the vast search space of regulation relations. In this paper we study the computational problem of reconstructing a chain function using a minimum number of experiments, in each of which only few genes are perturbed. We give optimal reconstruction schemes for several scenarios and show their application in reconstructing the regulation of galactose utilization in yeast."
2004,Perfect phylogeny and haplotype assignment.,"This paper is concerned with the reconstruction of perfect phylogenies from binary character data with missing values, and related problems of inferring complete haplotypes from haplotypes or genotypes with missing data. In cases where the problems considered are NP-hard we assume a rich data hypothesis under which they become tractable. Natural probabilistic models are introduced for the generation of character vectors, haplotypes or genotypes with missing data, and it is shown that these models support the rich data hypothesis. The principal results include:
A near-linear time algorithm for inferring a perfect phylogeny from binary character data (or haplotype data) with missing values, under the rich data hypothesis;
A quadratic-time algorithm for inferring a perfect phylogeny from genotype data with missing values with high probability, under certain distributional assumptions;
Demonstration that the problems of maximum-likelihood inference of complete haplotypes from partial haplotypes or partial genotypes can be cast as minimum-entropy disjoint set cover problems;
In the case where the haplotypes come from a perfect phylogeny, a representation of the set cover problem as minimum-entropy covering of subtrees of a tree by nodes;
An exact algorithm for minimum-entropy subtree covering, and demonstration that it runs in polynomial time when the subtrees have small diameter;
Demonstration that a simple greedy approximation algorithm solves the minimum-entropy subtree covering problem with relative error tending to zero when the number of partial haplotypes per complete haplotype is large;
An asymptotically consistent method of estimating the frequencies of the complete haplotypes in a perfect phylogeny, under an iid model for the distribution of missing data;
Computational results on real data demonstrating the effectiveness of a the greedy algorithm for inferring haplotypes from genotypes with missing data, even in the absence of a perfect phylogeny.
."
2004,Algorithms for inferring cis-regulatory structures and protein interaction networks.,"A major focus of functional genomics today is the discovery of the interactions between genes and proteins that regulate the transcription of genes and the responses of cells to external signals. The speaker will describe his recent efforts with several coworkers to solve pieces of this puzzle. The work divides into several parts:
A new approach to the recognition of transcription-factor binding sites, based on the principle that transcription factors divide naturally into families such as the leucine zippers and the zinc fingers, and that the binding site motifs for transcription factors within the same family have common features. These features may be obscure at the sequence level, but can be characterized at a higher level of description. By discovering and modeling such meta-sequence features one can improve the sensitivity and specificity with which binding sites can be determined for transcription factors within a family. [5], [6]
An algorithm and an associated web-based tool for finding recurrent cis-regulatory modules in the promoter regions of human genes. Each such module consists of a set of transcription factors that often bind to the same promoter regions and collectively enhance or inhibit the transcription of the corresponding genes. [4]
An algorithm for minimizing the number of gene perturbation experiments required to reconstruct signal transduction pathways whose regulatory structures can be described within the mathematical framework of chain functions. [1]
Algorithms for discovering protein complexes and regulatory pathways that are conserved in evolution, using protein sequence data and protein-protein interaction data for two or more organisms. [2], [3]."
2004,Identification of protein complexes by comparative analysis of yeast and bacterial protein interaction data.,"Mounting evidence shows that many protein complexes are conserved in evolution. Here we use conservation to find complexes that are common to yeast S. Cerevisiae and bacteria H. pylori. Our analysis combines protein interaction data, that are available for each of the two species, and orthology information based on protein sequence comparison. We develop a detailed probabilistic model for protein complexes in a single species, and a model for the conservation of complexes between two species. Using these models, one can recast the question of finding conserved complexes as a problem of searching for heavy subgraphs in an edge- and node-weighted graph, whose nodes are orthologous protein pairs.We tested this approach on the data currently available for yeast and bacteria and detected 11 significantly conserved complexes. Several of these complexes match very well with prior experimental knowledge on complexes in yeast only, and serve for validation of our methodology. The complexes suggest new functions for a variety of uncharacterized proteins. By identifying a conserved complex whose yeast proteins function predominantly in the nuclear pore complex, we propose that the corresponding bacterial proteins function as a coherent cellular membrane transport system. We also compare our results to two alternative methods for detecting complexes, and demonstrate that our methodology obtains a much higher specificity."
2004,Gapped Local Similarity Search with Provable Guarantees.,"Abstract
We present a program qhash, based on q-gram filtration and high-dimensional search, to find gapped local similarities between two sequences. Our approach differs from past q-gram-based approaches in two main aspects. Our filtration step uses algorithms for a sparse all-pairs problem, while past studies use suffix-tree-like structures and counters. Our program works in sequence-sequence mode, while most past ones (except QUASAR) work in pattern-database mode.
We leverage existing research in high-dimensional proximity search to discuss sparse all-pairs algorithms, and show them to be subquadratic under certain reasonable input assumptions. Our qhash program has provable sensitivity (even on worst-case inputs) and average-case performance guarantees. It is significantly faster than a fully sensitive dynamic-programming-based program for strong similarity search on longsequences."
2004,The Role of Experimental Algorithms in Genomics.,"Abstract
Biology has become a computational science. In their efforts to understand the functioning of cells at a molecular level, biologists make use of a growing array of databases that codify knowledge about genomes, the genes within them, the structure and function of the proteins encoded by the genes, and the interactions among genes, RNA molecules, proteins, molecular machines and other chemical components of the cell. Biologists have access to high-throughput measurement technologies such as DNA microarrays, which can measure the expression levels of tens of thousands of genes in a single experiment."
2003,Discovering Local Structure in Gene Expression Data: The Order-Preserving Submatrix Problem.,"This paper concerns the discovery of patterns in gene expression matrices, in which each element gives the expression level of a given gene in a given experiment. Most existing methods for pattern discovery in such matrices are based on clustering genes by comparing their expression levels in all experiments, or clustering experiments by comparing their expression levels for all genes. Our work goes beyond such global approaches by looking for local patterns that manifest themselves when we focus simultaneously on a subset G of the genes and a subset T of the experiments. Specifically, we look for order-preserving submatrices (OPSMs), in which the expression levels of all genes induce the same linear ordering of the experiments (we show that the OPSM search problem is NP-hard in the worst case). Such a pattern might arise, for example, if the experiments in T represent distinct stages in the progress of a disease or in a cellular process and the expression levels of all genes in G vary across the stages in the same way. We define a probabilistic model in which an OPSM is hidden within an otherwise random matrix. Guided by this model, we develop an efficient algorithm for finding the hidden OPSM in the random matrix. In data generated according to the model, the algorithm recovers the hidden OPSM with a very high success rate. Application of the methods to breast cancer data seem to reveal significant local patterns.
"
2003,The Restriction Scaffold Problem.,"Most shotgun sequencing projects undergo a long and costly phase of finishing, in which a partial assembly forms several contigs whose order, orientation, and relative distance is unknown. We propose here a new technique that supplements the shotgun assembly data by experimentally simple and commonly used complete restriction digests of the target. By computationally combining information from the contig sequences and the fragment sizes measured for several different enzymes, we seek to form a ""scaffold"" on which the contigs will be placed in their correct orientation, order, and distance. We give a heuristic search algorithm for solving the problem and report on promising preliminary simulation results. The key to the success of the search scheme is the very rapid solution of two time-critical subproblems that are solved to optimality in linear time. Our simulations indicate that with noise levels of some 3% relative error in measuring fragment sizes, using six enzymes, most datasets of 13 contigs spanning 300kb can be correctly ordered, and the remaining ones have most of their pairs of neighboring contigs correct. Hence, the technique has a potential to provide real help to finishing. Even without closing all gaps, the ability to order and orient the contigs correctly makes the partial assembly both more accessible and more useful for biologists.
"
2003,Coalescing times for IID random variables with applications to population biology.,"We consider a coalescing particle model where particles move in discrete time. At each time period, each remaining ball is independently put in one of n bins according to a probability distribution p = (p1, Ö, pn), and all balls put into the same bin merge into a single ball. Starting with k balls, we are interested in the properties of E[N(p, k)], the expected time until all balls merge into one. We derive both upper and lower bounds for E[N(p, k)], some asymptotic results, and show that P{N(p, k) > t}, and thus E[N(p, k)], is a Schur concave function of p. Applications to population biology are noted. © 2003 Wiley Periodicals, Inc. Random Struct. Alg., 23: 155ñ166, 2003
"
2003,A simple algorithm for finding frequent elements in streams and bags.,"We present a simple, exact algorithm for identifying in a multiset the items with frequency more than a threshold Œ∏. The algorithm requires two passes, linear time, and space 1/Œ∏. The first pass is an on-line algorithm, generalizing a well-known algorithm for finding a majority element, for identifying a set of at most 1/Œ∏ items that includes, possibly among others, all items with frequency greater than Œ∏."
2003,The Role of Algorithmic Research in Computational Genomics.,"Abstract:
In the early 1990s, after more than three decades of studying algorithms within the framework of theoretical computer science, I shifted my focus to algorithmic problems arising in genomics. There is a fundamental difference between the views of algorithms in the two fields: in theoretical computer science the input-output behavior of an algorithm is rigorously specified in advance, whereas in computational biology an algorithm is merely a vehicle for discovering Nature's ground truth. In order to be effective in computational genomics I have had to radically change my approach to research. On the occasion of this keynote address I will share some of the lessons I have learned, in the hope of making the way easier for computer scientists and mathematicians entering this field. These lessons will be encapsulated in a list of aphorisms, accompanied by illustrative examples."
2003,LOGOS: a modular Bayesian model for de novo motif detection.,"Abstract:
The complexity of the global organization and internal structures of motifs in higher eukaryotic organisms raises significant challenges for motif detection techniques. To achieve successful de novo motif detection it is necessary to model the complex dependencies within and among motifs and incorporate biological prior knowledge. In this paper, we present LOGOS, an integrated LOcal and GlObal motif Sequence model for biopolymer sequences, which provides a principled framework for developing, modularizing, extending and computing expressive motif models for complex biopolymer sequence analysis. LOGOS consists of two interacting submodels: HMDM, a local alignment model capturing biological prior knowledge and positional dependence within the motif local structure; and HMM, a global motif distribution model modeling frequencies and dependencies of motif occurrences. Model parameters can be fit using training motifs within an empirical Bayesian framework. A variational EM algorithm is developed for de novo motif detection. LOGOS improves over existing models that ignore biological priors and dependencies in motif structures and motif occurrences, and demonstrates superior performance on both semirealistic test data and cis-regulatory sequences from yeast and Drosophila sequences with regard to sensitivity, specificity, flexibility and extensibility."
2003,Load Balancing in Structured P2P Systems.,"Abstract
Most P2P systems that provide a DHT abstraction distribute objects among ‚Äúpeer nodes‚Äù by choosing random identifiers for the objects. This could result in an O(log N) imbalance. Besides, P2P systems can be highly heterogeneous, i.e. they may consist of peers that range from old desktops behind modem lines to powerful servers connected to the Internet through high-bandwidth lines. In this paper, we address the problem of load balancing in such P2P systems. We explore the space of designing load-balancing algorithms that uses the notion of ‚Äúvirtual servers‚Äù. We present three schemes that differ primarily in the amount of information used to decide how to re-arrange load. Our simulation results show that even the simplest scheme is able to balance the load within 80% of the optimal value, while the most complex scheme is able to balance the load within 95% of the optimal value."
2003,Detecting protein sequence conservation via metric embeddings.,"MOTIVATION
Comparing two protein databases is a fundamental task in biosequence annotation. Given two databases, one must find all pairs of proteins that align with high score under a biologically meaningful substitution score matrix, such as a BLOSUM matrix (Henikoff and Henikoff, 1992). Distance-based approaches to this problem map each peptide in the database to a point in a metric space, such that peptides aligning with higher scores are mapped to closer points. Many techniques exist to discover close pairs of points in a metric space efficiently, but the challenge in applying this work to proteomic comparison is to find a distance mapping that accurately encodes all the distinctions among residue pairs made by a proteomic score matrix. Buhler (2002) proposed one such mapping but found that it led to a relatively inefficient algorithm for protein-protein comparison.


RESULTS
This work proposes a new distance mapping for peptides under the BLOSUM matrices that permits more efficient similarity search. We first propose a new distance function on peptides derived from a given score matrix. We then show how to map peptides to bit vectors such that the distance between any two peptides is closely approximated by the Hamming distance (i.e. number of mismatches) between their corresponding bit vectors. We combine these two results with the LSH-ALL-PAIRS-SIM algorithm of Buhler (2002) to produce an improved distance-based algorithm for proteomic comparison. An initial implementation of the improved algorithm exhibits sensitivity within 5% of that of the original LSH-ALL-PAIRS-SIM, while running up to eight times faster."
2003,CREME: a framework for identifying cis-regulatory modules in human-mouse conserved segments.,"MOTIVATION
The binding of transcription factors to specific regulatory sequence elements is a primary mechanism for controlling gene transcription. Recent findings suggest a modular organization of binding sites for transcription factors that cooperate in the regulation of genes. In this work we establish a framework for finding recurrent cis-regulatory modules in the promoters of a selected set of genes and scoring their statistical significance.


RESULTS
Proceeding from a database of identified binding site motifs and their genomic locations we seek motifs whose frequency in the selected promoters is different than in a background promoter set. We present several statistical tests designed for this purpose. We provide a hashing algorithm for detecting combinations of these motifs that co-occur in clusters within the selected promoters. The significance of such co-occurrences is evaluated using novel statistical scores. Our methods are combined in CREME, a suite of software which includes a browser for viewing the pattern of occurrence of selected cis-regulatory modules. We applied our methodology to find modules within human-mouse conserved promoter segments, focusing on cell cycle regulated genes and stress response related genes. To validate the biological significance of the identified modules we tested whether the associated genes tended to be co-expressed or share similar function. In the cell cycle set five of the seven identified sets of genes were coherently expressed. On the stress response data four of the six detected sets fell predominantly into well-defined functional sub-categories."
2003,A Gambling Game Arising in the Analysis of Adaptive Randomized Rounding.,"Abstract
Let y be a positive real number and let {X i } be an infinite sequence of Bernoulli random variables with the following property: in every realization of the random variables,
‚àë
‚àû
i=1
E[
X
i
|
X
1
,
X
2
,‚ãØ,
X
i‚àí1
]‚â§y
. We specify a function F(x,y) such that, for every positive integer x and every positive real y,
P(
‚àë
‚àû
i=1
X
i
‚â•x)‚â§F(x,y)
P
; moreover, for every x and y, F(x,y) is the best possible upper bound. We give an interpretation of this stochastic process as a gambling game, characterize optimal play in this game, and explain how our results can be applied to the analysis of multi-stage randomized rounding algorithms, giving stronger results than can be obtained using the traditional Hoeffding bounds and martingale tail inequalities."
2003,Large scale reconstruction of haplotypes from genotype data.,"Critical to the understanding of the genetic basis for complex diseases is the modeling of human variation. Most of this variation can be characterized by single nucleotide polymorphisms (SNPs) which are mutations at a single nucleotide position. To characterize an individual's variation, we must determine an individual's haplotype or which nucleotide base occurs at each position of these common SNPs for each chromosome. In this paper, we present results for a highly accurate method for haplotype resolution from genotype data. Our method leverages a new insight into the underlying structure of haplotypes which shows that SNPs are organized in highly correlated ""blocks"". The majority of individuals have one of about four common haplotypes in each block. Our method partitions the SNPs into blocks and for each block, we predict the common haplotypes and each individual's haplotype. We evaluate our method over biological data. Our method predicts the common haplotypes perfectly and has a very low error rate (0.47%) when taking into account the predictions for the uncommon haplotypes. Our method is extremely efficient compared to previous methods, (a matter of seconds where previous methods needed hours). Its efficiency allows us to find the block partition of the haplotypes, to cope with missing data and to work with large data sets such as genotypes for thousands of SNPs for hundreds of individuals. The algorithm is available via webserver at http://www.cs.columbia.edu/compbio/hap."
2003,A stochastic process on the hypercube with applications to peer-to-peer networks.,"Consider the following stochastic process executed on a graph G=(V,E) whose nodes are initially uncovered. In each step, pick a node at random and if it is uncovered, cover it. Otherwise, if it has an uncovered neighbor, cover a random uncovered neighbor. Else, do nothing. This can be viewed as a structured coupon collector process. We show that for a large family of graphs, O(n) steps suffice to cover all nodes of the graph with high probability, where n is the number of vertices. Among these graphs are d-regular graphs with d =Œ©(log n log log n), random d-regular graphs with d =Œ©(log n) and the k-dimensional hypercube where n=2k.This process arises naturally in answering a question on load balancing in peer-to-peer networks. We consider a distributed hash table in which keys are partitioned across a set of processors, and we assume that the number of processors grows dynamically, starting with a single processor. If at some stage there are n processors, the number of queries required to find a key is log2 n+O(1), the number of pointers maintained by each processor is log2 n+O(1), and moreover the worst ratio between the loads of processors is O(1), with high probability. To the best of our knowledge, this is the first analysis of a distributed hash table that achieves asymptotically optimal load balance, while still requiring only O(log n) pointers per processor and O(log n) queries for locating a key; previous methods required Œ©(log2 n) pointers per processor and Œ©(log n) queries for locating a key."
2002,The Efficiency of Resolution and Davis--Putnam Procedures.,"We consider several problems related to the use of resolution-based methods for determining whether a given boolean formula in conjunctive normal form is satisfiable. First, building on the work of Clegg, Edmonds, and Impagliazzo in [Proceedings of the Twenty-Eighth Annual ACM Symposium on Theory of Computing, Philadelphia, PA, 1996, ACM, New York, 1996, pp. 174--183], we give an algorithm for unsatisfiability that when given an unsatisfiable formula of F finds a resolution proof of F. The runtime of our algorithm is subexponential in the size of the shortest resolution proof of F. Next, we investigate a class of backtrack search algorithms for producing resolution refutations of unsatisfiability, commonly known as Davis--Putnam procedures, and provide the first asymptotically tight average-case complexity analysis for their behavior on random formulas. In particular, for a simple algorithm in this class, called ordered DLL, we prove that the running time of the algorithm on a randomly generated k-CNF formula with n variables and m clauses is $2^{\Theta(n(n/m)^{1/(k-2)})}$ with probability $1-o(1)$. Finally, we give new lower bounds on $\mbox{res}(F)$, the size of the smallest resolution refutation of F, for a class of formulas representing the pigeonhole principle and for randomly generated formulas. For random formulas, Chvatal and Szemeredi [J. ACM, 35 (1988), pp. 759--768] had shown that random 3-CNF formulas with a linear number of clauses require exponential size resolution proofs, and Fu [On the Complexity of Proof Systems, Ph.D. thesis, University of Toronto, Toronto, ON, Canada, 1995] extended their results to k-CNF formulas. These proofs apply only when the number of clauses is $\Omega(n \log n)$. We show that a lower bound of the form $2^{n^{\gamma}}$ holds with high probability even when the number of clauses is $n^{(k+2)/4-\epsilon}$.


"
2002,Topologically-Aware Overlay Construction and Server Selection.,"Abstract:
A number of large-scale distributed Internet applications could potentially benefit from some level of knowledge about the relative proximity between its participating host nodes. For example, the performance of large overlay networks could be improved if the application-level connectivity between the nodes in these networks is congruent with the underlying IP-level topology. Similarly, in the case of replicated Web content, client nodes could use topological information in selecting one of multiple available servers. For such applications, one need not find the optimal solution in order to achieve significant practical benefits. Thus, these applications, and presumably others like them, do not require exact topological information and can instead use sufficiently informative hints about the relative positions of Internet hosts. In this paper, we present a binning scheme whereby nodes partition themselves into bins such that nodes that fall within a given bin are relatively close to one another in terms of network latency. Our binning strategy is simple (requiring minimal support from any measurement infrastructure), scalable (requiring no form of global knowledge, each node only needs knowledge of a small number of well-known landmark nodes) and completely distributed (requiring no communication or cooperation between the nodes being binned). We apply this binning strategy to the two applications mentioned above: overlay network construction and server selection. We test our binning strategy and its application using simulation and Internet measurement traces. Our results indicate that the performance of these applications can be significantly improved by even the rather coarse-grained knowledge of topology offered by our binning scheme."
2002,A Hierarchical Bayesian Markovian Model for Motifs in Biopolymer Sequences.,"We propose a dynamic Bayesian model for motifs in biopolymer sequences which captures rich biological prior knowledge and positional dependencies in motif structure in a principled way. Our model posits that the position-specific multinomial parameters for monomer distribution are distributed as a latent Dirichlet-mixture random variable, and the position-specific Dirichlet component is determined by a hidden Markov process. Model parameters can be fit on training motifs using a variational EM algorithm within an empirical Bayesian framework. Variational inference is also used for detecting hidden motifs. Our model improves over previous models that ignore biological priors and positional dependence. It has much higher sensitivity to motifs during detection and a notable ability to distinguish genuine motifs from false recurring patterns."
2002,Discovering local structure in gene expression data: the order-preserving submatrix problem.,"This paper concerns the discovery of patterns in gene expression matrices, in which each element gives the expression level of a given gene in a given experiment. Most existing methods for pattern discovery in such matrices are based on clustering genes by comparing their expression levels in all experiments, or clustering experiments by comparing their expression levels for all genes. Our work goes beyond such global approaches by looking for local patterns that manifest themselves when we focus simultaneously on a subset G of the genes and a subset T of the experiments. Specifically, we look for order-preserving submatrices (OPSMs), in which the expression levels of all genes induce the same linear ordering of the experiments (we show that the OPSM search problem is NP-hard in the worst case). Such a pattern might arise, for example, if the experiments in T represent distinct stages in the progress of a disease or in a cellular process, and the expression levels of all genes in G vary across the stages in the same way.We define a probabilistic model in which an OPSM is hidden within an otherwise random matrix. Guided by this model we develop an efficient algorithm for finding the hidden OPSM in the random matrix. In data generated according to the model the algorithm recovers the hidden OPSM with very high success rate. Application of the methods to breast cancer data seems to reveal significant local patterns.Our algorithm can be used to discover more than one OPSM within the same data set, even when these OPSMs overlap. It can also be adapted to handle relaxations and extensions of the OPSM condition. For example, we may allow the different rows of G x T to induce similar but not identical orderings of the columns, or we may allow the set T to include more than one representative of each stage of a biological process."
2002,The restriction scaffold problem.,"Most shotgun sequencing projects undergo a long and costly phase of finishing, in which a partial assembly forms several contigs whose order, orientation and relative distance is unknown. We propose here a new technique that supplements the shotgun assembly data by cheap and simple complete restriction digests of the target. By computationally combining information from the contig sequences and the fragment sizes measured for several different enzymes, we seek to form a ""scaffold"" on which the contigs will be placed in their correct orientation, order and distance. We give a heuristic search algorithm for solving the problem and report on promising preliminary simulation results. The key to the success of the search scheme is the very rapid solution of its two time-critical subproblems that are solved precisely in linear time.Our simulations indicate that with noise levels of some 3% relative error in measuring fragment sizes, using five enzymes, most datasets of 20 contigs can be correctly ordered, and the remaining ones have most of their pairs of neighboring contigs correct. Hence, the technique has a potential to provide real help to finishing. Even when the target clone remains unfinished, the ability to order and orient the contigs correctly makes the partial assembly both more accessible and more useful for biologists."
2002,Selfish behavior and stability of the internet: a game-theoretic analysis of TCP.,"For years, the conventional wisdom [7, 22] has been that the continued stability of the Internet depends on the widespread deployment of ""socially responsible"" congestion control. In this paper, we seek to answer the following fundamental question: If network end-points behaved in a selfish manner, would the stability of the Internet be endangered?.We evaluate the impact of greedy end-point behavior through a game-theoretic analysis of TCP. In this ""TCP Game"" each flowattempts to maximize the throughput it achieves by modifying its congestion control behavior. We use a combination of analysis and simulation to determine the Nash Equilibrium of this game. Our question then reduces to whether the network operates efficiently at these Nash equilibria.Our findings are twofold. First, in more traditional environments -- where end-points use TCP Reno-style loss recovery and routers use drop-tail queues -- the Nash Equilibria are reasonably efficient. However, when endpoints use more recent variations of TCP (e.g., SACK) and routers employ either RED or drop-tail queues, the Nash equilibria are very inefficient. This suggests that the Internet of the past could remain stable in the face of greedy end-user behavior, but the Internet of today is vulnerable to such behavior. Second, we find that restoring the efficiency of the Nash equilibria in these settings does not require heavy-weight packet scheduling techniques (e.g., Fair Queuing) but instead can be done with a very simple stateless mechanism based on CHOKe [21]."
2001,Optimal Search and One-Way Trading Online Algorithms.,"This paper is concerned with the time series search and one-way trading problems. In the (time series) search problem a player is searching for the maximum (or minimum) price in a sequence that unfolds sequentially, one price at a time. Once during this game the player can decide to accept the current price p in which case the game ends and the player's payoff is p . In the one-way trading problem a trader is given the task of trading dollars to yen. Each day, a new exchange rate is announced and the trader must decide how many dollars to convert to yen according to the current rate. The game ends when the trader trades his entire dollar wealth to yen and his payoff is the number of yen acquired.
The search and one-way trading are intimately related. Any (deterministic or randomized) one-way trading algorithm can be viewed as a randomized search algorithm. Using the competitive ratio as a performance measure we determine the optimal competitive performance for several variants of these problems. In particular, we show that a simple threat-based strategy is optimal and we determine its competitive ratio which yields, for realistic values of the problem parameters, surprisingly low competitive ratios.
We also consider and analyze a one-way trading game played against an adversary called Nature where the online player knows the probability distribution of the maximum exchange rate and that distribution has been chosen by Nature. Finally, we consider some applications for a special case of portfolio selection called two-way trading in which the trader may trade back and forth between cash and one asset.
"
2001,Algorithms for graph partitioning on the planted partition model.,"The NP?hard graph bisection problem is to partition the nodes of an undirected graph into two equal?sized groups so as to minimize the number of edges that cross the partition. The more general graph l?partition problem is to partition the nodes of an undirected graph into l equal?sized groups so as to minimize the total number of edges that cross between groups. We present a simple, linear?time algorithm for the graph l?partition problem and we analyze it on a random ìplanted l?partitionî model. In this model, the n nodes of a graph are partitioned into l groups, each of size n/l; two nodes in the same group are connected by an edge with some probability p, and two nodes in different groups are connected by an edge with some probability r<p. We show that if p?r?n?1/2+? for some constant ?, then the algorithm finds the optimal partition with probability 1? exp(?n?(?)).?© 2001 John Wiley & Sons, Inc.?Random Struct. Alg., 18: 116ñ140, 2001
"
2001,Theoretical Improvements in Algorithmic Efficiency for Network Flow Problems.,"Abstract
This paper presents new algorithms for the maximum flow problem, the Hitchcock transportation problem and the general minimum-cost flow problem. Upper bounds on the number of steps in these algorithms are derived, and are shown to improve on the upper bounds of earlier algorithms."
2001,Feature selection for high-dimensional genomic microarray data.,"We report on the successful application of feature selection methods to a classification problem in molecular biology involving only 72 data points in a 7130 dimensional space. Our approach is a hybrid of filter and wrapper approaches to feature selection. We make use of a sequence of simple filters, culminating in Koller and Sahamiís (1996) Markov Blanket filter, to decide on particular feature subsets for each subset cardinality. We compare between the resulting subset cardinalities using cross validation. The paper also investigates regularization methods as an alternative to feature selection, showing that feature selection methods are preferable in this problem."
2001,CLIFF: clustering of high-dimensional microarray data via iterative feature filtering using normalized cuts.,"We present CLIFF, an algorithm for clustering biological samples using gene expression microarray data. This clustering problem is difficult for several reasons, in particular the sparsity of the data, the high dimensionality of the feature (gene) space, and the fact that many features are irrelevant or redundant. Our algorithm iterates between two computational processes, feature filtering and clustering. Given a reference partition that approximates the correct clustering of the samples, our feature filtering procedure ranks the features according to their intrinsic discriminability, relevance to the reference partition, and irredundancy to other relevant features, and uses this ranking to select the features to be used in the following round of clustering. Our clustering algorithm, which is based on the concept of a normalized cut, clusters the samples into a new reference partition on the basis of the selected features. On a well-studied problem involving 72 leukemia samples and 7130 genes, we demonstrate that CLIFF outperforms standard clustering approaches that do not consider the feature selection issue, and produces a result that is very close to the original expert labeling of the sample set."
2001,Application-Level Multicast Using Content-Addressable Networks.,"Abstract
Most currently proposed solutions to application-level multicast organise the group members into an application-level mesh over which a Distance-Vector routingp rotocol, or a similar algorithm, is used to construct source-rooted distribution trees. The use of a global routing protocol limits the scalability of these systems. Other proposed solutions that scale to larger numbers of receivers do so by restricting the multicast service model to be single-sourced. In this paper, we propose an application-level multicast scheme capable of scaling to large group sizes without restrictingthe service model to a single source. Our scheme builds on recent work on Content-Addressable Networks (CANs). Extendingthe CAN framework to support multicast comes at trivial additional cost and, because of the structured nature of CAN topologies, obviates the need for a multicast routingalg orithm. Given the deployment of a distributed infrastructure such as a CAN, we believe our CAN-based multicast scheme offers the dual advantages of simplicity and scalability."
2001,A scalable content-addressable network.,"Hash tables - which map ""keys"" onto ""values"" - are an essential building block in modern software systems. We believe a similar functionality would be equally valuable to large distributed systems. In this paper, we introduce the concept of a Content-Addressable Network (CAN) as a distributed infrastructure that provides hash table-like functionality on Internet-like scales. The CAN is scalable, fault-tolerant and completely self-organizing, and we demonstrate its scalability, robustness and low-latency properties through simulation."
2000,Algorithms for Optical Mapping.,"Optical mapping is a novel technique for determining the restriction sites on a DNA molecule by directly observing a number of partially digested copies of the molecule under a light microscope. The problem is complicated by uncertainty as to the orientation of the molecules and by erroneous detection of cuts. In this paper we study the problem of constructing a restriction map based on optical mapping data. We give several variants of a polynomial reconstruction algorithm, as well as an algorithm that is exponential in the number of cut sites, and hence is appropriate only for small number of cut sites. We give a simple probabilistic model for data generation and for the errors and prove probabilistic upper and lower bounds on the number of molecules needed by each algorithm in order to obtain a correct map, expressed as a function of the number of cut sites and the error parameters. To the best of our knowledge, this is the first probabilistic analysis of algorithms for the problem. We also provide experimental results confirming that our algorithms are highly effective on simulated data.
"
2000,Universal DNA Tag Systems: A Combinatorial Design Scheme.,"Custom-designed DNA arrays offer the possibility of simultaneously monitoring thousands of hybridization reactions. These arrays show great potential for many medical and scientific applications, such as polymorphism analysis and genotyping. Relatively high costs are associated with the need to specifically design and synthesize problem-specific arrays. Recently, an alternative approach was suggested that utilizes fixed, universal arrays. This approach presents an interesting design problem - the arrays should contain as many probes as possible, while minimizing experimental errors caused by cross-hybridization. We use a simple thermodynamic model to cast this design problem in a formal mathematical framework. Employing new combinatorial ideas, we derive an efficient construction for the design problem and prove that our construction is near-optimal.
"
2000,An Algorithm Combining Discrete and Continuous Methods for Optical Mapping.,"Optical mapping is a novel technique for generating the restriction map of a DNA molecule by observing many single, partially digested copies of it, using fluorescence microscopy. The real-life problem is complicated by numerous factors: false positive and false negative cut observations, inaccurate location measurements, unknown orientations, and faulty molecules. We present an algorithm for solving the real-life problem. The algorithm combines continuous optimization and combinatorial algorithms applied to a nonuniform discretization of the data. We present encouraging results on real experimental data and on simulated data.
"
2000,An Optimal Algorithm for Monte Carlo Estimation.,"A typical approach to estimate an unknown quantity $\mu$ is to design an experiment that produces a random variable Z, distributed in [0,1] with E[Z]=\mu$, run this experiment independently a number of times, and use the average of the outcomes as the estimate. In this paper, we consider the case when no a priori information about Z is known except that is distributed in [0,1]. We describe an approximation algorithm ${\cal A}{\cal A}$ which, given $\epsilon$ and $\delta$, when running independent experiments with respect to any Z, produces an estimate that is within a factor $1+\epsilon$ of $\mu$ with probability at least $1-\delta$. We prove that the expected number of experiments run by ${\cal A}{\cal A}$ (which depends on Z) is optimal to within a constant factor {for every} Z.
"
2000,Parallel Sorting with Limited Bandwidth.,"We study the problem of sorting on a parallel computer with limited communication bandwidth. By using the PRAM(m) model, where p processors communicate through a globally shared memory which can service m requests per unit time, we focus on the trade-off between the amount of local computation and the amount of interprocessor communication required for parallel sorting algorithms. Our main result is a lower bound of $\Omega(\frac{n \log m}{m \log n})$ on the time required to sort n numbers on the exclusive-read and queued-read variants of the PRAM(m). We also show that Leighton's Columnsort can be used to give an asymptotically matching upper bound in the case where m grows as a fractional power of n. The bounds are of a surprising form in that they have little dependence on the parameter p. This implies that attempting to distribute the workload across more processors while holding the problem size and the size of the shared memory fixed will not improve the optimal running time of sorting in this model. We also show that both the lower and the upper bounds can be adapted to bridging models that address the issue of limited communication bandwidth: the LogP model and the bulk-synchronous parallel (BSP) model. The lower bounds provide further convincing evidence that efficient parallel algorithms for sorting rely strongly on high communication bandwidth."
2000,Optimization Problems in Congestion Control.,"Abstract:
One of the crucial elements in the Internet's success is its ability to adequately control congestion. The paper defines and solves several optimization problems related to Internet congestion control, as a step toward understanding the virtues of the TCP congestion control algorithm currently used and comparing it with alternative algorithms. We focus on regulating the rate of a single unicast flow when the bandwidth available to it is unknown and may change over time. We determine near-optimal policies when the available bandwidth is unchanging, and near-optimal competitive policies when the available bandwidth is changing in a restricted manner under the control of an adversary."
2000,Randomized Rumor Spreading.,"Abstract:
Investigates the class of epidemic algorithms that are commonly used for the lazy transmission of updates to distributed copies of a database. These algorithms use a simple randomized communication mechanism to ensure robustness. Suppose n players communicate in parallel rounds in each of which every player calls a randomly selected communication partner. In every round, players can generate rumors (updates) that are to be distributed among all players. Whenever communication is established between two players, each one must decide which of the rumors to transmit. The major problem is that players might not know which rumors their partners have already received. For example, a standard algorithm forwarding each rumor form the calling to the called players for /spl Theta/(ln n) rounds needs to transmit the rumor /spl Theta/(n ln n) times in order to ensure that every player finally receives the rumor with high probability. We investigate whether such a large communication overhead is inherent to epidemic algorithms. On the positive side, we show that the communication overhead can be reduced significantly. We give an algorithm using only O(n ln ln n) transmissions and O(ln n) rounds. In addition, we prove the robustness of this algorithm. On the negative side, we show that any address-oblivious algorithm needs to send /spl Omega/(n ln ln n) messages for each rumor, regardless of the number of rounds. Furthermore, we give a general lower bound showing that time and communication optimality cannot be achieved simultaneously using random phone calls, i.e. every algorithm that distributes a rumor in O(ln n) rounds needs /spl omega/(n) transmissions."
2000,The Genomics Revolution and Its Challenges for Algorithmic Research.,"Abstract
A fundamental goal of biology is to understand how living cells work. Recent developments in biotechnology and information processing have revolutionized this research field. Computational biology is a major component of this revolution and a fertile source of interesting problems related to algorithm design, combinatorics, statistics, combinatorial optimization, pattern recognition, data mining and computational learning theory. The speaker will provide an overview of this field, describing such areas as genomic mapping and sequencing, sequence analysis and analysis of gene expression data. He will then describe how his research in this field has called upon his background in theoretical computer science but required a shift in his approach to the design and development of algorithms."
2000,Universal DNA tag systems: a combinatorial design scheme.,"Custom-designed DNA arrays offer the possibility of simultaneously monitoring thousands of hybridization reactions These arrays show great potential for many medical and scientific applications such as polymorphism analysis and genotyping. Relatively high costs are associated with the need to specifically design and synthesize problem specific arrays. Recently, an alternative approach was suggested that utilizes fixed, universal arrays. This approach presents an interesting design problem‚Äîthe arrays should contain as many probes as possible, while minimizing experimental errors caused by cross-hybridization. We use a simple thermodynamic model to cast this design problem in a formal mathematical framework. Employing new combinatorial ideas, we derive an efficient construction for the design problem, and prove that our construction is near-optimal."
1999,An Algorithmic Approach to Multiple Complete Digest Mapping.,"Multiple Complete Digest (MCD) mapping is a method of determining the locations of restriction sites along a target DNA molecule. The resulting restriction map has many potential applications in DNA sequencing and genetics. In this work, we present a heuristic algorithm for fragment identification, a key step in the process of constructing an MCD map. Given measurements of the restriction fragment sizes from one or more complete digestions of each clone in a clone library covering the molecule to be mapped, the algorithm identifies groups of restriction fragments on different clones that correspond to the same region of the target DNA. Once these groups are correctly determined the desired map can be constructed by solving a system of simple linear inequalities. We demonstrate the effectiveness of our algorithm on real data provided by the Genome Center at the University of Washington.
"
1999,Error-resilient DNA computation.,"The DNA model of computation, with test tubes of DNA molecules encoding bit sequences, is based on three primitives: Extract?A?Bit, which splits a test tube into two test tubes according to the value of a particular bit x, Merge?Two?Tubes, and Detect?Emptiness. If perfect, these operations can test the satisfiability of any boolean formula in linear time. However, in reality the Extract operation is faulty and misclassifies some of the strands. We consider the following reduction problem: given an algorithm based on perfect Extract, Merge, and Detect operations, convert it to an algorithm that is correct with high probability even though the Extract operation is faulty. The fundamental problem in such a reduction is the simulation of a single highly reliable Extract operation. We determine the number of faulty Extract operations to simulate a highly reliable Extract operation, with matching upper and lower bounds (up to a constant factor). We then propose a reduction to convert any algorithm based on error?free operations to an error?resilient algorithm. In the case of simple n?variable boolean functions, Conjunction, Disjunction, and Parity, we prove that our error?resilient algorithms are optimal.?©1999 John Wiley & Sons, Inc.?Random Struct. Alg., 15, 450ñ466, 1999
"
1999,An Algorithm Combining Discrete and Continuous Methods for Optical Mapping.,"Optical mapping is a novel technique for generating the restriction map of a DNA molecule by observing many single, partially digested, copies of it, using florescence microscopy. The real-life problem is complicated by numerous factors: false positive and false negative cut observations, inaccurate location measurements, unknown orientations and faulty molecules. We present an algorithm for solving the real-life problem. The algorithm combines continuous optimization and combinatorial algorithms, applied to a non-uniform discretization of the data. We present encouraging results on real experimental data."
1999,Algorithms for Graph Partitioning on the Planted Partition Model.,"Abstract
The NP-hard graph bisection problem is to partition the nodes of an undirected graph into two equal-sized groups so as to minimize the number of edges that cross the partition. The more general graph l-partition problem is to partition the nodes of an undirected graph into l equal-sized groups so as to minimize the total number of edges that cross between groups.
We present a simple, linear-time algorithm for the graph l-partition problem and analyze it on a random ‚Äúplanted l-partition‚Äù model. In this model, the n nodes of a graph are partitioned into l groups, each of size n/l; two nodes in the same group are connected by an edge with some probability p, and two nodes in different groups are connected by an edge with some probability r < p. We show that if p ‚Äì r > n ‚Äâ‚àí‚Äâ‚àí‚Äâ1/2‚Äâ+‚ÄâŒµ for some constant Œµ, then the algorithm finds the optimal partition with probability
1‚àíexp(‚àí
n
Œ∏(œµ)
)
1
."
1999,Algorithms for choosing differential gene expression experiments.,"Understanding biological systems at the level of genes and proteins is a major challenge. In this paper we represent the interactions among external environmental inputs and genes in a biological system with a graph-theoretic model called a biological pathway. Our goal is to verify a proposed biological pathway by observing the mRNA levels in the associated biological system under changing external environmental inputs and internal gene perturbations. DNAmicroarrays allow large-scale comparisons of mRNA levels in pairs of cell cultures. A DNA microarray contains thousands of spots, each containing some portion of a gene. In a di erential test, mRNAs from two di erent cell cultures are reverse transcribed to cDNAs, labeled with uorescent dyes of two di erent colors, and applied to the array. The relative hybridization levels of the two cDNAs determine the colors of the spots. These colors form the di erential gene expression data. In this paper we assume that biological pathways can be represented as boolean circuits without feedback, di erential tests can be modeled as perturbations of the external inputs and genes, and di erent classes of genes controlled by the pathway can be associated with different outputs of the boolean circuit. A biological pathway can be veri ed by applying a set of di erential tests and comparing the outcomes of the hybridization experiments with the predicted outputs of the pathway. Thus, selecting an economical set of di erential tests to distinguish all the outputs is essential to the veri cation of biological pathways. In this paper we give an algorithm to construct such a set of tests. We have applied the algorithm to a model of the mating pathway in yeast"
1998,Mapping Clones with a Given Ordering or Interleaving.,"We study the problem of constructing a most compact physical map for a collection of clones whose ordering or interleaving on a DNA molecule are given. Each clone is a contiguous section of the DNA and is represented by its fingerprint obtained from biochemical experiments. In this paper we consider two kinds of mapping: single complete digest mapping, in which the fingerprint of a clone is a multiset containing the sizes of the restriction fragments occurring in the clone, and mapping by hybridization of probes, in which the fingerprint of a clone is a multiset consisting of the short oligonucleotide probes occurring in the clone. Our goal is to position the clones and restriction fragments (or probes) on the DNA consistently with the given ordering or interleaving so that the total number of restriction fragments (resp. probes) required on the DNA is minimized.
We first formulate this as a constrained path cover problem on a multistage graph. Using this formulation, it is shown that finding a most compact map for clones with a given ordering is NP-hard. The approximability of the problem is then considered. We present a simple approximation algorithm with ratio 2 . This is in fact the best possible as the above NP-hardness proof actually shows that achieving ratio 2-? is impossible for any constant ? > 0 , unless P = NP. We also give a polynomial-time approximation scheme when the multiplicity is bounded by one (i.e., when the multisets are actually sets). The exact complexity of the problem in this special case is presently unknown. Finally we consider the mapping problem when an interleaving is given which depicts how the clones overlap with each other on the DNA. In the case of restriction fragment data, it is shown that finding a consistent map is NP-complete even if the multiplicity is bounded by 3 . This may suggest that information about the interleaving of clones does not necessarily make the problem computationally easier in single complete digest mapping. On the other hand, in the case of hybridization data, there is an efficient algorithm to construct a most compact map when the interleaving of clones is given.
"
1998,"Graph Traversals, Genes and Matroids: An Efficient Case of the Travelling Salesman Problem.","Abstract
In this paper we consider graph traversal problems (Euler and Travelling Salesman traversals) that arise from a particular technology for DNA sequencing - sequencing by hybridization (SBH). We first explain the connection of the graph problems to SBH and then focus on the traversal problems. We describe a practical polynomial time solution to the Travelling Salesman Problem in a rich class of directed graphs (including edge weighted binary de Bruijn graphs), and provide bounded-error approximation algorithms for the maximum weight TSP in a superset of those directed graphs. We also establish the existence of a matroid structure defined on the set of Euler and Hamilton paths in the restricted class of graphs."
1998,On Parallel Evaluation of Game Trees.,"A class of parallel algorithms for evaluating game trees is presented. These algorithms parallelize a standard sequential algorithm for evaluating AND/OR trees and the &agr;-&bgr; pruning procedure for evaluating MIN/MAX trees. It is shown that, uniformly on all instances of uniform AND/OR trees, the parallel AND/OR tree algorithm achieves an asymptotic linear speedup using a polynomial number of processors in the height of the tree. The analysis of linear speedup using more than a linear number of processors is due to J. Harting. A numerical lower bound rigorously establishes a good speedup for the uniform AND/OR trees with parameters that are typical in practice. The performance of the parallel &agr;-&bgr; algorithm on best-ordered MIN/MAX trees is analyzed."
1998,Constructing maps using the span and inclusion relations.,n/a
1998,Algorithms for optical mapping.,"Optical mapping is a novel technique for determining the restriction sites on a DNA molecule by directly observing a number of partially digested copies of the molecule under a light microscope. The problem is complicated by uncertainty as to the orientation of the molecules and by erroneous detection of cuts. In this paper we study the problem of constructing a restriction map based on optical mapping data. We give several variants of a polynomial reconstruction algorithm, as well as an algorithm that is exponential in the number of cut sites, and hence is appropriate only for small number of cut sites. We give a simple probabilistic model for data generation and for the errors and prove probabilistic upper and lower bounds on the number of molecules needed by each algorithm in order to obtain a correct map, expressed as a function of the number of cut sites and the error parameters. To the best of our knowledge, this is the first probabilistic analysis of algorithms for the problem. We also provide experimental results confirming that our algorithms are highly effective on simulated data."
1998,"Random Graphs, Random Walks, Differential Equations and the Probabilistic Analysis of Algorithms.",n/a
1998,On the Complexity of Unsatisfiability Proofs for Random k-CNF Formulas.,"WC study the complexity of proving unsatisfiability for random k-CNP formulas with clause density A = m/n where 111 is number of clauses and n is the number of variables. We prove the first nontrivial general upper bound, giving algorithmo that, in particular, for k = 3 produce refutations almost certainly in time 20tîiA). This is polynomial when "
1997,Nearly Optimal Competitive Online Replacement Policies.,"This paper studies the online replacement problem. In this problem an online player is engaged at each time in one activity. Associated with each activity are a changeover cost and flow rate. While involved in an activity the player's budget is depleted at the activity's flow rate. The player can switch to a new activity whenever it is offered but he pays a changeover cost. The player's goal is to decide when to switch activities so that his total cost is minimized. Typical applications are: equipment, jobs and supplier replacement, mortgage refinancing, etc.
With respect to the competitive ratio performance measure, this paper seeks to determine the best possible competitive ratio achievable by an online replacement policy. Our results include the following: a general lower bound on the performance of any deterministic policy, a policy that is optimal in several special cases and a simple policy that is approximately optimal.
"
1997,The rank of sparse random matrices over finite fields.,"Let M be a random (n◊n)?matrix over GF[q] such that for each entry Mij in M and for each nonzero field element ? the probability Pr[Mij=?] is p/(q?1), where p=(log?n?c)/n and c is an arbitrary but fixed positive constant. The probability for a matrix entry to be zero is 1?p. It is shown that the expected rank of M is n???(1). Furthermore, there is a constant A such that the probability that the rank is less than n?k is less than A/qk. It is also shown that if c grows depending on n and is unbounded as n goes to infinity, then the expected difference between the rank of M and n is unbounded.?© 1997 John Wiley & Sons, Inc.?Random Struct. Alg., 10, 407ñ419, 1997
"
1997,Emerging opportunities for theoretical computer science.,"The principles underlying this report can be summarized as follows:1. A strong theoretical foundation is vital to computer science.2. Theory can be enriched by practice.3. Practice can be enriched by theory.4. If we are guided by (2) and (3), the value, impact, and funding of theory will be enhanced.In order to achieve a greater synergy between theory and application, and to sustain and expand on the remarkable successes of Theory of Computing (TOC), we consider it essential to increase the impact of theory on key application areas. This requires additional financial resources in support of theory, and closer interaction between theoreticians and researchers in other areas of computer science and in other disciplines.The report does not make a detailed assessment of the overall state of theoretical computer science or fully chronicle the achievements of this field. Instead, it has the specific objective of recommending ways to harness these remarkable achievements for the solution of challenging problems emerging from new developments such as the information superhighway.Section 1 describes the events leading up to this report and delineates the report's objectives. Section 2 establishes the context for the report. It traces the history of TOC, describes the impact that TOC has achieved in the areas of core theory and fundamental algorithms, points out the differences between these areas and application-oriented theory, and calls for an intensified effort to bring the methods of TOC to bear on applications. It then goes on to define the four main categories into which our recommen- dations fall: building bridges between theory and applications, algorithm engineering, communication, and education. Section 3 discusses some specific opportunities for stimulating interactions between TOC and applied areas. Section 4 proposes an applied research initiative, Information Access in a Globally Distributed Environment, which identifies an exciting current technological area that we believe presents challenging opportunities for excellent theoretical work. Section 5 proposes a second applied research initiative, The Algorithmic Stockroom, that would exploit and extend the body of theoretical knowledge in the field of algorithms. Section 6 proposes a broadening in graduate education with two purposes in mind: to better prepare theoreticians to interact creatively with practitioners, and to provide future practitioners with the background they will need to benefit from this exchange."
1997,Fast and Intuitive Clustering of Web Documents.,"Conventional document retrieval systems (e.g., Alta Vista) return long lists of ranked documents in response to user queries. Recently, document clustering has been put forth as an alternative method of organizing retrieval results. A person browsing the clusters can discover patterns that could be overlooked in the traditional presentation. This paper describes two novel clustering methods that intersect the documents in a cluster to determine the set of words (or phrases) shared by all the documents in the cluster. We report on experiments that evaluate these intersection-based clustering methods on collections of snippets returned from Web search engines. First, we show that word-intersection clustering produces superior clusters and does so faster than standard techniques. Second, we show that our O(n log n) time phrase-intersection clustering method produces comparable clusters and does so more than two orders of magnitude faster than all methods tested."
1997,An algorithmic approach to multiple complete digest mapping.,"Multiple Complete Digest (MCD) mapping is a method of determining the locations of restriction sites along a target DNA molecule. The resulting restriction map has many potential applications in DNA sequencing and genetics. In this work, we present a heuristic algorithm for fragment identification, a key step in the process of constructing an MCD map. Given measurements of the restriction fragment sizes from one or more complete digestions of each clone in a clone library covering the molecule to be mapped, the algorithm identifies groups of restriction fragments on different clones that correspond to the same region of the target DNA. Once these groups are correctly determined the desired map can be constructed by solving a system of simple linear inequalities. We demonstrate the effectiveness of our algorithm on real data provided by the Genome Center at the University of Washington"
1997,Mapping clones with a given ordering or interleaving (abstract).,"We study the problem of constructing a most compact physical map for a collection of clones whose ordering or interleaving on a DNA molecule are given. Each clone is a contiguous section of the DNA and is represented by its fingerprint obtained from biochemical experiments. In this paper we consider two kinds of mapping: single complete digest mapping, in which the fingerprint of a clone is a multiset containing the sizes of the restriction fragments occurring in the clone, and mapping by hybridization of probes, in which the fingerprint of a clone is a multiset consisting of the short oligonucleotide probes occurring in the clone. Our goal is to position the clones and restriction fragments (or probes) on the DNA consistently with the given ordering or interleaving so that the total number of restriction fragments (resp. probes) required on the DNA is minimized. We first formulate this as a constrained path cover problem on a multistage graph. Using this formulation, it is shown that finding a most compact map for clones with a given ordering is NP-hard. The approximability of the problem is then considered. We present a simple approximation algorithm with ratio 2 . This is in fact the best possible as the above NP-hardness proof actually shows that achieving ratio 2-? is impossible for any constant ? > 0 , unless P = NP. We also give a polynomial-time approximation scheme when the multiplicity is bounded by one (i.e., when the multisets are actually sets). The exact complexity of the problem in this special case is presently unknown. Finally we consider the mapping problem when an interleaving is given which depicts how the clones overlap with each other on the DNA. In the case of restriction fragment data, it is shown that finding a consistent map is NP-complete even if the multiplicity is bounded by 3 . This may suggest that information about the interleaving of clones does not necessarily make the problem computationally easier in single complete digest mapping. On the other hand, in the case of hybridization data, there is an efficient algorithm to construct a most compact map when the interleaving of clones is given."
1997,Mapping Clones with a Given Ordering or Interleaving (Extended Abstract).,"We study the problem of constructing a most compact physical map for a collection of clones whose ordering or interleaving on a DNA molecule are given. Each clone is a contiguous section of the DNA and is represented by its fingerprint obtained from biochemical experiments. In this paper we consider two kinds of mapping: single complete digest mapping, in which the fingerprint of a clone is a multiset containing the sizes of the restriction fragments occurring in the clone, and mapping by hybridization of probes, in which the fingerprint of a clone is a multiset consisting of the short oligonucleotide probes occurring in the clone. Our goal is to position the clones and restriction fragments (or probes) on the DNA consistently with the given ordering or interleaving so that the total number of restriction fragments (resp. probes) required on the DNA is minimized. We first formulate this as a constrained path cover problem on a multistage graph. Using this formulation, it is shown that finding a most compact map for clones with a given ordering is NP-hard. The approximability of the problem is then considered. We present a simple approximation algorithm with ratio 2 . This is in fact the best possible as the above NP-hardness proof actually shows that achieving ratio 2-? is impossible for any constant ? > 0 , unless P = NP. We also give a polynomial-time approximation scheme when the multiplicity is bounded by one (i.e., when the multisets are actually sets). The exact complexity of the problem in this special case is presently unknown. Finally we consider the mapping problem when an interleaving is given which depicts how the clones overlap with each other on the DNA. In the case of restriction fragment data, it is shown that finding a consistent map is NP-complete even if the multiplicity is bounded by 3 . This may suggest that information about the interleaving of clones does not necessarily make the problem computationally easier in single complete digest mapping. On the other hand, in the case of hybridization data, there is an efficient algorithm to construct a most compact map when the interleaving of clones is given."
1996,Efficient PRAM Simulation on a Distributed Memory Machine.,"Abstract
We present algorithms for the randomized simulation of a shared memory machine (PRAM) on a Distributed Memory Machine (DMM). In a PRAM, memory conflicts occur only through concurrent access to the same cell, whereas the memory of a DMM is divided into modules, one for each processor, and concurrent accesses to the same module create a conflict. Thedelay of a simulation is the time needed to simulate a parallel memory access of the PRAM. Any general simulation of anm processor PRAM on ann processor DMM will necessarily have delay at leastm/n. A randomized simulation is calledtime-processor optimal if the delay isO(m/n) with high probability. Using a novel simulation scheme based on hashing we obtain a time-processor optimal simulation with delayO(log log(n) log*(n)). The best previous simulations use a simpler scheme based on hashing and have much larger delay: Œò (log(n)/log log(n)) for the simulation of an n processor PRAM on ann processor DMM, and Œò(log(n)) in the case where the simulation is time-processor optimal.
Our simulations use several (two or three) hash functions to distribute the shared memory among the memory modules of the PRAM. The stochastic processes modeling the behavior of our algorithms and their analyses based on powerful classes of universal hash functions may be of independent interest."
1996,A Method for Obtaining Randomized Algorithms with Small Tail Probabilities.,"Abstract
We study strategies for converting randomized algorithms of the Las Vegas type into randomized algorithms with small tail probabilities."
1996,LogP: A Practical Model of Parallel Computation.,n/a
1996,"Graph Traversals, Genes, and Matroids: An Efficient Case of the Travelling Salesman Problem.","Abstract
In this paper we consider graph traversal problems that arise from a particular technology for DNA sequencing ‚Äî sequencing by hybridization (SBH). We first explain the connection of the graph problems to SBH and then focus on the traversal problems. We describe a practical polynomial time solution to the Travelling Salesman Problem in a rich class of directed graphs (including edge weighted binary de Bruijn graphs), and provide a bounded-error approximation algorithm for the maximum weight TSP in a superset of those directed graphs. We also establish the existence of a matroid structure defined on the set of Euler and Hamilton paths in the restricted class of graphs."
1996,Efficient Information Gathering on the Internet (extended abstract).,"Abstract:
The Internet offers unprecedented access to information. At present most of this information is free, but information providers ore likely to start charging for their services in the near future. With that in mind this paper introduces the following information access problem: given a collection of n information sources, each of which has a known time delay, dollar cost and probability of providing the needed information, find an optimal schedule for querying the information sources. We study several variants of the problem which differ in the definition of an optimal schedule. We first consider a cost model in which the problem is to minimize the expected total cost (monetary and time) of the schedule, subject to the requirement that the schedule may terminate only when the query has been answered or all sources have been queried unsuccessfully. We develop an approximation algorithm for this problem and for an extension of the problem in which more than a single item of information is being sought. We then develop approximation algorithms for a reward model in which a constant reward is earned if the information is successfully provided, and we seek the schedule with the maximum expected difference between the reward and a measure of cost. The monetary and time costs may either appear in the cost measure or be constrained not to exceed a fixed upper bound; these options give rise to four different variants of the reward model."
1996,Error-Resilient DNA Computation.,The DNA model of computation with test tubes of DNA molecules encoding bit sequences is based on three primitives Extract A Bit Merge Two Tubes and Detect Emptiness Perfect operations can test the satis ability of any boolean formula in linear time However in re ality the Extract operation is faulty We determine the minimum num ber of faulty Extract operations required to simulate a single highly reliable Extract operation and derive a method for converting any algorithm based on error free operations to an error resilient one
1995,Physical Mapping of Chromosomes: A Combinatorial Problem in Molecular Biology.,"This paper is concerned wth the physical mapping of DNA molecules using data about the hybridization of oligonucleotide probes to a library of clones. In mathematical terms, the DNA molecule corresponds to an interval on the real line, each clone to a subinterval, and each probe occurs at a finite set of points within the interval. A stochastic model for the occurrences of the probes and the locations of the clones is assumed. Given a matrix of incidences between probes and clones, the task is to reconstruct the most likely interleaving of the clones. Combinatorial algorithms are presented for solving approximations to this problem, and computational results are presented.
"
1995,An algorithm for analysing probed partial digestion experiments.,"A partial digestion of DNA (e.g. cosmid, Lambda, YAC, chromosome) is performed and the lengths of thoses fragments which hybridize to a labeled probe are measured using gel electrophoresis. We give an efficient algorithm that takes as input this experimental data and proposes one or more candidate solutions. Each solution designates the location of each restriction site and spec the endpoints of each fragment. (Further experiments can then be designed to select the correct solution from this small set of candidates.) The algorithm works well even when the experiment gives inexact values for the lengths.
"
1995,Physical Mapping of Chromosomes Using Unique Probes.,"The goal of physical mapping of the genome is to reconstruct a strand of DNA given a collection of overlapping fragments, or clones, from the strand. We present several algorithms to infer how the clones overlap, given data about each clone. We focus on data used to map human chromosomes 21 and Y, in which relatively short substrings, or probes, are extracted from the ends of clones. The substrings are long enough to be unique with high probability. The data we are given is an incidence matrix of clones and probes. In the absence of error, the correct placement can be found easily using a PQ-tree. The data are never free from error, however, and algorithms are differentiated by their performance in the presence of errors. We approach errors from two angles: by detecting and removing them, and by using algorithms that are robust in the presence of errors. We have also developed a strategy to recover noiseless data through an interactive process that detects anomalies in the data and retests questionable entries in the incidence matrix of clones and probes. We evaluate the effectiveness of our algorithms empirically, using simulated data as well as real data from human chromosome 21.
"
1995,Bounded Branching Process AND/OR Tree Evaluation.,"We study the tail distribution of supercritical branching processes for which the number of offspring of an element is bounded. Given a supercritical branching process {Zn}urn:x-wiley:10429832:media:RSA3240070202:tex2gif-stack-1 with a bounded offspring distribution, we derive a tight bound, decaying super?exponentially fast as c increases, on the probability Pr[Zn > cE(Zn)], and a similar bound on the probability Pr[Zn ? E(Zn)/c] under the assumption that each element generates at least two offspring. As an application, we observe that the execution of a canonical algorithm for evaluating uniform AND/OR trees in certain probabilistic models can be viewed as a two?type supercritical branching process with bounded offspring, and show that the execution time of this algorithm is likely to concentrate around its expectation, with a standard deviation of the same order as the expectation.
"
1995,A Graph-Theoretic Game and Its Application to the k-Server Problem.,"This paper investigates a zero-sum game played on a weighted connected graph G between two players, the tree player and the edge player. At each play, the tree player chooses a spanning tree T and the edge player chooses an edge e. The payoff to the edge player is $\textit{cost} (T, e)$, defined as follows: If e lies in the tree T then $\textit{cost}(T, e) = 0$; if e does not lie in the tree then $\textit{cost}(T, e) = cycle(T, e)/w(e)$, where $w(e)$ is the weight of edge e and $\textit{cycle}(T, e)$ is the weight of the unique cycle formed when edge e is added to the tree T. The main result is that the value of the game on any n-vertex graph is bounded above by $\exp(O(\sqrt{\log n \log \log n}))$. It is conjectured that the value of the game is $O(\log n)$.

The game arises in connection with the k-server problem on a road network; i.e., a metric space that can be represented as a multigraph G in which each edge e represents a road of length $w(e)$. It is shown that, if the value of the game on G is $\textit{Val}(G, w)$, then there is a randomized strategy that achieves a competitive ratio of $k(1 + \textit{Val}(G, w))$ against any oblivious adversary. Thus, on any n-vertex road network, there is a randomized algorithm for the k-server problem that is $k \cdot \exp(O(\sqrt{\log n \log \log n}))$ competitive against oblivious adversaries.

At the heart of the analysis of the game is an algorithm that provides an approximate solution for the simple network design problem. Specifically, for any n-vertex weighted, connected multigraph, the algorithm constructs a spanning tree T such that the average, over all edges e, of $\textit{cost}(T, e)$ is less than or equal to $\exp(O(\sqrt{\log n \log \log n}))$. This result has potential application to the design of communication networks. It also improves substantially known estimates concerning the existence of a sparse basis for the cycle space of a graph.
"
1995,When is the Assignment Bound Tight for the Asymmetric Traveling-Salesman Problem?,"We consider the probabilistic relationship between the value of a random asymmetric traveling salesman problem $\textit{ATSP}(M)$ and the value of its assignment relaxation $\textit{AP}(M)$. We assume here that the costs are given by an $n \times n$ matrix M whose entries are independently and identically distributed. We focus on the relationship between $Pr(\textit{ATSP}(M) = \textit{AP}(M))$ and the probability $p_{n}$ that any particular entry is zero. If $np_{n} \rightarrow \infty $ with n then we prove that $\textit{ATSP}(M) = \textit{AP}(M)$ with probability 1-o(1). This is shown to be best possible in the sense that if $np (n) \rightarrow c,\, c > 0$ and constant, then $Pr(\textit{ATSP}(M) = \textit{AP}(M)) < 1 - \phi (c)$ for some positive function $\phi$. Finally, if $np_{n} \rightarrow 0$ then $Pr(\textit{ATSP}(M) = \textit{AP}(M)) \rightarrow 0$.
"
1995,An Optimal Algorithm for Monte Carlo Estimation (Extended Abstract).,"Abstract:
A typical approach to estimate an unknown quantity /spl mu/ is to design an experiment that produces a random variable Z distributed in [O,1] with E[Z]=/spl mu/, run this experiment independently a number of times and use the average of the outcomes as the estimate. In this paper, we consider the case when no a priori information about Z is known except that is distributed in [0,1]. We describe an approximation algorithm AA which, given /spl epsiv/ and /spl delta/, when running independent experiments with respect to any Z, produces an estimate that is within a factor 1+/spl epsiv/ of /spl mu/ with probability at least 1-/spl delta/. We prove that the expected number of experiments ran by AA (which depends on Z) is optimal to within a constant factor for every Z."
1995,The Bit Vector Intersection Problem (Preliminary Version).,"Abstract:
This paper introduces the bit vector intersection problem: given a large collection of sparse bit vectors, find all the pairs with at least t ones in common for a given input parameter t. The assumption is that the number of ones common to any two vectors is significantly less than t, except for an unknown set of O(n) pairs. This problem has important applications in DNA physical mapping, clustering, and searching for approximate dictionary matches. We present two randomized algorithms that solve this problem with high probability and in sub-quadratic expected time. One of these algorithms is based on a recursive tree-searching procedure, and the other on hashing. We analyze the tree scheme in terms of branching processes, while our analysis of the hashing scheme is based on Markov chains. Since both algorithms have similar asymptotic performance, we also examine experimentally their relative merits in practical situations. We conclude by showing that a fundamental problem arising in the Human Genome Project is captured by the bit vector intersection problem described above and hence can be solved by our algorithms."
1995,Modeling parallel communication.,"Abstract:
The LogP model provides a framework for analyzing the performance of algorithms on distributed-memory multiprocessors. The model takes into account both the cost of computation and the cost of communication. It describes a multiprocessor in terms of four parameters, representing computation power, communication bandwidth, communication latency and the degree of overlap between computation and communication. In this paper we discuss the rationale for the different features of the LogP model. We then compare the model with other models having similar aims, such as network-based models, Valiant's BSP model, and the PRAM(m) model. Finally, we call upon the work of a number of students at Berkeley to illustrate the application of the model to problems of broadcasting, sorting, FFT computation, summing, prefix summing and solution of triangular and tridiagonal systems.< >"
1995,Scheduling Parallel Communication: The h-relation Problem.,"Abstract
This paper is concerned with the efficient scheduling and routing of point-to-point messages in a distributed computing system with n processors. We examine the h-relation problem, a routing problem where each processor has at most h messages to send and at most h messages to receive. Communication is carried out in rounds. Direct communication is possible from any processor to any other, and in each round a processor can send one message and receive one message. The off-line version of the problem arises when every processor knows the source and destination of every message. In this case the messages can be routed in at most h rounds. More interesting, and more typical, is the on-line version, in which each processor has knowledge only of h and of the destinations of those messages which it must send. The on-line version of the problem is the focus of this paper.
The difficulty of the h-relation problem stems from message conflicts, in which two or more messages are sent to the same processor in a given round, but at most one can be received. The problem has been well studied in the OCPC optical network model, but not for other contemporary network architectures which resolve message conflicts using other techniques. In this paper, we study the h-relation problem under alternative models of conflict resolution, most notably a FIFO queue discipline motivated by wormhole routing and an arbitrary write discipline motivated by packet-switching networks. In each model the problem can be solved by a randomized algorithm in an expected number of rounds of the form ch+o(h)+log Œò(1) n, and we focus on obtaining the smallest possible asymptotic constant factor c. We first present a lower bound, proving that a constant factor of 1 is not achievable in general. We then present a randomized algorithm for each discipline and show that they achieve small constant factors."
1995,Parallel Sorting with Limited Bandwidth.,"We study the problem of sorting on a parallel computer with limited communication bandwidth. By using the PRAM(m) model, where p processors communicate through a globally shared memory which can service m requests per unit time, we focus on the trade-off between the amount of local computation and the amount of interprocessor communication required for parallel sorting algorithms. Our main result is a lower bound of ?( n log m m log n ) on the time required to sort n numbers on the exclusive-read and queued-read variants of the PRAM(m). We also show that Leighton's Columnsort can be used to give an asymptotically matching upper bound in the case where m grows as a fractionalpower of n. The bounds are of a surprising form in that they have little dependence on the parameter p. This implies that attempting to distribute the workload across more processors while holding the problem size and the size of the shared memory fixed will not improve the optimal running time of sorting in this model. We also show that both the lower and the upper bounds can be adapted to bridging models that address the issue of limited communication bandwidth: the LogP model and the bulk-synchronous parallel (BSP) model. The lower bounds provide further convincing evidence that efficient parallel algorithms for sorting rely strongly on high communication bandwidth."
1994,On the Power of Randomization in On-Line Algorithms.,"Against in adaptive adversary, we show that the power of randomization in on-line algorithms is severely limited! We prove the existence of an efficient ìsimulationî of randomized on-line algorithms by deterministic ones, which is best possible in general. The proof of the upper bound is existential. We deal with the issue of computing the efficient deterministic algorithm, and show that this is possible in very general cases.
"
1994,Coding Techniques for Handling Failures in Large Disk Arrays.,"Abstract
A crucial issue in the design of very large disk arrays is the protection of data against catastrophic disk failures. Although today single disks are highly reliable, when a disk array consists of 100 or 1000 disks, the probability that at least one disk will fail within a day or a week is high. In this paper we address the problem of designing erasure-correcting binary linear codes that protect against the loss of data caused by disk failures in large disk arrays. We describe how such codes can be used to encode data in disk arrays, and give a simple method for data reconstruction. We discuss important reliability and performance constraints of these codes, and show how these constraints relate to properties of the parity check matrices of the codes. In so doing, we transform code design problems into combinatorial problems. Using this combinatorial framework, we present codes and prove they are optimal with respect to various reliability and performance constraints."
1994,Probabilistic Recurrence Relations.,"This paper is concerned with recurrence relations that arise frequently in the analysis of divide and conquer algorithms. In order to solve a problem instance of size x, such as an algorithm invests an amount of work a(x) to break the problem into subproblems of sizes , and then proceeds to solve the subproblems. Our particular interest is in the case where the sizes are random variables; this may occur either because of randomization within the algorithm or because the instances to be solved are assumed to be drawn from a probability distribution. When the are random variables the running time of the algorithm on instances of size x is also a random variable T(x). We give several easy-to-apply methods for obtaining fairly tight bounds on the upper tails of the probability distribution of T(x), and present a number of typical applications of these bounds to the analysis of algorithms. The proofs of the bounds are based on an interesting analysis of optimal strategies in certain gambling games."
1994,Average Case Analysis of a Heuristic for the Assignment Problem.,"Our main contribution is an O(n log n) algorithm that determines with high probability a perfect matching in a random 2-out bipartite graph. We also show that this algorithm runs in O(n) expected time. This algorithm can be used as a subroutine in an O(n2) heuristic for the assignment problem. When the weights in the assignment problem are independently and uniformly distributed in the interval [0, 1], we prove that the expected weight of the assignment returned by thus heuristic is bounded above by 3 + O(n??), for some posrtive constant a.
"
1994,Physical Mapping of Chromosomes Using Unique Probes.,"The goal of physical mapping of the genome is to reconstruct a strand of DNA given a collection of overlapping fragments, or clones, from the strand. We present several algorithms to infer how the clones overlap, given data about each clone. We focus on data used to map human chromosomes 21 and Y, in which relatively short substrings, or probes, are extracted from the ends of clones. The substrings are long enough to be unique with high probability. The data we are given is an incidence matrix of clones and probes. In the absence of error, the correct placement can be found easily using a PQ-tree. The data are never free from error, however, and algorithms are differentiated by their performance in the presence of errors. We approach errors from two angles: by detecting and removing them, and by using algorithms that are robust in the presence of errors. We have also developed a strategy to recover noiseless data through an interactive process that detects anomalies in the data and retests questionable entries in the incidence matrix of clones and probes. We evaluate the effectiveness of our algorithms empirically, using simulated data as well as real data from human chromosome 21.
"
1994,Selection in the Presence of Noise: The Design of Playoff Systems.,n/a
1993,Randomized Parallel Algorithms for Backtrack Search and Branch-and-Bound Computation.,"Universal randomized methods for parallelizing sequential backtrack search and branch-and-bound computation are presented. These methods execute on message-passing multiprocessor systems, and require no global data structures or complex communication protocols. For backtrack search, it is shown that, uniformly on all instances, the method described in this paper is likely to yield a speed-up within a small constant factor from optimal, when all solutions to the problem instance are required. For branch-and-bound computation, it is shown that, uniformly on all instances, the execution time of this method is unlikely to exceed a certain inherent lower bound by more than a constant factor. These randomized methods demonstrate the effectiveness of randomization in distributed parallel computation."
1993,Probabilistic Analysis of Network Flow Algorithms.,"This paper is concerned with the design and probabilistic analysis of algorithms for the maximum-flow problem and capacitated transportation problems. These algorithms run in linear time and, under certain assumptions about the probability distribution of edge capacities, obtain an optimal solution with high probability. The design of our algorithms is based on the following general method, which we call the mimicking method, for solving problems in which some of the input data are deterministic and some are random with a known distribution:

1. Replace each random variable in the problem by its expectation; this gives a deterministic problem instance that has a special form, making it particularly easy to solve;

2. Solve the resulting deterministic problem instance;

3. Taking into account the actual values of the random variables, mimic the solution of the deterministic instance to obtain a near-optimal solution to the original problem;

4. Fine-tune this suboptimal solution to obtain an optimal solution.

We present linear time algorithms to compute a feasible flow in directed and undirected capacitated transportation problem instances. The algorithms are shown to be successful with high probability when the probability distribution of the input data satisfies certain assumptions. We also consider the maximum flow problem with multiple sources and sinks. We show that with high probability the minimum cut isolates either the sources or the sinks, and we give a linear-time algorithm that produces a maximum flow with high probability."
1993,A Monte-Carlo Algorithm for Estimating the Permanent.,"Let A be an $n \times n$ matrix with 0-1 valued entries, and let ${\operatorname{per}}(A)$ be the permanent of A. This paper describes a Monte-Carlo algorithm that produces a ìgood in the relative senseî estimate of ${\operatorname{per}}(A)$ and has running time ${\operatorname{poly}}(n)2^{{n / 2}} $, where ${\operatorname{poly}}(n)$ denotes a function that grows polynomially with n.
"
1993,The Mortgage Problem.,"Abstract:
Mortgage refinancing is a complex real-life problem involving a sequence of decisions, each of which requires a trade-off between the transaction cost associated with refinancing and the benefit of obtaining a lower interest rate. The authors present a simplified mathematical model of this problem. Within this model, they seek to determine the best possible competitive ratio achievable by an on-line mortgage refinancing policy. The main results are the following: under the assumption that the initial mortgage is obtained with an interest rate M and that future interest rates cannot decrease below m>or=0, they show a lower bound r= Omega (/sup 1nM///sub (m+1)lnlnM/) on the competitive ratio of any mortgage refinancing policy. Then they give an on-line policy that is optimal in some special cases, including the cases m=0 and M<(1+/sup 2///sub m+1/)(m+2). For other values of m, M the on-line policy is proven to be r/sup 2/-competitive.< >"
1993,LogP: Towards a Realistic Model of Parallel Computation.,"A vast body of theoretical research has focused either on overly simplistic models of parallel computation, notably the PRAM, or overly specific models that have few representatives in the real world. Both kinds of models encourage exploitation of formal loopholes, rather than rewarding development of techniques that yield performance across a range of current and future parallel machines. This paper offers a new parallel machine model, called LogP, that reflects the critical technology trends underlying parallel computers. it is intended to serve as a basis for developing fast, portable parallel algorithms and to offer guidelines to machine designers. Such a model must strike a balance between detail and simplicity in order to reveal important bottlenecks without making analysis of interesting problems intractable. The model is based on four parameters that specify abstractly the computing bandwidth, the communication bandwidth, the communication delay, and the efficiency of coupling communication and computation. Portable parallel algorithms typically adapt to the machine configuration, in terms of these parameters. The utility of the model is demonstrated through examples that are implemented on the CM-5."
1993,Physical Mapping of Chromosomes: A Combinatorial Problem in Molecular Biology.,"This paper is concerned wth the physical mapping of DNA molecules using data about the hybridization of oligonucleotide probes to a library of clones. In mathematical terms, the DNA molecule corresponds to an interval on the real line, each clone to a subinterval, and each probe occurs at a finite set of points within the interval. A stochastic model for the occurrences of the probes and the locations of the clones is assumed. Given a matrix of incidences between probes and clones, the task is to reconstruct the most likely interleaving of the clones. Combinatorial algorithms are presented for solving approximations to this problem, and computational results are presented.
"
1993,Optimal Broadcast and Summation in the LogP Model.,"In many distributed memory parallel computers the only built-in communication primitive is point-to-point message transmission, and more powerful operations such as broadcast and synchronization must be realized through this primitive. Within the LogP model of parallel computation we present algorithms that yield optimal communication schedules for several broadcast and synchronization operations. Most of our algorithms are the absolute best possible in that not even the constant factors can be improved upon. For one particular broadcast problem, called continuous broadcast, the optimality of our algorithm is not yet completely proven, although proofs have been achieved for a certain range of parameters. We also devise an optimal algorithm for summing or, more generally, applying a non-commutative associative binary operator to a set of operands. "
1993,Mapping the genome: some combinatorial problems arising in molecular biology.,"The ultimate goal of the Human Genome Project and many other efforts in molecular biology is to sequence the chromosomal DNA of humans and other species and elucidate the genetic information contained therein. A less ambitious intermediate goal is to construct physical maps of our 23 pairs of chromosomes. A physical map specifies the locations of markers identifiable fragments of DNA along the DNA molecule. These markers provide a kind of random access to the linear DNA molecule. To locate a feature of interest such as a gene, an experimenter can start at a marker that is known to be close to the gene and ìwalkî along the DNA until the gene is identified"
1993,A Generalization of Binary Search.,"Abstract
Let f be a nondecreasing integer-valued function whose domain is the set of integers [0., n]. The (n, m) problem is the problem of determining f at all points of its domain, given that f(0)=0 and f(n)=m. The paper [HM] determines the worst-case number of function evaluations needed to solve the (n, m) problem and gives one particular algorithm achieving the worst-case bound. We obtain the following further results concerning this problem:
A family of deterministic algorithms that minimizes the worst-case number of function evaluations needed to solve the (n, m)-problem;
A deterministic algorithm that comes within one step of minimizing the worst-case number of parallel steps required to solve the (n,m)-problem, where a given number p of concurrent function evaluations may be performed in each parallel step. This result requires that p ‚â§ m;
A deterministic algorithm that minimizes the expected number of function evaluations when the function f is drawn from a probability distribution satisfying a natural symmetry property;
A randomized algorithm that minimizes the worst-case expected number of function evaluations required to solve the (n, 1)-problem;
Lower and upper bounds on the worst-case expected number of function evaluations required by a randomized algorithm to solve the (n, m)-problem for m > 1;
All the algorithms presented in the paper are extremely simple.
The (n, m) problem is equivalent to the following natural search problem: given a table consisting of n entries in increasing order, and given keys x1 < x2 < ... < xm, determine which of the given keys lie in the table. It is easily seen that the worst-case number of table entries that must be inspected in the search problem is equal to the worst-case number of function evaluations needed to solve the (n, m) problem."
1992,Three-Stage Generalized Connectors.,"An acyclic directed network with n sources and m sinks is called a generalized connector if, for any request pattern in which each sink asks to be connected to some source, the required configuration of noninterfering connecting paths can be set up. This paper presents new families of two- and three-stage connection networks and gives a method of establishing that particular networks in these families are generalized connectors. This method is based on the Erdˆs probabilistic method and consists of two steps: 1. First it is proven that a given network is a generalized connector provided that certain events in a probability space $\Omega $ are of sufficiently low probability. 2. Then it is shown that the probabilities of these events are indeed sufficiently small.For the family of designs presented in Section 5, the second step is accomplished by explicitly calculating the probabilities of the events in question, using dynamic programming; these calculations provide rigorous proofs that the designs are correct. However, for the more economical designs of Section 6, the probabilities of the events in question are not calculated exactly, but instead are estimated by drawing pseudo-random samples from the probability space $\Omega $. Thus, we have only a ìstatistical proofî (albeit a very convincing one) of the correctness of these designs."
1992,Competitive Analysis of Financial Games.,"Abstract:
In the unidirectional conversion problem an on-line player is given the task of converting dollars to yen over some period of time. Each day, a new exchange rate is announced and the player must decide how many dollars to convert. His goal is to minimize the competitive ratio. defined as sup/sub E/ (P/sub OPT/(E)/P/sub X/E) where E ranges over exchange rate sequences. P/sub OPT/(E) is the number of yen obtained by an optimal off-line algorithm, and Px(E) is the number of yen obtained by the on-line algorithm X. The authors also consider a continuous version of the problem. in which the exchange rate varies over a continuous time interval. The on-line line players a priori information about the fluctuation of exchange rates distinguishes different variants of the problem. For three variants they show that a simple threat-based strategy is optimal for the on-line player and determine its competitive ratio. They also derive and analyze an optimal policy for the on-line player when he knows the probability distribution of the maximum value that the exchange rate will reach. Finally, they consider a bidirectional conversion problem, which the player may trade dollars for yen or yen for dollars.< >"
1992,On-Line Algorithms Versus Off-Line Algorithms: How Much is it Worth to Know the Future?,"An online algorithm is one that receives a sequence of requests and performs an immediate action in response to each request. Online algorithms arise in any situation where decisions must be made and resources allocated without knowledge of the future. The effectiveness of an online algorithm may be measured by its competitive ratio, defined as the worst-case ratio between its cost and that of a hypothetical offline algorithm which knows the entire sequence of requests in advance and chooses its actions optimally. In a variety of settings, we discuss techniques for proving upper and lower bounds on the competitive ratios achievable by online algorithms. In particular, we discuss the advantages of randomized online algorithms over deterministic ones."
1992,When is the Assignment Bound Tight for the Asymmetric Traveling Salesman Problem?,"We consider the probabilistic relationship between the value of a random asymmetric traveling salesman problem $ATSP(M)$ and the value of its assignment relaxation $AP(M)$. We assume here that the costs are given by an $n\times n$ matrix $M$ whose entries are independently and identically distributed. We focus on the relationship between $Pr(ATSP(M)=AP(M))$ and the probability $p_n$ that any particular entry is zero. If $np_n\rightarrow \infty$ with $n$ then we prove that $ATSP(M)=AP(M)$ with probability 1-o(1). This is shown to be best possible in the sense that if $np(n)\rightarrow c$, $c>0$ and constant, then $Pr(ATSP(M)=AP(M))<1-\phi(c)$ for some positive function $\phi$. Finally, if $np_n\rightarrow 0$ then $Pr(ATSP(M)=AP(M))\rightarrow 0$"
1992,Efficient PRAM Simulation on a Distributed Memory Machine.,"We present a randomized simulation of a nlog log (n) log (n)-processor shared memory machine (DMM) with optimal expected delay O(log log (n)) per step of simulation. The time bound for the delay is guaranteed with overwhelming probability. The algorithm is based on hashing and uses a novel simulation scheme. The best previous simulations use a simpler scheme based on hashing and have much larger expected delay: &THgr;(log(n)/log log (n)) for the simulation of an n-processor PRAM on an n processor DMM, and &THgr;(log(n)) in the case where the simulation preserves the processor-time product."
1991,"FFD Bin Packing for Item Sizes with Uniform Distributions on [0, 1/2].","Abstract
We study the expected behavior of the FFD bin-packing algorithm applied to items whose sizes are distributed in accordance with a Poisson process with rateN on the interval[0, 1/2] of item sizes. By viewing the algorithm as a succession of queueing processes we show that the expected wasted space for FFD bin-packing is bounded above by 11.3 bins, independent ofN."
1991,An introduction to randomized algorithms.,"Abstract
Research conducted over the past fifteen years has amply demonstrated the advantages of algorithms that make random choices in the course of their execution. This paper presents a wide variety of examples intended to illustrate the range of applications of randomized algorithms, and the general principles and approaches that are of greatest use in their construction. The examples are drawn from many areas, including number theory, algebra, graph theory, pattern matching, selection, sorting, searching, computational geometry, combinatorial enumeration, and parallel and distributed computation."
1991,Transitive Compaction in Parallel via Branchings.,"Abstract
We study the following problem: given a strongly connected digraph, find a minimal strongly connected spanning subgraph of it. Our main result is a parallel algorithm for this problem, which runs in polylog parallel time and uses O(n3) processors on a PRAM. Our algorithm is simple and the major tool it uses is computing a minimum-weight branching with zero-one weights. We also present sequential algorithms for the problem that run in time O(m + n ¬∑ log n)."
1991,Competitive Paging Algorithms.,"Abstract
The paging problem is that of deciding which pages to keep in a memory of k pages in order to minimize the number of page faults. We develop the marking algorithm, a randomized on-line algorithm for the paging problem. We prove that its expected cost on any sequence of requests is within a factor of 2Hk of optimum. (Where Hk is the kth harmonic number, which is roughly ln k.) The best such factor that can be achieved is Hk. This is in contrast to deterministic algorithms, which cannot be guaranteed to be within a factor smaller than k of optimum. An alternative to comparing an on-line algorithm with the optimum off-line algorithm is the idea of comparing it to several other on-line algorithms. We have obtained results along these lines for the paging problem. Given a set of on-line algorithms and a set of appropriate constants, we describe a way of constructing another on-line algorithm whose performance is within the appropriate constant factor of each algorithm in the set."
1991,A Graph-Theoretic Game and its Application to the k-Server Problem (Extended Abstract).,"This paper investigates a zero-sum game played on a weighted connected graph $G$ between two players, the tree player and the edge player. At each play, the tree player chooses a spanning tree $T$ and the edge player chooses an edge $e$. The payoff to the edge player is $cost(T,e)$, defined as follows: If $e$ lies in the tree $T$ then $cost(T,e)=0$; if $e$ does not lie in the tree then $cost(T,e) = cycle(T,e)/w(e)$, where $w(e)$ is the weight of edge $e$ and $cycle(T,e)$ is the weight of the unique cycle formed when edge $e$ is added to the tree $T$. The main result is that the value of the game on any $n$-vertex graph is bounded above by $\exp(O(\sqrt{\log n \log\log n}))$. It is conjectured that the value of the game is $O(\log n)$.
The game arises in connection with the $k$-server problem on a road network; i.e., a metric space that can be represented as a multigraph $G$ in which each edge $e$ represents a road of length $w(e)$. It is shown that, if the value of the game on $G$ is $Val(G,w)$, then there is a randomized strategy that achieves a competitive ratio of $k(1 + Val(G,w))$ against any oblivious adversary. Thus, on any $n$-vertex road network, there is a randomized algorithm for the $k$-server problem that is $k\cdot\exp(O(\sqrt{\log n \log\log n}))$ competitive against oblivious adversaries.
At the heart of the analysis of the game is an algorithm that provides an approximate solution for the simple network design problem. Specifically, for any $n$-vertex weighted, connected multigraph, the algorithm constructs a spanning tree $T$ such that the average, over all edges $e$, of $cost(T,e)$ is less than or equal to $\exp(O(\sqrt{\log n \log\log n}))$. This result has potential application to the design of communication networks. It also improves substantially known estimates concerning the existence of a sparse basis for the cycle space of a graph."
1991,Probabilistic Recurrence Relations.,"This paper is concerned with recurrence relations that arise frequently in the analysis of divide and conquer algorithms. In order to solve a problem instance of size x, such as an algorithm invests an amount of work a(x) to break the problem into subproblems of sizes , and then proceeds to solve the subproblems. Our particular interest is in the case where the sizes are random variables; this may occur either because of randomization within the algorithm or because the instances to be solved are assumed to be drawn from a probability distribution. When the are random variables the running time of the algorithm on instances of size x is also a random variable T(x). We give several easy-to-apply methods for obtaining fairly tight bounds on the upper tails of the probability distribution of T(x), and present a number of typical applications of these bounds to the analysis of algorithms. The proofs of the bounds are based on an interesting analysis of optimal strategies in certain gambling games."
1990,Subtree isomorphism is in random NC.,"Abstract
Given two trees, a guest tree G and a host tree H, the subtree isomorphism problem is to determine whether there is a subgraph of H that is isomorphic to G. We present a randomized parallel algorithm for finding such an isomorphism, if it exists. The algorithm runs in time O(log3n) on a CREW PRAM, where n is the number of nodes in H. The number of processors required by the algorithm is polynomial in n. Randomization is used (solely) to solve each of a series of bipartite matching problems during the course of the algorithm. We demonstrate the close connection between the two problems by presenting a log-space reduction from bipartite perfect matching to subtree isomorphism. Finally, we present some techniques to reduce the number of processors used by the algorithm."
1990,The Transitive Closure of a Random Digraph.,"In a random n?vertex digraph, each arc is present with probability p, independently of the presence or absence of other arcs. We investigate the structure of the strong components of a random digraph and present an algorithm for the construction of the transitive closure of a random digraph. We show that, when n is large and np is equal to a constant c greater than 1, it is very likely that all but one of the strong components are very small, and that the unique large strong component contains about ?2n vertices, where ? is the unique root in [0, 1] of the equation 1 ? x ? e?ex = 0. Nearly all the vertices outside the large strong component line in strong components of size 1. Provided that the expected degree of a vertex is bounded away from 1, our transitive closure algorithm runs in expected time O(n). for all choices of n and p, the expected execution time of the algorithm is O(w(n) (n log n)4/3), where w(n) is an arbitrary nondecreasing unbounded function. To circumvent the fact that the size of the transitive closure may be ?(n2) the algorithm presents the transitive closure in the compact form (A ◊ B)U C, where A and B are sets of vertices, and C is a set of arcs.
"
1990,An Optimal Algorithm for On-line Bipartite Matching.,"There has been a great deal of interest recently in the relative power of on-line and off-line algorithms. An on-line algorithm receives a sequence of requests and must respond to each request as soon as it is receiveD. An off-line algorithm may wait until all requests have been received before determining its responses. One approach to evaluating an on-line algorithm is to compare its performance with that of the best possible off-line algorithm for the same problem. Thus, given a measure of ""profit"", the performance of an on-line algorithm can be measured by the worst-case ratio of its profit to that of the optimal off-line algorithm. This general approach has been applied in a number of contexts, including data structures [SITa], bin packing [CoGaJo], graph coloring [GyLe] and the k-server problem [MaMcSI]. Here we apply it to bipartite matching and show that a simple randomized on-line algorithm achieves the best possible performance."
1990,On the Power of Randomization in Online Algorithms (Extended Abstract).,"Against an adaptive adversary, we show that the power of randomization in on-line algorithms is severely limited! We prove the existence of an efficient ìsimulationî of randomized on-line algorithms by deterministic ones, which is best possible in general. The proof of the upper bound is existential. We deal with the issue of computing the efficient deterministic algorithm, and show that this is possible in very general cases.†"
1989,Monte-Carlo Approximation Algorithms for Enumeration Problems.,"Abstract
We develop polynomial time Monte-Carlo algorithms which produce good approximate solutions to enumeration problems for which it is known that the computation of the exact solution is very hard. We start by developing a Monte-Carlo approximation algorithm for the DNF counting problem, which is the problem of counting the number of satisfying truth assignments to a formula in disjunctive normal form. The input to the algorithm is the formula and two parameters Œµ and Œ¥. The algorithm produces an estimate which is between 1 ‚àí œµ and 1 + œµ times the number of satisfying truth assignments with probability at least 1 ‚àí Œ¥. The running time of the algorithm is linear in the length of the formula times
times
. On the other hand, the problem of computing the exact answer for the DNF counting problem is known to be #P-complete, which implies that there is no polynomial time algorithm for the exact solution if P ‚â† NP. This paper improves and gives new applications of some of the work previously reported. Variants of an œµ, Œ¥ approximation algorithm for the DNF counting problem have been highly tailored to be especially efficient for the network reliability problems to which they are applied. In this paper the emphasis is on the development and analysis of a much more efficient œµ, Œ¥ approximation algorithm for the DNF counting problem. The running time of the algorithm presented here substantially improves the running time of versions of this algorithm given previously. We give a new application of the algorithm to a problem which is relevant to physical chemistry and statistical physics. The resulting œµ, Œ¥ approximation algorithm is substantially faster than the fastest known deterministic solution for the problem."
1989,Failure Correction Techniques for Large Disk Arrays.,"The ever increasing need for I/O bandwidth will be met with ever larger arrays of disks. These arrays require redundancy to protect against data loss. This paper examines alternative choices for encodings, or codes, that reliably store information in disk arrays. Codes are selected to maximize mean time to data loss or minimize disks containing redundant data, but are all constrained to minimize performance penalties associated with updating information or recovering from catastrophic disk failures. We also codes that give highly reliable data storage with low redundant data overhead for arrays of 1000 information disks."
1989,On Parallel Evaluation of Game Trees.,"We present parallel algorithms for evaluating game trees. These algorithms parallelize the ""left-to-right"" sequential algorithm for evaluating AND/OR trees and the alpha-beta pruningn procedure for evaluating MIN/MAX trees. We show that, on every instance of a uniform tree, these parallel algorithms achieve a linear speed-up over their corresponding sequential algorithms, if number of processors used is close to the height of the input tree. These are the first non-trivial deterministic speed-up bounds known for the ""left-to-right"" algorithm and the alpha-beta pruning procedure."
1988,The Complexity of Parallel Search.,"This paper studies parallel search algorithms within the framework of independence systems. It is motivated by earlier work on parallel algorithms for concrete problems such as the determination of a maximal independent set of vertices or a maximum matching in a graph and by the general question of determining the parallel complexity of a search problem when an oracle is available to solve the associated decision problem. Our results provide a parallel analog of the self-reducibility process that is so useful in sequential computation. An abstract independence system is specified by a ground set†E†and a family of subsets of†E†called the independent sets; it is required that every subset of an independent set be independent. We investigate parallel algorithms for determining a maximal independent set through oracle queries of the form ìIs the set†A†independent?,î as well as parallel algorithms for determining a maximum independent set through queries to a more powerful oracle called a rank oracle. We also study these problems for three special types of independence systems: matroids, graphic matroids, and partition matroids. We derive lower and upper bounds on the deterministic and randomized complexity of these problems. These bounds are sharp enough to give a clear picture of the processor-time trade-offs that are possible, to establish that randomized parallel algorithms are much more powerful than deterministic ones, and to show that even randomized algorithms cannot make effective use of extremely large numbers of processors."
1988,Deferred Data Structuring.,"We consider the problem of answering a series of on-line queries on a static data set. The conventional approach to such problems involves a preprocessing phase which constructs a data structure with good search behavior. The data structure representing the data set then remains fixed throughout the processing of the queries. Our approach involves dynamic or query-driven structuring of the data set; our algorithm processes the data set only when doing so is required for answering a query. A data structure constructed progressively in this fashion is called a deferred data structure.

We develop the notion of deferred data structures by solving the problem of answering membership queries on an ordered set. We obtain a randomized algorithm which achieves asymptotically optimal performance with high probability. We then present optimal deferred data structures for the following problems in the plane: testing convex-hull membership, half-plane intersection queries and fixed-constraint multi-objective linear programming. We also apply the deferred data structuring technique to multi-dimensional dominance query problems.
"
1988,Subtree Isomorphism is in Random NC.,"Abstract
Given two trees, a guest tree G and a host tree H, the subtree isomorphism problem is to determine whether there is a subgraph of H that is isomorphic to G. We present a randomized parallel algorithm for finding such an isomorphism, if it exists. The algorithm runs in time O(log3 n) on a CREW PRAM, where n is the number of nodes in H. Randomization is used (solely) to solve each of a series of bipartite matching problems during the course of the algorithm. We demonstrate the close connection between the two problems by presenting a log space reduction from bipartite perfect matching to subtree isomorphism. Finally, we present some techniques to reduce the number of processors used by the algorithm."
1988,A Randomized Parallel Branch-and-Bound Procedure.,"We present a universal randomized method called Local Best-First Search for parallelizing sequential branch-and-bound algorithms. The method executes on a message-passing multiprocessor system, and requires no global data structures or complex communication protocols. We show that, uniformly on all instances, the execution time of the method is unlikely to exceed a certain inherent lower bound by more than a constant factor."
1987,Global Wire Routing in Two-Dimensional Arrays.,"Abstract
We examine the problem of routing wires of a VLSI chip, where the pins to be connected are arranged in a regular rectangular array. We obtain tight bounds for the worst-case ‚Äúchannel-width‚Äù needed to route ann√ón array, and develop provably good heuristics for the general case. Single-turn routings are proved to be near-optimal in the worst-case.
A central result of our paper is a ‚Äúrounding algorithm‚Äù for obtaining integral approximations to solutions of linear equations. Given a matrix A and a real vector x, then we can find an integral x such that for alli, ¬¶x i -x i ¬¶ <1 and (Ax) i -(Ax) i <Œî. Our error bound Œî is defined in terms of sign-segregated column sums of A:
Œî=
max
j
(max{
‚àë
i:
a
ij
>0
a
ij
,
‚àë
i:
a
ij
<0
‚àí
a
ij
}).
Œî=maxj‚Å°(max{‚àëi:aij>0aij,‚àëi:aij<0‚àíaij})."
1987,Efficient Randomized Pattern-Matching Algorithms.,"Abstract:
We present randomized algorithms to solve the following string-matching problem and some of its generalizations: Given a string X of length n (the pattern) and a string Y (the text), find the first occurrence of X as a consecutive block within Y. The algorithms represent strings of length n by much shorter strings called fingerprints, and achieve their efficiency by manipulating fingerprints instead of longer strings. The algorithms require a constant number of storage locations, and essentially run in real time. They are conceptually simple and easy to implement. The method readily generalizes to higher-dimensional pattern-matching problems."
1987,"A simplex variant solving an m times d linear program in O(min(m2, d2) expected number of pivot steps.","Abstract
We present a variant of the Simplex method which requires on the average at most 2 (min(m, d) + 1)2 pivots to solve the linear program min cT, Ax ‚â• b, x ‚â• 0 with A Œµ Rm√ód. The underlying probabilistic distribution is assumed to be invariant under inverting the sense of any subset of the inequalities. In particular, this implies that under Smale's spherically symmetric model this variant requires an average of no more than 2(d + 1)2 pivots, independent of m, where d ‚â§ m."
1986,"Combinatorics, Complexity, and Randomness.",The 1985 Turing Award winner presents his perspective on the development of the field that has come to be called theoretical computer science.
1986,Constructing a perfect matching is in random NC.,"Abstract
We show that the problem of constructing a perfect matching in a graph is in the complexity class Random NC; i.e., the problem is solvable in polylog time by a randomized parallel algorithm using a polynomial-bounded number of processors. We also show that several related problems lie in Random NC. These include:
(i)
Constructing a perfect matching of maximum weight in a graph whose edge weights are given in unary notation;
 (ii)
Constructing a maximum-cardinality matching;
 (iii)
Constructing a matching covering a set of vertices of maximum weight in a graph whose vertex weights are given in binary;
 (iv)
Constructing a maximums-t flow in a directed graph whose edge weights are given in unary.
 "
1986,A Family of Simplex Variants Solving an m √ó d Linear Program in Expected Number of Pivot Steps Depending on d Only.,"We present a family of variants of the Simplex method, which are based on a Constraint-By-Constraint procedure: the solution to a linear program is obtained by solving a sequence of subproblems with an increasing number of constraints. We discuss several probabilistic models for generating linear programs. In all of them the underlying distribution is assumed to be invariant under changing the signs of rows or columns in the problem data. A weak regularity condition is also assumed. Under these models, for linear programs with d variables and m + d inequality constraints, the expected number of pivots required by these algorithms is bounded by a function of min(m, d) only. In particular this means that, for a fixed number of variables, the expected number of pivots is bounded by a constant when the number of constraints tends to infinity. Since Smale's original model [Smale, S. 1983. On the average speed of the simplex method of linear programming. Math. Programming27.] satisfies our probabilistic assumptions, the same results apply to his model, although not to the particular algorithm he analyzes. We also present some results for models generating only feasible linear programs, and for Bland's pivoting rule. We conclude with a discussion of our probabilistic models, and show why they are inadequate for obtaining meaningful results unless d and m are of the same order of magnitude.
"
1986,On a Search Problem Related to Branch-and-Bound Procedures.,n/a
1986,"FFD Bin Packing for Item Sizes with Distributions on [0,1/2].","Abstract:
We study the expected behavior of the FFD binpacking algorithm applied to items whose sizes are distributed in accordance with a Poisson process with rate N on the interval [0,1/2] of item sizes. By viewing the algorithm as a succession of queueing processes we show that the expected wasted space for FFD bin-packing is bounded above by 9.4 bins, independent of N. We extend this upper bound to a FFD bin-packing of items in accordance with a non-homogeneous Poisson process with a nonincreasing intensity function Œª(t) on [0,1/2]."
1985,A Fast Parallel Algorithm for the Maximal Independent Set Problem.,"A parallel algorithm is presented that accepts as input a graph G and produces a maximal independent set of vertices in G. On a P-RAM without the concurrent write or concurrent read features, the algorithm executes in O((log n)4) time and uses O((n/(log n))3) processors, where n is the number of vertices in G. The algorithm has several novel features that may find other applications. These include the use of balanced incomplete block designs to replace random sampling by deterministic sampling, and the use of a ‚Äúdynamic pigeonhole principle‚Äù that generalizes the conventional pigeonhole principle."
1985,Monte-Carlo algorithms for the planar multiterminal network reliability problem.,"Abstract
This paper presents a general framework for the construction of Monte-Carlo algorithms for the solution of enumeration problems. As an application of the general framework, a Monte-Carlo method is constructed for estimating the failure probability of a multiterminal planar network whose edges are subject to independent random failures. The method is guaranteed to be effective when the failure probabilities of the edges are sufficiently small."
1985,The Complexity of Parallel Computation on Matroids.,"Abstract:
In [KUW1] we have proposed the setting of independence systems to study the relation between the computational complexity of search and decision problems. The universal problem that captures this relation, which we termed the S-search problem, is: ""Given an oracle for the input system, find a maximal independent subset in it"". Many interesting and important search problems can be described by a special class of independence systems, called matroids. This paper is devoted to die complexity of the S- search problem for matroids. Our main result is a lower bound on any probabilistic algorithm for the S-search problem that acquires information about the input system by interrogating an independence oracle. We prove that the expected time of any such probabilistic algorithm that uses a sub-exponential number of processors is Œ©(n1/3-Œµ). This is one of the first nontrivial, super-logarithmic lower bounds on a randomized parallel computation. It implies that in our model of computation Random-NC is strictly contained in P. Another consequence of the lower bound is that the O(‚àön) time probabilistic upper bound for arbitrary independence systems, presented in [KUW1], is close to optimal and cannot be significantly improved, even for matroids. However, fills O(‚àön) upper bound can be improved in a different sense for matroids -it can be made deterministic, still with polynomially many processors. Finally, we show that the lower bound can be beaten for the special case of graphic matroids. Here, the S-search problem is simply to find a spanning forest of a graph, when the algorithm cannot see the graph, but can only ask whether subsets of edges are forests or not. We give an O(logn) time deterministic parallel algoritlun that uses nO(logn) processors. From the upper bounds on parallel time above we deduce similar bounds (up to a poly-log factor) on thc sequential space required by a deterministic Turing machine with an independence oracle to solve the S-search problem.
(View more)"
1985,Constructing a Perfect Matching is in Random NC.,"We show that the problem of constructing a perfect matching in a graph is in the complexity class Random NC: i.e., the problem is solvable in polylog time by a randomized parallel algorithm using a polynomial-bounded number of processors. We also show that several related problems lie in Random NC. These include: (i) Constructing a perfect matching of maximum weight in a graph whose edge weights are given in unary notation; (ii) Constructing a maximum-cardinality matching; (iii) Constructing a matching covering a set of vertices of maximum weight in a graph whose vertex weights are given in binary; (iv) Constructing a maximum s-t flow in a directed graph whose edge weights are given in unary."
1985,Are Search and Decision Problems Computationally Equivalent?,"From the point of view of sequential polynomial time computation, the answer to the question in the title is 'yes'. The process of self-reducibility is a linear time Turing (oracle) reduction from a given combinatorial search problem to an appropriately defined decision problem. However, from the point of view of fast parallel computation, the answer is not so clear. Many of the sequential algorithms that were 'marked off' as being ‚Äúinherently sequential‚Äù embed within them the self-reducibility process. Can this inherently sequential process be parallelized? To study this problem, we define an abstract setting (namely that of an independence system) in which one, universal search problem captures all combinatorial search problems. We consider several natural decision and function oracles to which this search problem may be reduced. On the positive side, we give efficient probabilistic parallel reductions to these oracles. These reductions constitute a scheme for parallelizing search problems in case the oracles for these problems are themselves efficiently computable in parallel. We give examples of problems that did not yield to parallelism before, but can be parallelized using this scheme. On the negative side, we prove lower bounds on any determininistic parallel reductions to the same oracles. If p processors are used, the sequential (linear) running time cannot be enhanced by more than a factor of &Ogr;(log p) and hence for any polynomial number of processors the problem remains inherently sequential. This proves that randomization can be exponentially more powerful than determinism in our model, and suggests that NC ‚â† Random NC. Finally, we state some intriguing conjectures and suggest new directions of research in complexity theory that arise from this work."
1984,A Fast Parallel Algorithm for the Maximal Independent Set Problem.,"A parallel algorithm is presented which accepts as input a graph G and produces a maximal independent set of vertices in G. On a P-RAM without the concurrent write or concurrent read features, the algorithm executes in O((log n)4) time and uses O((n/log n)3) processors, where n is the number of vertices in G. The algorithm has several novel features that may find other applications. These include the use of balanced incomplete block designs to replace random sampling by deterministic sampling, and the use of a ‚Äúdynamic pigeonhole principle‚Äù that generalizes the conventional pigeonhole principle."
1984,A Probabilistic Analysis of Multidimensional Bin Packing Problems.,"This paper gives probabilistic analyses of two kinds of multidimensional bin packing problems: vector packing and rectangle packing. In the vector packing problem each of the d dimensions can be interpreted as a resource. A given object i consumes aij units of the jth resource, and the objects packed in any given bin may not collectively consume more than one unit of any resource. Subject to this constraint, the objects are to be packed into a minimum number of bins. The rectangle packing problem is more geometric in character. The ith object is a d-dimensional box whose jth side is of length aij, and the goal is to pack the objects into a minimum number of cubical boxes of side 1. We study these problems on the assumption that the aij are drawn independently from the uniform distribution over [0,1]. We study a vector packing heuristic called VPACK that tries to place two objects in each bin and a rectangle packing heuristic called RPACK that tries to pack one object into each of the 2d corners of each bin. We show that each of these heuristics tends to produce packings in which very little of the capacity of the bins is wasted. In the case of rectangle packing, we show that the results can be extended to a wide class of distributions of the piece sizes."
1983,Searching for an Optimal Path in a Tree with Random Costs.,"Abstract
We consider the problem of finding an optimal path leading from the root of a tree to any of its leaves. The tree is known to be uniform, binary, and of height N, and each branch independently may have a cost of 1 or 0 with probability p and 1‚àíp, respectively.
We show that for p<1/2 the uniform cost algorithm can find a cheapest path in linear expected time. By contrast, when p>1/2, every algorithm which guarantees finding an exact cheapest path, or even a path within a fixed cost ratio of the cheapest, must run in exponential average time. If, however, we are willing to accept a near optimal solution almost always, then a pruning algorithm exists which finds such a solution in linear expected time. The algorithm employs a depth-first strategy which stops at regular intervals to appraise its progress and, if the progress does not meet a criterion based on domain-specific knowledge, the current node is irrevocably pruned."
1983,Monte-Carlo Algorithms for Enumeration and Reliability Problems.,"1. Introduction We present a simple but very general Monte-Carlo technique for the approximate solution of enumeration and reliability problems. Several applications are given, including: 1. Estimating the number of triangulated plane maps with a given number of ver-tices; 2. Estimating the cardinality of a union of sets; 3. Estimating the number of input combinations for which a boolean function, presented in disjunctive normal form,†"
1983,Global Wire Routing in Two-Dimensional Arrays (Extended Abstract).,"Abstract:
We examine the problem of routing wires on a VLSI chip, where the pins to be connected are arranged in a regular rectangular array. We obtain tight bounds for the worst-case ""channel-width"" needed to route an n √ó n array, and develop provably good heuristics for the general case. An interesting ""rounding algorithm"" for obtaining integral approximations to solutions of linear equations is used to show the near-optimality of single-turn routings in the worst-case."
1982,On the Security of Ping-Pong Protocols.,"Consider the class of protocols, for two participants, in which the initiator applies a sequence of operators to a message M and sends it to the other participant; in each step, one of the participants applies a sequence of operators to the message received last, and sends it back. This ‚Äúping-pong‚âì action continues several times, using sequences of operators as specified by the protocol. The set of operators may include public-key encryptions and decryptions. An O(n3) algorithm which determines the security of a given protocol (of length n) is presented. This is an improvement of the algorithm of Dolev and Yao (IEEE Trans. Inform. Theory IT-30 (2) (1983), 198208)."
1982,Dynamic programming meets the principle of inclusion and exclusion.,"Abstract
We show that several combinatorial existence problems can be attacked by solving associated enumeration problems using a combination of dynamic programming and the principle of inclusion and exclusion. In comparison with the usual dynamic programming algorithms, the new methods have somewhat greater execution time but require far less storage."
1982,On Linear Characterizations of Combinatorial Optimization Problems.,"We show that there can be no computationally tractable description by linear inequalities of the polyhedron associated with any NP-complete combinatorial optimization problem unless NP = co-NPóa very unlikely event. We also apply the ellipsoid method for linear programming to show that a combinatorial optimization problem is solvable in polynomial time if and only if it admits a small generator of violated inequalities.
"
1982,On the Security of Ping-Pong Protocols.,"Abstract
Consider the class of protocols, for two participants, in which the initiator applies a sequence of operators to a message M and sends it to the other participant; in each step, one of the participants applies a sequence of operators to the message received last, and sends it back. This ‚Äúping-pong‚Äù action continues several times, using sequences of operators as specified by the protocol. The set of operators may include public-key encryptions and decryptions.
We present an O(n3) algorithm which determines the security of a given protocol (of length n). This is an improvement of the algorithm of Dolev and Yao [DY]."
1982,An Efficient Approximation Scheme for the One-Dimensional Bin-Packing Problem.,"Abstract:
We present several polynomial-time approximation algorithms for the one-dimensional bin-packing problem. using a subroutine to solve a certain linear programming relaxation of the problem. Our main results are as follows: There is a polynomial-time algorithm A such that A(I) ‚â§ OPT(I) + O(log2 OPT(I)). There is a polynomial-time algorithm A such that, if m(I) denotes the number of distinct sizes of pieces occurring in instance I, then A(I) ‚â§ OPT(I) + O(log2 m(I)). There is an approximation scheme which accepts as input an instance I and a positive real number Œµ, and produces as output a packing using as most (1 + Œµ) OPT(I) + O(Œµ-2) bins. Its execution time is O(Œµ-c n log n), where c is a constant. These are the best asymptotic performance bounds that have been achieved to date for polynomial-time bin-packing. Each of our algorithms makes at most O(log n) calls on the LP relaxation subroutine and takes at most O(n log n) time for other operations. The LP relaxation of bin packing was solved efficiently in practice by Gilmore and Gomory. We prove its membership in P, despite the fact that it has an astronomically large number of variables."
1981,Parametric shortest path algorithms with an application to cyclic staffing.,"Abstract
Let G=(V, E) be a digraph with n vertices including a special vertex s. Let E‚Ä≤ ‚äÜ E be a designated subset of edges. For each e œµ E there is an associated real number
∆í
. Furthermore, let
∆í
if e œµ E‚Ä≤ and
∆í
if e œµ E ‚àí E‚Ä≤. The length of edge e is
∆í
∆í
, where Œª is a parameter that takes on real values. Thus the length varies additively in Œª for each edge of E‚Ä≤.
We shall present two algorithms for computing the shortest path from s to each vertex œÖ œµ V parametrically in the parameter Œª, with respective running times O(n3) and O(n|E|log n). For dense digraphs the running time of the former algorithm is comparable to the fastest (non-parametric) shortest path algorithm known.
This work generalizes the results of Karp [2] concerning the minimum cycle mean of a digraph, which reduces to the case that E‚Ä≤=E. Furthermore, the second parametric algorithm may be used in conjunction with a transformation given by Bartholdi, Orlin, and Ratliff [1] to give an O(n2 log n) algorithm for the cyclic staffing problem."
1981,The Complexity of Testing Whether a Graph is a Superconcentrator.,n/a
1981,Maximum Matchings in Sparse Random Graphs.,
1980,Linear Expected-Time Algorithms for Connectivity Problems.,"Abstract
This paper describes fast average-time algorithms for four graph connectivity problems. Algorithms that run in O(n) average time on n-vertex graphs are developed for finding connected components, strong components, and blocks. An O(m)-time algorithm to find a minimum spanning forest in an m-graph is also presented. The analysis of these algorithms uses the random graph model of Erd√∂s and Renyi. All the algorithms are optimum to within a constant factor."
1980,An algorithm to solve the m √ó n assignment problem in expected time O(mn log n).,"We give an algorithm to solve the m?source, n?destination assignment problem in expected time O(mn log n) under the assumption that the edge costs are independent random variables and the costs of the edges incident with any given source are identically distributed. The algorithm achieves its efficiency through an unusual application of priority queues.
"
1980,On Linear Characterizations of Combinatorial Optimization Problems.,"Abstract:
We show that there can be no computationally tractable description by linear inequalities of the polyhedron associated with any NP-complete combinatorial optimization problem unless NP = co-NP -- a very unlikely event. We also apply the ellipsoid method for linear programming to show that a combinatorial optimization problem is solvable in polynomial time if and only if it admits a small generator of violated inequalities."
1980,Some Connections between Nonuniform and Uniform Complexity Classes.,"It is well known that every set in P has small circuits [13]. Adleman [1] has recently proved the stronger result that every set accepted in polynomial time by a randomized Turing machine has small circuits. Both these results are typical of the known relationships between uniform and nonuniform complexity bounds. They obtain a nonuniform upper bound as a consequence of a uniform upper bound. The central theme here is an attempt to explore the converse direction. That is, we wish to understand when nonuniform upper bounds can be used to obtain uniform upper bounds. In this section we will define our basic notion of nonuniform complexity. Then we will show how to relate it to more common notions."
1980,Linear Expected-Time Algorithms for Connectivity Problems (Extended Abstract).,"Researchers in recent years have developed many graph algorithms that are fast in the worst case, but little work has been done on graph algorithms that are fast on the average. (Exceptions include the work of Angluin and Valiant [1], Karp [7], and Schnorr [9].) In this paper we analyze the expected running time of four algorithms for solving graph connectivity problems. Our goal is to exhibit algorithms whose expected time is within a constant factor of optimum and to shed light on the properties of random graphs. In Section 2 we develop and analyze a simple algorithm that finds the connected components of an undirected graph with n vertices in O(n) expected time. In Sections 3 and 4 we describe algorithms for finding the strong components of a directed graph and the blocks of an undirected graph in O(n) expected time. The time required for these three problems is Œ©(m) in the worst case, where m is the number of edges in the graph, since all edges must be examined; but our results show that only O(n) edges must be examined on the average.*@@@@ In Section 5 we present an algorithm for finding a minimum weight spanning forest in an undirected graph with edge weights in O(m) expected time."
1979,A Patching Algorithm for the Nonsymmetric Traveling-Salesman Problem.,"We present an algorithm for the approximate solution of the nonsymmetric n-city traveling-salesman problem. An instance of this problem is specified by a $n \times n$ distance matrix $D = (d_{ij} )$. The algorithm first solves the assignment problem for the matrix D, and then patches the cycles of the optimum assignment together to form a tour. The execution time of the algorithm is comparable to the time required to solve an $n \times n$ assignment problem.

If the distances $d_{ij} $ are drawn independently from a uniform distribution then, with probability tending to 1, the ratio of the cost of the tour produced by the algorithm to the cost of an optimum tour is $ < 1 + \varepsilon (n)$, where $\varepsilon (n)$ goes to zero as $n \to \infty $. Hence the method tends to give nearly optimal solutions when the number of cities is extremely large.
"
1979,"Random Walks, Universal Traversal Sequences, and the Complexity of Maze Problems.",
1979,Recent Advances in the Probabilistic Analysis of Graph-Theoretic Algorithms (Abstract).,n/a
1978,A characterization of the minimum cycle mean in a digraph.,"Abstract
Let C = (V,E) be a digraph with n vertices. Let f be a function from E into the real numbers, associating with each edge e ‚àà E a weight
∆í
. Given any sequence of edges œÉ = e1,e2,‚Ä¶,ep define w(œÉ), the weight of œÉ, as
∆í
, and define m(œÉ), the mean weight of œÉ, as w(œÉ)‚ß∏p. Let
where C ranges over all directed cycles in G; Œª‚àó is called the minimum cycle mean. We give a simple characterization of Œª‚àó, as well as an algorithm for computing it efficiently."
1977,Probabilistic Analysis of Partitioning Algorithms for the Traveling-Salesman Problem in the Plane.,"We consider partitioning algorithms for the approximate solution of large instances of the traveling-salesman problem in the plane. These algorithms subdivide the set of cities into small groups, construct an optimum tour through each group, and then patch the subtours together to form a tour through all the cities. If the number of cities in the problem is n, and the number of cities in each group is t, then the worst-case error is . If the cities are randomly distributed, then the relative error is O(t?1/2) (with probability one). Hybrid schemes are suggested, in which partitioning is used in conjunction with existing heuristic algorithms. These hybrid schemes may be expected to give near-optimum solutions to problems with thousands of cities.
"
1975,Two special cases of the assignment problem.,"Abstract
The assignment problem may be stated as follows: Given finite sets of points S and T, with|S| ‚©æ |T|, and given a ‚Äúmetric‚Äù which assigns a distance d(x, y) to each pair (x, y) such that x ‚àà T and y ‚àà S find a 1‚àí1 function Q: T‚Üí S which minimizes Œ£x‚ààTd(x, Q(x)) We consider the two special cases in which the points lie (1) on a line segment and (2) on a circle, and the metric is the distance along the line segment or circle, respectively. In each case, we show that the optimal assignment Q can be computed in a number of steps (additions and comparisons) proportional to the number of points. The problem arose in connection with the efficient rearrangement of desks located in offices along a corridor which encircles one floor of a building."
1975,Near-Optimal Solutions to a 2-Dimensional Placement Problem.,"We consider the problem of placing records in a 2-dimensional storage array so that expected distance between consecutive references is minimized. A simple placement heuristic which uses only relative frequency of access for different records is shown to be within an additive constant of optimal when distance is measured by the Euclidean metric. For the rectilinear and maximum metrics, we show that there is no such heuristic. For the special case in which all access probabilities are equal, however, heuristics within an additive constant of optimal do exist, and their implementation requires solution of differential equations for which we give numerical solutions.
"
1973,An n5/2 Algorithm for Maximum Matchings in Bipartite Graphs.,"The present paper shows how to construct a maximum matching in a bipartite graph with n vertices and m edges in a number of computation steps proportional to $(m + n)\sqrt n $.



"
1972,Theoretical Improvements in Algorithmic Efficiency for Network Flow Problems.,"This paper presents new algorithms for the maximum flow problem, the Hitchcock transportation problem, and the general minimum-cost flow problem. Upper bounds on the numbers of steps in these algorithms are derived, and are shown to compare favourably with upper bounds on the numbers of steps required by earlier algorithms.First, the paper states the maximum flow problem, gives the Ford-Fulkerson labelling method for its solution, and points out that an improper choice of flow augmenting paths can lead to severe computational difficulties. Then rules of choice that avoid these difficulties are given. We show that, if each flow augmentation is made along an augmenting path having a minimum number of arcs, then a maximum flow in an n-node network will be obtained after no more than augmentations; and then we show that if each flow change is chosen to produce a maximum increase in the flow value then, provided the capacities are integral, a maximum flow will be determined within at most augmentations, where is the value of the maximum flow and M is the maximum number of arcs across a cut. Next a new algorithm is given for the minimum-cost flow problem, in which all shortest-path computations are performed on networks with all weights nonnegative. In particular, this algorithm solves the n x n assignment problem in steps. Following that we explore a ""scaling"" tecnique for solving a minimum-cost flow problem by treating a sequence of derived problems with ""scaled down"" capacities. It is shown that, using this technique, the solution of a Hitchcock transportation problem with m sources and n sinks, and maximum flow B, requires at most flow augmentations. Similar results are also given for the general minimum-cost flow problem.  "
1972,A Phenomenon in the Theory of Sorting.,n/a
1972,Reducibility Among Combinatorial Problems.,"Abstract
A large class of computational problems involve the determination of properties of graphs, digraphs, integers, arrays of integers, finite families of finite sets, boolean formulas and elements of other countable domains. Through simple encodings from such domains into the set of words over a finite alphabet these problems can be converted into language recognition problems, and we can inquire into their computational complexity. It is reasonable to consider such a problem satisfactorily solved when an algorithm for its solution is found which terminates within a number of steps bounded by a polynomial in the length of the input. We show that a large number of classic unsolved problems of covering, matching, packing, routing, assignment and sequencing are equivalent, in the sense that either each of them possesses a polynomial-bounded algorithm or none of them does."
1972,"Rapid Identification of Repeated Patterns in Strings, Trees and Arrays.","In this paper we look at a number of matching problems and devise general techniques for attacking such problems. In particular, we describe a strategy for constructing efficient algorithms for solving two types of matching problems. We use this strategy to develop explicit algorithms for these two problems applied to strings (where the patterns are substrings) and arrays (where the patterns are subarrays or blocks). We also develop algorithms for these and related problems for trees, where the patterns are subtrees. Certain special cases of these algorithms are also discussed. Although we do not claim that these algorithms are optimal, we analyze each algorithm to estimate its computational cost. This provides some basis for choosing which algorithm is most desirable in any given situation."
1971,The traveling-salesman problem and minimum spanning trees: Part II.,"Abstract
The relationship between the symmetric traveling-salesman problem and the minimum spanning tree problem yields a sharp lower bound on the cost of an optimum tour. An efficient iterative method for approximating this bound closely from below is presented. A branch-and-bound procedure based upon these considerations has easily produced proven optimum solutions to all traveling-salesman problems presented to it, ranging in size up to sixty-four cities. The bounds used are so sharp that the search trees are minuscule compared to those normally encountered in combinatorial problems of this type."
1971,A simple derivation of Edmonds' algorithm for optimum branchings.,"Edmonds [1] has given an algorithm for constructing a maximum?weight branching in a weighted directed graph. His proof that the algorithm is correct is based on linear programming theory, and establishes as a by?product that a certain polyhedron has integer vertices. Here we give a direct combinatorial proof of the correctness of the algorithm.
"
1971,A n^5/2 Algorithm for Maximum Matchings in Bipartite Graphs.,"Abstract:
The present paper shows how to construct a maximum matching in a bipartite graph with n vertices and m edges in a number of computation steps proportional to (m+n) n."
1970,The Traveling-Salesman Problem and Minimum Spanning Trees.,"This paper explores new approaches to the symmetric traveling-salesman problem in which 1-trees, which are a slight variant of spanning trees, play an essential role. A 1-tree is a tree together with an additional vertex connected to the tree by two edges. We observe that (i) a tour is precisely a 1-tree in which each vertex has degree 2, (ii) a minimum 1-tree is easy to compute, and (iii) the transformation on ìintercity distancesî cij ? Cij + ?i + ?j leaves the traveling-salesman problem invariant but changes the minimum 1-tree. Using these observations, we define an infinite family of lower bounds w(?) on C*, the cost of an optimum tour. We show that max?w(?) = C* precisely when a certain well-known linear program has an optimal solution in integers. We give a column-generation method and an ascent method for computing max?w(?), and construct a branch-and-bound method in which the lower bounds w(?) control the search for an optimum tour.
"
1970,A Phenomenon in the Theory of Sorting.,n/a
1969,Parallel Program Schemata.,"Abstract
This paper introduces a model called the parallel program schema for the representation and study of programs containing parallel sequencing. The model is related to Ianov's program schema, but extends it, both by modelling memory structure in more detail and by admitting parallel computation. The emphasis is on decision procedures, both for traditional properties, such as equivalence, and for new properties particular to parallel computation, such as determinacy and boundedness."
1967,Some Bounds on the Storage Requirements of Sequential Machines and Turing Machines.,"Any sequential machine M represents a function fM from input sequences to output symbols. A function f is representable if some finite-state sequential machine represents it. The function fM is called an n-th order approximation to a given function f if fM is equal to f for all input sequences of length less than or equal to n. It is proved that, for an arbitrary nonrepresentable function f, there are infinitely many n such that any sequential machine representing an nth order approximation to f has more than n/2 + 1 states. An analogous result is obtained for two-way sequential machines and, using these and related results, lower bounds are obtained for two-way sequential machines and, using these and related results, lower bounds are obtained on the amount of work tape required online and offline Turing machines that compute nonrepresentable functions."
1967,The Organization of Computations for Uniform Recurrence Equations.,"A set equations in the quantities ai(p), where i = 1, 2, ¬∑ ¬∑ ¬∑, m and p ranges over a set R of lattice points in n-space, is called a system of uniform recurrence equations if the following property holds: If p and q are in R and w is an integer n-vector, then ai(p) depends directly on aj(p - w) if and only if ai(q) depends directly on aj(q - w). Finite-difference approximations to systems of partial differential equations typically lead to such recurrence equations. The structure of such a system is specified by a dependence graph G having m vertices, in which the directed edges are labeled with integer n-vectors. For certain choices of the set R, necessary and sufficient conditions on G are given for the existence of a schedule to compute all the quantities ai(p) explicitly from their defining equations. Properties of such schedules, such as the degree to which computation can proceed ‚Äúin parallel,‚Äù are characterized. These characterizations depend on a certain iterative decomposition of a dependence graph into subgraphs. Analogous results concerning implicit schedules are also given."
1967,Parallel Program Schemata: A Mathematical Model for Parallel Computation.,"Abstract:
In this paper we report some results of a study on the range of possible structure of programming languages. The main emphasis is on the range of graphical (""topological"" or flowchart) and syntactic structure. For the sake of simplicity and precision we rather severely limit the ""semantic structure"" of the languages--we restrict ourselves to command (instruction) languages for Turing machines. As we show, this apparently strong limitation imposes very little restriction on the graphical and syntactic structure. The bulk of the paper consists of the presentation of six Turing machine languages. These languages serve to illustrate the range of possible structure and, more important, they allow us to establish the range of a number of structural parameters. All the languages are universal in the sense that in each one we can program every computable function. However, they differ greatly in syntax, graphical structure, ease of compilation (assembly), and in the type of machine, if any, which can operate directly in the language. In brief, we present languages with finite-state, context-free and more complex syntax; languages with ""conventional"" graphical structure, with block structure and only one transfer per block, with only nested transfers (nested loops), with transfers only to the immediately neighboring instructions, and with only one transfer per program."
1966,Index Register Allocation.,A procedure for index register allocation is described. The rules of this procedure are shown to yield an optimal allocation for ‚Äústraight line‚Äù programs.
1965,The Construction of Discrete Dynamic Programming Algorithms.,"Abstract:
Thus, while dynamic programming, particularly when combined with the method of successive approximations, is a powerful and flexible weapon for attacking operations research type problems and belongs in the arsenal of every systems engineer, the other available techniques of linear programming, simulation, and simple decision rules, should be considered in each case."
1964,Some Techniques of State Assignment for Synchronous Sequential Machines.,"Abstract:
This paper is an extension of the work of Hartmanis and Stearns on sequential machine state assignments with reduced dependence. In Section II the critical partition pairs of transition-incomplete sequential machines are defined, and methods are given for computing them. Section III pursues certain combinatorial problems associated with the construction of assignments with reduced dependence, once the critical partition pairs are known. The algorithms derived in this paper provide the nucleus of a systematic procedure for the construction of economical state assignments."
1962,Minimization Over Boolean Graphs.,"Abstract:
This paper presents a systematic procedure for the design of gate-type combinational switching circuits without directed loops. Each such circuit (Boolean graph) is in correspondence with a sequence of decompositions of the Boolean function which it realizes. A general approach to functional decomposition is given and, in terms of a convenient positional representation, efficient tests for the detection of decompositions are derived. These results are employed in the development of an alphabetic search procedure for determining minimum-cost Boolean graphs which satisfy any given design specifications."
1961,Minimum-redundancy coding for the discrete noiseless channel.,"Abstract:
This paper gives a method for constructing minimum-redundancy prefix codes for the general discrete noiseless channel without constraints. The costs of code letters need not be equal, and the symbols encoded are not assumed to be equally probable. A solution had previously been given by Huffman [10] in 1952 for the special case in which all code letters are of equal cost. The present development is algebraic. First, structure functions are defined, in terms of which necessary and sufficient conditions for the existence of prefix codes may be stated. From these conditions, linear inequalities are derived which may be used to characterize prefix codes. Gomory's integer programming algorithm is then used to construct optimum codes subject to these inequalities; computational experience is presented to demonstrate the practicability of the method. Finally, some additional coding problems are discussed and a problem of classification is treated."
1961,A computer program for the synthesis of combinational switching circuits.,"Abstract:
This paper describes an IBM 7090 program for the design of single output combinational switching circuits for arbitrary sets of primitive logical elements. The only restriction on circuit configurations is that no feedback loops may occur. The procedure is an outgrowth of one given by Roth. The decomposition techniques are generalizations of those discussed by Ashenhurst. Given a function F (A, B, C), where A, B, and C are states of binary variables and F may have don't care combinations, a representation of the form F = G [alpha (A, B), B, C] is called decomposition of F. Any loop-free circuit can be described by a sequence of decompositions. Efficient procedures for the detection of decompositions are given in terms of a convenient normal form representation of switching functions. Simplifications obtained by considering only the subclass of so called vertex-functions are discussed. The program carries out a systematic search through the admissable sequences of decompositions, using a lexicographic ordering, designed so that the most promising sequences are investigated first. Several further refinements are used to limit the search. The calculation yields a list of successively improved implementations, eventually including one of minimum cost. Examples are given of several circuits of minimum or near minimum cost derived by the program."
1960,A Note on the Applicaton of Graph Theory to Digital Computer Programming.,"A graph-theoretic model for the description of flowcharts and programs is defined. It is shown that properties of directed graphs and the associated connection matrices can be used to detect errors and eliminate redundancies in programs. These properties are also used in the synthesis of composite programs. Finally, the model is expanded to take into account frequencies of execution of portions of a program, and a problem concerning optimum arrangement of a program in storage is solved."
