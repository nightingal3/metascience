2014,"Cryptography miracles, secure auctions, matching problem verification.",A solution to the persistent problem of preventing collusion in Vickrey auctions.
2012,Strictly-Black-Box Zero-Knowledge and Efficient Validation of Financial Transactions.,"Abstract
Zero Knowledge Proofs (ZKPs) are one of the most striking innovations in theoretical computer science. In practice, the prevalent ZKP methods are, at times, too complicated to be useful for real-life applications. In this paper we present a practically efficient method for ZKPs which has a wide range applications. Specifically, motivated by the need to provide an upon-demand efficient validation of various financial transactions (e.g., the high-volume Internet auctions), we have developed a novel secure and highly efficient method for validating correctness of the output of a transaction while keeping input values secret. The method applies to input values which are publicly committed to by employing generic commitment functions (even input values submitted using tamper-proof hardware solely with input/ output access can be used.) We call these: strictly black box [SBB] commitments. Hence these commitments are typically much faster than public-key ones, and are the only cryptographic/ security tool we give the poly-time players, throughout. The general problem we solve in this work is: Let SLC be a publicly known staight line computation on n input values taken from a finite field and having k output values. The inputs are publicly committed to in a SBB manner. An Evaluator performs the SLC on the inputs and announces the output values. Upon demand the Evaluator, or a Prover acting on his behalf, can present to a Verifier a proof of correctness of the announced output values. This is done in a manner that (1) The input values as well as all intermediate values of the SLC remain information theoretically secret. (2) The probability that the Verifier will accept a false claim of correctness of the output values can be made exponentially small. (3) The Prover can supply any required number of proofs of correctness to multiple Verifiers. (4) The method is highly efficient. The application to financial processes is straight forward. To this end (1) we first use a novel technique for representation of values from a finite field which we call ‚Äúsplit representation‚Äù, the two coordinates of the split representation are generically committed to; (2) next, the SLC is augmented by the Prover into a ‚Äùtranslation‚Äù which is presented to the Verifier as a sequence of generically committed split representations of values; (3) using the translation, the Prover and Verifier conduct a secrecy preserving proof of correctness of the announced SLC output values; (4) in order to exponentially reduce the probability of cheating by the Prover and also to enable multiple proofs, a novel highly efficient method for preparation of any number of committed-to split representations of the n input values is employed. The extreme efficiency of these ZK methods is of decisive importance for large volume applications. Secrecy preserving validation of announced results of Vickrey auctions is our demonstrative example."
2012,Never too early to begin: computer science for high-school students.,"Computer science and technology innovated over the past sixty years, have revolutionized science, the economy and societal interactions. Inherently CS constitutes a new science combining mathematics, logic, information theory and electronics, on par with physics, chemistry and the life sciences. It is appropriate to educate students in the fundamentals of this science. The curriculum should emphasize the scientific content rather than provide mere training in some programming language."
2009,Cryptographic Combinatorial Clock-Proxy Auctions.,"Abstract
We present a cryptographic protocol for conducting efficient, provably correct and secrecy-preserving combinatorial clock-proxy auctions. The ‚Äúclock phase‚Äù functions as a trusted auction despite price discovery: bidders submit encrypted bids, and prove for themselves that they meet activity rules, and can compute total demand and thus verify price increases without revealing any information about individual demands. In the sealed-bid ‚Äúproxy phase‚Äù, all bids are revealed the auctioneer via time-lapse cryptography and a branch-and-bound algorithm is used to solve the winner-determination problem. Homomorphic encryption is used to prove the correctness of the solution, and establishes the correctness of the solution to any interested party. Still an NP-hard optimization problem, the use of homomorphic encryption imposes additional computational time on winner-determination that is linear in the size of the branch-and-bound search tree, and thus roughly linear in the original (search-based) computational time. The result is a solution that avoids, in the usual case, the exponential complexity of previous cryptographically-secure combinatorial auctions."
2008,"Practical secrecy-preserving, verifiably correct and trustworthy auctions.","Abstract
We present a practical protocol based on homomorphic cryptography for conducting provably fair sealed-bid auctions. The system preserves the secrecy of the bids, even after the announcement of auction results, while also providing for public verifiability of the correctness and trustworthiness of the outcome. No party, including the auctioneer, receives any information about bids before the auction closes, and no bidder is able to change or repudiate any bid. The system is illustrated through application to first-price, uniform-price and second-price auctions, including multi-item auctions. Empirical results based on an analysis of a prototype demonstrate the practicality of our protocol for real-world applications."
2007,Highly Efficient Secrecy-Preserving Proofs of Correctness of Computations and Applications.,"Abstract:
We present a highly efficient method for proving correctness of computations while preserving secrecy of the input values. This is done in an Evaluator-Prover model which can also be realized by a secure processor. We describe an application to secure auctions."
2007,DISC 20th Anniversary: Invited Talk Provably Unbreakable Hyper-Encryption Using Distributed Systems.,"Abstract
Encryption is a fundamental building block for computer and communications technologies. Existing encryption methods depend for their security on unproven assumptions. We propose a new model, the Limited Access model for enabling a simple and practical provably unbreakable encryption scheme. A voluntary distributed network of thousands of computers each maintain and update random pages, and act as Page Server Nodes. A Sender and Receiver share a random key K. They use K to randomly select the same PSNs and download the same random pages. These are employed in groups of say 30 pages to extract One Time Pads common to S and R. Under reasonable assumptions of an Adversary‚Äôs inability to monitor all PSNs, and easy ways for S and R to evade monitoring while downloading pages, Hyper Encryption is clearly unbreakable. The system has been completely implemented."
2006,"Practical secrecy-preserving, verifiably correct and trustworthy auctions.","We present a practical system for conducting sealed-bid auctions that preserves the secrecy of the bids while providing for verifiable correctness and trustworthiness of the auction. The auctioneer must accept all bids submitted and follow the published rules of the auction. No party receives any useful information about bids before the auction closes and no bidder is able to change or repudiate her bid. Our solution uses Paillier's homomorphic encryption scheme [25] for zero knowledge proofs of correctness. Only minimal cryptographic technology is required of bidders; instead of employing complex interactive protocols or multi-party computation, the single auctioneer computes optimal auction results and publishes proofs of the results' correctness. Any party can check these proofs of correctness via publicly verifiable computations on encrypted bids. The system is illustrated through application to first-price, uniform-price and second-price auctions, including multi-item auctions. Our empirical results demonstrate the practicality of our method: auctions with hundreds of bidders are within reach of a single PC, while a modest distributed computing network can accommodate auctions with thousands of bids."
2005,Provably unbreakable hyper-encryption in the limited access model.,"Abstract:
Encryption is a fundamental building block for computer and communications technologies. Existing encryption methods depend for their security on unproven assumptions. We propose a new model, the limited access model for enabling a simple and practical provably unbreakable encryption scheme. A voluntary network of tens of thousands of computers each maintain and update random pages, and act as page server nodes. A sender and receiver share a random key K. They use K to randomly select the same PSNs and download the same random pages. These are employed in groups of say 30 pages to extract one time pads common to S and R. Under reasonable assumptions of an adversary's inability to monitor all PSNs, and easy ways for S and R to evade monitoring while downloading pages, hyper encryption is clearly unbreakable. The system has been completely implemented"
2004,Identity-Based Zero Knowledge.,"Abstract
We introduce and define the notion of identity-based zero-knowledge, concentrating on the non-interactive setting. In this setting, our notion allows any prover to widely disseminate a proof of a statement while protecting the prover from plagiarism in the following sense: although proofs are transferable (i.e., publicly verifiable), they are also bound to the identity of the prover in a way which is recognizable to any verifier. Furthermore, an adversary is unable to change this identity (i.e., to claim the proof as his own, or to otherwise change the authorship), unless he could have proved the statement on his own.
While we view the primary contribution of this work as a formal definition of the above notion, we also explore the relation of this notion to that of non-malleable (non-interactive) zero-knowledge. On the one hand, we show that these two notions are incomparable: that is, there are proof systems which are non-malleable but not identity-based, and vice versa. On the other hand, we show that a proof system of either type essentially implies a proof system of the other type."
2003,Hyper Encryption and Everlasting Secrets.,"Abstract
A fundamental problem in cryptography is that of secure communication over an insecure channel, where a sender Alice wishes to communicate with a receiver Bob, in the presence of a powerful Adversary
AD
A
. The primary goal of encryption is to protect the privacy of the conversation between Alice and Bob against
AD
A
. Modern cryptographic research has identified additional essentially important criteria for a secure encryption scheme. Namely that the encryption be non-malleable, be resistant to various chosen plaintext and ciphertext attacks, and if so desired, will allow the receiver to authenticate the received message and its sender. All these issues are now settled for the case that the Adversary
AD
A
is computationally unbounded."
2003,Zero-Knowledge Sets.,"Abstract:
We show how a polynomial-time prover can commit to an arbitrary finite set S of strings so that, later on, he can, for any string x, reveal with a proof whether x /spl isin/ S or x /spl notin/ S, without revealing any knowledge beyond the verity of these membership assertions. Our method is non interactive. Given a public random string, the prover commits to a set by simply posting a short and easily computable message. After that, each time it wants to prove whether a given element is in the set, it simply posts another short and easily computable proof, whose correctness can be verified by any one against the public random string. Our scheme is very efficient; no reasonable prior way to achieve our desiderata existed. Our new primitive immediately extends to providing zero-knowledge databases."
2002,Online Scheduling of Parallel Programs on Heterogeneous Systems with Applications to Cilk.,"We study the problem of executing parallel programs, in particular Cilk programs, on a collection of processors of different speeds. We consider a model in which each processor maintains an estimate of its own speed, where communication between processors has a cost, and where all scheduling must be online. This problem has been considered previously in the fields of asynchronous parallel computing and scheduling theory. Our model is a bridge between the assumptions in these fields. We provide a new more accurate analysis of an old scheduling algorithm called the maximum utilization scheduler . Based on this analysis, we generalize this scheduling policy and define the high utilization scheduler . We next focus on the Cilk platform and introduce a new algorithm for scheduling Cilk multithreaded parallel programs on heterogeneous processors. This scheduler is inspired by the high utilization scheduler and is modified to fit in a Cilk context. A crucial aspect of our algorithm is that it keeps the original spirit of the Cilk scheduler. In fact, when our new algorithm runs on homogeneous processors, it exactly mimics the dynamics of the original Cilk scheduler.
"
2002,Everlasting security in the bounded storage model.,"Abstract:
We address the problem of the-security of cryptographic protocols in face of future advances in computing technology and algorithmic research. The problem stems from the fact may be deemed that computations which at a given point in time may be deemed infeasible, can, in the course of years or decades, be made possible with improved hardware and/or breakthroughs in code-breaking algorithms. In such cases, the security of historical , but nonetheless highly confidential data may be in jeopardy. We present a scheme for efficient secure two-party communication with provable everlasting security. The security is guaranteed in face of any future technological advances, given the current state of of the art. Furthermore, the security of the messages is also guaranteed even if the secret encryption/decryption key is revealed in the future, The scheme is based on the bounded storage model and provides information-theoretic security in this model. The bounded storage model postulates an adversary who is computationally unbounded, and is only bounded in the amount of storage (not computation space) available to store the output of his computation. The bound on the storage can be arbitrarily large (e.g., 100 Tbytes), as long as it is fixed. Given this storage bound, our protocols guarantee that even a computationally all powerful adversary gains no information about a message (except with a probability that is exponentially small in the security parameter k). The bound on storage space need only hold at the time of the message transmission. Thereafter, no additional storage space or, computational power can help the adversary in deciphering the message. We present two protocols. The first protocol, which elaborates on the autoregressive (AR) protocol of Aumann and Rabin (see Advances in Cryptology-Crypto '99, p. 65-79, 1999), employs a short secret key whose size is independent of the length of the message, but uses many public random bits. The second protocol uses an optimal ...
(View more)"
2002,Hyper-Encryption and Everlasting Security.,"Abstract
We present substantial extensions of works [1], [2], and all previous works, on encryption in the bounded storage model introduced by Maurer in [25]. The major new result is that the shared secret key employed by the sender Alice andthe receiver Bob can be re-used to send an exponential number of messages, against strong adaptive attacks. This essential step enhances the usability of the encryption method, and also allows strong authentication andnon-malleability described below.
We give an encryption scheme that is provably secure against adaptive attacks by a computationally unbounded adversary in the bounded storage model. In the model, a sender Alice and a receiver Bob have access to a public random string Œ±, and share a secret key s. Alice and Bob observe Œ± on the fly, andb y use of s extract bits from which they create a one-time pad X used to encrypt M as C = X‚äïM. The size of the secret key s is ‚à£s‚à£ = klog2 ‚à£Œ±‚à£, where k is a security parameter. An Adversary AD can compute andstore any function A 1(Œ±) = Œ∑, subject to the bound on storage ‚à£Œ∑‚à£ ‚â§Œ≥. ¬∑‚à£Œ±‚à£, Œ≥ < 1, and captures C. Even if AD later gets the key s and is computationally unbounded, the encryption is provably secure. Assume that the key s is repeatedly used with successive strings Œ±1, Œ±2, ... to produce encryptions C 1, C 2, ... of messagesM 1,M 2, .... AD computes Œ∑1 = A 1(Œ±1), obtains C 1, and gets to see the first message M 1. Using these he computes andstores Œ∑2 = A 1(Œ±2, Œ±1, C 1,M 1), and so on. When he has stored Œ∑l andcaptured C l, he gets the key s (but not M l). The main result is that the encryption C l is provably secure against this adaptive attack, where l, the number of time the secret key s is re-used, is exponentially large in the security parameter k. On this we base non-interactive protocols for authentication and non-malleability. Again, the shared secret key used in these protocols can be securely re-usedan exponential number of times against adaptive attacks. The method of proof is stronger than the one in [1], [2], and yields ergodic results of independent interest. We discuss in the Introduction the feasibility of the bounded storage model, and outline a solution. Furthermore, the existence of an encryption scheme with the provable strong security properties presented here, may prompt other implementations of the bounded storage model."
2001,Linear-Consistency Testing.,"Abstract
We extend the notion of linearity testing to the task of checking linear consistency of multiple functions. Informally, functions are ‚Äúlinear‚Äù if their graphs form straight lines on the plane. Two such functions are ‚Äúconsistent‚Äù if the lines have the same slope. We propose a variant of a test of M. Blum et al. (J. Comput. System Sci.47 (1993), 549‚Äì595) to check the linear consistency of three functions f1, f2, f3 mapping a finite Abelian group G to an Abelian group H: Pick x, y‚ààG uniformly and independently at random and check if f1(x)+f2(y)=f3(x+y). We analyze this test for two cases: (1) G and H are arbitrary Abelian groups and (2) G=
n2 and H=
2. Questions bearing close relationship to linear-consistency testing seem to have been implicitly considered in recent work on the construction of PCPs and in particular in the work of J. H√•stad [9] (in ‚ÄúProceedings of the Twenty-Ninth Annual ACM Symposium on Theory of Computing, El Paso, Texas, 4‚Äì6 May 1997,‚Äù pp. 1‚Äì10). It is abstracted explicitly for the first time here. As an application of our results we give yet another new and tight characterization of NP, namely ‚àÄŒµ>0, NP=MIP1‚àíŒµ, 1/2[O(log n), 3, 1]. That is, every language in NP has 3-prover 1-round proof systems in which the verifier tosses O(log n) coins and asks each of the three provers one question each. The provers respond with one bit each such that the verifier accepts instance of the language with probability 1‚àíŒµ and rejects noninstances with probability at least
. Such a result is of some interest in the study of probabilistically checkable proofs."
2000,Scheduling Cilk multithreaded parallel programs on processors of different speeds.,"We study the problem of executing parallel programs, in particular Cilk programs, on a collection of processors of different speeds. We consider a model in which each processor maintains an estimate of its own speed, where communication between processors has a cost, and where all scheduling must be online. This problem has been considered previously in the fields of asynchronous parallel computing and scheduling theory. Our model is a bridge between the assumptions in these fields. We provide a new more accurate analysis of of an old scheduling algorithm called the maximum utilization scheduler. Based on this analysis, we generalize this scheduling policy and define the high utilization scheduler. We next focus on the Cilck platform and introduce a new algorithm for scheduling Cilk multithreaded parallel programs on heterogeneous processors. This scheduler is inspired by the high utilization scheduler and is modified to fit in a Cilk context. A crucial aspect of our algorithm is that it keeps the original spirit of the Cilk scheduler. In fact, when our new algorithm runs on homogeneous processors, it exactly mimics the dynamics of the original Cilk scheduler."
1999,Information Theoretically Secure Communication in the Limited Storage Space Model.,"Abstract
We provide a simple secret-key two-party secure communication scheme, which is provably information-theoretically secure in the limited-storage-space model. The limited-storage-space model postulates an eavesdropper who can execute arbitrarily complex computations, and is only limited in the total amount of storage space (not computation space) available to him. The bound on the storage space can be arbitrarily large (e.g. terabytes), as long as it is fixed. Given this bound, the protocol guarantees that the probability of the eavesdropper of gaining any information on the message is exponentially small. The proof of our main results utilizes a novel combination of linear algebra and Kolmogorov complexity considerations."
1999,Verifiable Random Functions.,"Abstract:
We efficiently combine unpredictability and verifiability by extending the Goldreich-Goldwasser-Micali (1986) construction of pseudorandom functions f/sub s/ from a secret seed s, so that knowledge of s not only enables one to evaluate f/sub s/ at any point x, but also to provide an NP-proof that the value f/sub s/(x) is indeed correct without compromising the unpredictability of f/sub s/ at any other point for which no such a proof was provided."
1999,Linear Consistency Testing.,"Abstract
We extend the notion of linearity testing to the task of checking linear-consistency of multiple functions. Informally, functions are ‚Äúlinear‚Äù if their graphs form straight lines on the plane. Two such functions are ‚Äúconsistent‚Äù if the lines have the same slope. We propose a variant of a test of Blum, Luby and Rubinfeld [8] to check the linear-consistency of three functions f 1,f 2,f 3 mapping a finite Abelian group G to an Abelian group H: Pick x,y ‚àà G uniformly and independently at random and check if  f 1(x) + f 2(y) = f 3(x + y). We analyze this test for two cases: (1) G and H are arbitrary Abelian groups and (2)
G=
F
n
2
G
and
H=
F
2
H
.
Questions bearing close relationship to linear-consistency testing seem to have been implicitly considered in recent work on the construction of PCPs (and in particular in the work of H√•stad [9]). It is abstracted explicitly for the first time here. We give an application of this problem (and of our results): A (yet another) new and tight characterization of NP, namely ‚àÄ Œµ > 0.
NP=
MIP
1‚àíœµ,
1
2
[=(logn),3,1]
N
I.e., every language in NP has 3-prover 1-round proof systems in which the verifier tosses O(log n) coins and asks each of the three provers one question each. The provers respond with one bit each such that the verifier accepts instance of the language with probability 1‚Äì Œµ and rejects non-instances with probability at least
1
2
1
. Such a result is of some interest in the study of probabilistically checkable proofs."
1998,Lower Bounds for Randomized Mutual Exclusion.,"We establish, for the first time, lower bounds for randomized mutual exclusion algorithms (with a read-modify-write operation). Our main result is that a constant-size shared variable cannot guarantee strong fairness, even if randomization is allowed. In fact, we prove a lower bound of $\Omega (\log\log n)$ bits on the size of the shared variable, which is also tight.
We investigate weaker fairness conditions and derive tight (upper and lower) bounds for them as well. Surprisingly, it turns out that slightly weakening the fairness condition results in an exponential reduction in the size of the required shared variable. Our lower bounds rely on an analysis of Markov chains that may be of interest on its own and may have applications elsewhere.
"
1998,"Authentication, Enhanced Security and Error Correcting Codes (Extended Abstract).","Abstract
In electronic communications and in access to systems, the issue of authentication of the Sender S of a message M, as well as of the message itself, is of paramount importance. Recently S. Goldwasser has raised the additional issue of Deniable Authentication where the sender S authenticates the message M to the Receiver's (R) satisfaction, but can later deny his authorship of M even to an Inquisitor INQ who has listened to the exchange between S and R and who gains access to all of the the secret information used by S and R. We present two practical schemes for Deniable Authentication of messages M of arbitrary length n. In both schemes the Receiver R is assured with probability greater than 1 ‚àí 2‚àík , where k is a chosen security parameter, that M originated with the Sender S. Deniability is absolute in the information theoretic sense. The first scheme requires 2.4kn XOR operations on bits and one public key encoding and decoding of a short message. The second scheme requires the same number of XOR operations and k multiplications mod N, where N is some fixed product of two large primes. A key new feature of our method is the use of a Shannon-style error correction code. Traditional authentication for a long message M starts by hashing M down to a standard word-size. We expand M through error correction. The first Deniable Authentication method is provably valid for any encryption scheme with minimal security properties, i.e. this method is generic. The second Deniable Authentication method is provably valid under the usual assumption that factorization is intractable."
1998,Simplified VSS and Fast-Track Multiparty Computations with Applications to Threshold Cryptography.,"The goal of this paper is to introduce a simple verifiable secret sharing scheme, to improve the efficiency of known secure multiparty protocols and, by employing these techniques, to improve the efficiency of applications which use these protocols. First we present a very simple verifiable secret sharing protocol which is based on fast cryptographic primitives and avoids altogether the need for expensive zero-knowledge proofs."
1997,Correctness of Programs and Protocols through Randomization (Extended Abstract).,n/a
1997,"DNA2DNA computations: A potential ""killer app?"".",n/a
1997,"Hashing on strings, cryptography, and protection of privacy.","Abstract:
Summary form only given. The issues of privacy and reliability of personal data are of paramount importance. If L is a list of people carrying some harmful defective gene, we want questions as to whether a person is in L to be reliably answered without compromising the data concerning anybody else. Reliability means that once the list is formed, nobody can play with the answer. Thus the answer should be checkable by the agent posing the question. We present an efficient protocol for this task. Our solution has very strong privacy protection properties."
1996,Computationally Hard Algebraic Problems (extended abstract).,"Abstract:
In this paper we present a simple geometric-like series of elements in a finite field F/sub q/, and show that computing its sum is NP-hard. This problem is then reduced to the problem of counting mod p the number of zeroes in a linear recurrence sequence with elements in a finite F/sub p/, where p is a small prime. Hence the latter problem is also NP-hard. In the lecture we shall also survey other computationally hard algebraic problems."
1995,On Lotteries with Unique Winners.,"We establish, for the first time, lower bounds for randomized mutual exclusion algorithms (with a read-modify-write operation). Our main result is that a constant-size shared variable cannot guarantee strong fairness, even if randomization is allowed. In fact, we prove a lower bound of log log n bits on the size of the shared variable, which is also tight. We investigate weaker fairness conditions and derive tight (upper and lower) bounds for them as well. Surprisingly, it turns out that slightly weakening the fairness condition results in an exponential reduction in the size of the required shared variable. Our lower bounds rely on an analysis of Markov chains that may be of interest on its own and may have applications elsewhere. "
1995,"Parallel Processing on Networks of Workstations: A Fault-Tolerant, High Performance Approach.","Abstract:
One of the most sought after software innovation of this decade is the construction of systems using off-the-shelf-workstations that actually deliver and even surpass, the power and reliability of supercomputers. Using completely novel techniques: eager scheduling, evasive memory layouts and dispersed data management it is possible to build an execution environment for parallel programs on workstation networks. These techniques were originally developed in a theoretical framework for an abstract machine which models a shared memory asynchronous multiprocessor. The network of workstations platform presents an inherently asynchronous environment for the execution of our parallel program. This gives rise to substantial problems of correctness of the computation and of proper automatic load balancing of the work amongst the processors, so that a slow processor will not hold up the total computation. A limiting case of asynchrony is when a processor becomes infinitely slow, i.e. fails. Our methodology copes with all these problems, as well as with memory failures. An interesting feature of this system is that it is neither a fault-tolerant system extended for parallel processing nor is it parallel processing system extended for fault tolerance. The same novel mechanisms ensure both properties."
1994,Clock Construction in Fully Asynchronous Parallel Systems and PRAM Simulation.,"Abstract
We consider the problem of simulating synchronous computations on asynchronous shared memory systems. The systems we consider allow for arbitrary asynchronous behavior of the processors. In addition, we make very limited (and in some cases no) assumptions about the atomicity of read and write operations to shared memory. We provide detailed definitions of these asynchronous systems and their atomicity properties.
The first construction in this paper is a novel clock for asynchronous systems. The clock is a basic tool for synchronization in the asynchronous environment. The constructiion we give is extremely robust, and can be implemented in a system with no atomicity assumptions, and in the presence of an adaptive adversary scheduler The correct behavior of the clock is obtained with overwhelming probability (>1‚àí2‚àíŒ±n, Œ±>0).
We then show how to harness this clock to drive an efficient PRAM simulation on an asynchronous system. The simulation requires an O(log2 n) work, and O(log n) space, overhead. This improves by a log n factor on the efficiency of previously obtained simulation results, while relaxing the assumptions on the underlying asynchronous system."
1993,Highly Efficient Asynchronous Execution of Large-Grained Parallel Programs.,"Abstract:
An n-thread parallel program p is large-grained if in every parallel step the computations on each of the threads are complex procedures requiring numerous processor instructions. This practically relevant style of programs differs from PRAM programs in its large granularity and the possibility that within a parallel step the computations on different threads may considerably vary in size. Let M be an n-processor asynchronous parallel system, with no restriction on the degree of asynchrony and without any specialized synchronization mechanisms. It is a challenging theoretical as well as practically important problem to ensure correct execution of P on such a parallel machine. Let P be a large-grained program requiring total work W for its execution on a synchronous a-processor parallel system. We present a transformation (compilation) of P into a program C(P) which correctly and efficiently effects the computation of P on the asynchronous machine M. Under moderate assumptions on the granularity of threads and the size of the program variables, execution of C(P) requires just O(Wlog* n) expected total work, and the memory space overhead is a small multiplicative constant.< >"
1993,Lower bounds for randomized mutual exclusion.,"We establish, for the first time, lower bounds for randomized mutual exclusion algorithms (with a read-modify-write operation). Our main result is that a constant-size shared variable cannot guarantee strong fairness, even if randomization is allowed. In fact, we prove a lower bound of $\Omega (\log\log n)$ bits on the size of the shared variable, which is also tight.
We investigate weaker fairness conditions and derive tight (upper and lower) bounds for them as well. Surprisingly, it turns out that slightly weakening the fairness condition results in an exponential reduction in the size of the required shared variable. Our lower bounds rely on an analysis of Markov chains that may be of interest on its own and may have applications elsewhere.
"
1992,Clock Construction in Fully Asynchronous Parallel Systems and PRAM Simulation (Extended Abstract).,"Abstract:
The authors discuss the question of simulating synchronous computations on asynchronous systems. They consider an asynchronous system with very weak, or altogether lacking any, atomicity assumptions. The first contribution of this paper is a novel clock for asynchronous systems. The clock is a basic tool for synchronization in the asynchronous environment. It is a very robust construction and can operate in a system with no atomicity assumptions, and in the presence of a dynamic scheduler. The behavior of the clock is obtained with overwhelming probability (1-2/sup - alpha n/, alpha >0). The authors show how to harness this clock to drive a PRAM simulation on an asynchronous system. The resulting simulation scheme is more efficient than existing ones, while actually relaxing the assumptions on the underlying asynchronous system.< >"
1992,Dependable Parallel Computing by Randomization (Abstract).,n/a
1992,Randomized Mutual Exclusion Algorithms Revisited.,"In [4] a randomized algorithm for mutual exclusion with bounded waiting, employing a logarithmic sized shared variable, was given. Saias and Lynch [5] pointed out that the adversary scheduler postulated in the above paper can observe the behavior of processes in the interval between an opening of the critical section and the next closing of the critical section. it can then draw conclusions about values of their local variables as well as the value of the randomized round number component of the shared variable, and arrange the schedule so as to discriminate against a chosen process. This invalidates the claimed properties of the algorithm. In the present paper the algorithm in [4] is modified, using the ideas of [4], so as to overcome this difficulty, obtaining essentially the same results. Thus, as in [4], randomization yields simple algorithms for mutual-exclusion with bounded waiting, employing a shared variable of considerably smaller size than the lower-bound established in [1] for deterministic algorithms."
1992,Efficient Program Transformations for Resilient Parallel Computation via Randomization (Preliminary Version).,"In this paper, we address the problem of automatically transforming arbitrary programs written for an ideal parallel machine to run on a completely asynchronous machine. We present a transformation which can be applied to an ideal program such that the resulting program's execution on an asynchronous machine is work and space efficient, relative to the ideal program from which it is derived. Above all, the transformation will guarantee that the ideal program will execute in a continually progressive manner on the asynchronous machine; these instructions are not universal. Furthermore, the individual processors can get delayed for arbitrary amounts of time while executing any instruction. In contrast, previous work relied either on the asynchronous machine having universal read-modify-write instructions as primitives, or on limited asynchrony by restricting the relative speeds of the processors."
1991,Set systems with no union of cardinality 0 modulom.,"Abstract
Letq be a prime power. It is shown that for any hypergraph‚Ñ± = {F1,...,Fd(q‚àí1)+1} whose maximal degree isd, there exists √ò ‚â† ‚Ñ±0 ‚äÇ ‚Ñ±, such that
‚â° 0 (modq)."
1989,Biased Coins and Randomized Algorithms.,n/a
1989,"Efficient dispersal of information for security, load balancing, and fault tolerance.","An Information Dispersal Algorithm (IDA) is developed that breaks a file F of length L = ‚Üø F‚Üæ into n pieces Fi, l ‚â§ i ‚â§ n, each of length ‚ÜøFi‚Üæ = L/m, so that every m pieces suffice for reconstructing F. Dispersal and reconstruction are computationally efficient. The sum of the lengths ‚ÜøFi‚Üæ is (n/m) ¬∑ L. Since n/m can be chosen to be close to l, the IDA is space efficient. IDA has numerous applications to secure and reliable storage of information in computer networks and even on single disks, to fault-tolerant and efficient transmission of information in networks, and to communications between processors in parallel computers. For the latter problem provably time-efficient and highly fault-tolerant routing on the n-cube is achieved, using just constant size buffers."
1989,Maximum Matchings in General Graphs Through Randomization.,"Abstract
A new randomized algorithm for the maximum matching problem is presented. Unlike conventional matching algorithms which are combinatorial, our algorithm is algebraic and works on the Tutte matrix of the given graph. Although slower than the best known matching algorithm, our algorithm has the advantage of being conceptually simple and easy to program."
1989,ITOSS: An Integrated Toolkit For Operating System Security.,n/a
1987,A Logic to Reason about Likelihood.,"Abstract
We present a logic LL which uses a modal operator L to help capture the notion of being likely. Despite the fact that likelihood is not assigned quantitative values through probabilities, LL captures many of the properties of likelihood in an intuitively appealing way. We give a possible-worlds style semantics to LL, and, using standard techniques of modal logic, we give a complete axiomatization for LL and show that satisfiability of LL formulas can be decided in exponential time. We discuss how the logic might be used in areas such as medical diagnosis, where decision making in the face of uncertainties is crucial. We conclude by using LL to give a formal proof of correctness of some aspects of a protocol for exchanging secrets."
1987,Efficient Randomized Pattern-Matching Algorithms.,"Abstract:
We present randomized algorithms to solve the following string-matching problem and some of its generalizations: Given a string X of length n (the pattern) and a string Y (the text), find the first occurrence of X as a consecutive block within Y. The algorithms represent strings of length n by much shorter strings called fingerprints, and achieve their efficiency by manipulating fingerprints instead of longer strings. The algorithms require a constant number of storage locations, and essentially run in real time. They are conceptually simple and easy to implement. The method readily generalizes to higher-dimensional pattern-matching problems."
1987,Achieving Independence in Logarithmic Number of Rounds.,"Simultaneous broadcast [CGMAJ is a fundamental tool in designing secure protocols for fault tolerant distributed computing. A system that supports it enables n processes to globally commit to independently chosen values (a significantly harder task than mere agreement). It is also a basic building block in a recent %ompletenessî theorem of [GMWZ]. In this paper we present a new protocol for simultaneous broadcast. Building upon past work, we introduce a novel method of concurrently alternating and interleaving n executions of verifiable secret sharing protocols. This approach greatly improves the time complexity (number of communication rounds) of simultaneous broadcast. Previous protocols (combination of [CGMA] and [GMW]) q re uired the complete serialization of the ra verifiable secret sharings, resulting in n(n) communication rounds. Our protocol is constructive, and requires only log n + log log n serial executions of verifiable secret sharings. It preserves maximum fault tolerance (t < n/2 faults), and polynomial resource bounds (internal computation and communication bits). The same improvement appiies to the general simulation in [GMWX]. In light of its improved performance, it is significant that our our protocol has a fairly simple correctness proof. In the slippery business of distributed cryptographic protocols, simpler proofs are important. * Research supported by NSF Grant MCS81-21431 at Harvard University t Contact author. Email address: rabinQharvard.harvard.edu Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission"
1983,Transaction Protection by Beacons.,"Abstract
Protocols for implementing contract signing, confidential disclosures, and certified mail in an electronic mail system are proposed. These transactions are provably impossible without a trusted intermediary. However, they can be implemented with just a small probability of a participant cheating his partner, by use of a beacon emitting random integers. Applications include privacy protection of personal information in data banks, as well as the protection of business transactions."
1983,Randomized Byzantine Generals.,"Abstract:
We present a randomized solution for the Byzantine Generals Problems. The solution works in the synchronous as well as the asynchronous case and produces Byzantine Agreement within a fixed small expected number of computational rounds, independent of the number n of processes and the bound t on the number of faulty processes. The solution uses A. Shamir's method for sharing secrets. It specializes to produce a simple solution for the Distributed Commit problem."
1983,A Logic to Reason about Likelihood.,"We present a logic LL which uses a modal operator L to help capture the notion of likely. Despite the fact that no use is made of numbers, LL can capture many of the properties of likelihood in an intuitively appealing way. Using standard techniques of modal logic, we give a complete axiomatization for LL and show that satisfiability of LL formulas can be decided in exponential time. We discuss how the logic might be used in areas where decision making is crucial, such as management and medical diagnosis, and conclude by using LL to give a formal proof of correctness of a protocol for exchanging secrets."
1982,The Choice Coordination Problem.,"In the course of a concurrent computation, processes†P1,...,†Pn†must reach a common choice of one out of†k†alternatives†A†1,...,†A†k. They do this by protocols using k shared variables, one for each alternative. If the range of the variables has m values then†12n???3?m12n3?m†is necessary, and†n†+ 2?m†is sufficient, for deterministic protocols solving the choice coordination problem (C.C.P.). We introduce very simple randomizing protocols which, independently of n, solve the C.C.P. by use of a fixed alphabet. A single-byte (256-valued) alphabet permits a solution with non-termination probability smaller than 2?127. Many software and hardware tasks involving concurrency can be interpreted as choice coordination problems. Choice coordination problems occur also in nature."
1982,N-Process Mutual Exclusion with Bounded Waiting by 4 Log_2 N-Valued Shared Variable.,"Abstract
The problem of implementing mutual exclusion of N asynchronous parallel processes in a model where the primitive communication mechanism is a test-and-set operation on a shared variable has been the subject of extensive research. While a two-valued variable suffices to insure mutual exclusion, it is shown in (Burns, et al., J. Assoc. Comput. Mach. 9 (1982)) that N/2 values are necessary to avoid lockout of any process, and N + 1 values are required to ensure bounded waiting time. We introduce the idea of employing randomization in the mutual exclusion protocol and achieve a mutual exclusion, and with probability 1 lockout-free and bounded-waiting, solution using just a 4 ¬∑ log2 N-valued shared variable. The protocol is extremely simple, easy to implement, and avoids certain undesirable features present in some of the other solutions. The protocols of the processes are identical and this symmetry is preserved throughout the computation. In particular, unlike (Burns, et al., J. Assoc. Comput. Mach. 9 (1982); Cremers and Hibbard, ‚ÄúMutual Exclusion of N Processors using O(N)-Valued Message Variable‚Äù, Extended Abstract, University of Southern California, 1977), no single process ever becomes, even temporarily, controller of the computation, which would make everything depend on it."
1981,On the Advantages of Free Choice: A Symmetric and Fully Distributed Solution to the Dining Philosophers Problem.,"It is shown that distributed systems of probabilistic processors are essentially more powerful than distributed systems of deterministic processors, i.e., there are certain useful behaviors that can be realized only by the former. This is demonstrated on the dining philosophers problem. It is shown that, under certain natural hypotheses, there is no way the philosophers can be programmed (in a deterministic fashion) so as to guarantee the absence of deadlock (general starvation). On the other hand, if the philosophers are given some freedom of choice one may program them to guarantee that every hungry philosopher will eat (with probability one) under any circumstances (even an adversary scheduling). The solution proposed here is fully distributed and does not involve any central memory or any process with which every philosopher can communicate."
1980,Probabilistic Algorithms in Finite Fields.,"We present probabilistic algorithms for the problems of finding an irreducible polynomial of degree n over a finite field, finding roots of a polynomial, and factoring a polynomial into its irreducible factors over a finite field. All of these problems are of importance in algebraic coding theory, algebraic symbol manipulation, and number theory. These algorithms have a very transparent, easy to program structure. For finite fields of large characteristic p, so that exhaustive search through ${\text{Z}}_p $, is not feasible, our algorithms are of lower order in the degrees of the polynomial and fields in question, than previously published algorithms.

"
1980,N-Process Synchronization by 4 log _2 N-Valued Shared Variables.,"Abstract:
The problem of implementing mutual exclusion of N asynchronous parallel processes in a model where the primitive communication mechanism is a test-and-set operation on a shared variable, was the subject of extensive research. While a two-valued variable suffices to insure mutual exclusion, it is shown in [1] that N/2 values are necessary to avoid lockout of any process, and N + 1 values are required to insure bounded waiting time. We introduce the idea of employing randomization in the synchronization protocol and achieve a mutual exclusion, lockout-free, bounded-waiting solution using just 4(log2N+4)-valued shared variable. The protocol is extremely simple, easy to implement, and avoids certain undesirable features present in some of the other solutions."
1977,Complexity of Computations.,"The framework for research in the theory of complexity of computations is described, emphasizing the interrelation between seemingly diverse problems and methods. Illustrative examples of practical and theoretical significance are given. Directions for new research are discussed."
1974,Theoretical Impediments to Artificial Intelligence.,n/a
1974,A Characterization of the Power of Vector Machines.,"Random access machines (RAMs) are usually defined to have registers that hold integers. While this captures in part the structure of a commercial computer, it overlooks an implementation-dependent feature of most binary oriented machines, namely their ability to operate bit by bit on the bit vectors used to represent integers. Typical operations are bit-wise Boolean operations (and, or, not, etc.) and shifts by an amount specified in some register. These operations are ideal for certain problems, such as dealing with sets represented as bit vectors, some parsing algorithms [4], propositional calculus theorem proving, and analysis of sorting networks. A RAM so implemented we shall call a vector machine."
1972,Proving Simultaneous Positivity of Linear Forms.,n/a
1972,Solving Linear Equations by Means of Scalar Products.,"Abstract
We shall state and solve a problem proposed by R. Brent and P. Wolfe concerning the solution of a system of linear equations by algorithms where the only operation permitted on the coefficient vectors a1, a2, ..., is the formation of scalar products ai ¬∑v. Each such calculation is called a test. The objective is to solve the equations using a minimum number of tests."
1971,Meeting of the Association for Symbolic Logic.,n/a
1966,Decidability and Undecidability of Extensions of Second (First) Order Theory of (Generalized) Successor.,n/a
1963,Probabilistic Automata.,"Probabilistic automata (p.a.) are a generalization of finite deterministic automata. We follow the formulation of finite automata in Rabin and Scott (1959) where the automata
have two-valued output and thus can be viewed as defining the set T(
) of all tapes accepted by
. This involves no loss of generality. A p.a. is an automaton which, when in state s and when input is œÉ, has a probability pi(s, œÉ)} of going into any state si. With any cut-point 0 ‚â§ Œª < 1, there is associated the set T
of tapes accepted by
with cut-point Œª.
Here we develop a general theory of p.a. and solve some of the basic problems. Aside from the mathematical interest in pursuing this natural generalization of finite automata, the results also bear on questions of reliability of sequential circuits.
P.a. are, in general, stronger than deterministic automata (Theorem 2). By studying the way we may want to use p.a. we are led to introduce the concept of isolated cut-point. It turns out that every p.a. with isolated cut-point is equivalent to a suitable deterministic automaton (the Reduction Theorem 3). It is interesting to note that in passing from a minimal deterministic automaton to an equivalent p.a. we can sometimes save states (Section VII).
The Reduction Theorem is applied to prove the existence of an approximate calculation procedure for a calculation problem involving products of stochastic matrices (Section VIII). The problem is of a new kind in that there is no a-priori bound on the number of operations (matrix multiplications) which we may have to perform and therefore classical numerical estimates of round-off errors do not apply.
Actual automata (Definition 9) have the property, often existing in actual unreliable circuits, that all transition probabilities are strictly positive. Actual automata are proved to give only definite events. This points to the restrictions we may have to impose on a probabilistic sequential circuit if we want it to perform general tasks, namely, some transitions should be prohibited.
Finally we treat the important problem of stability. Is the operation of a p.a. stable (unchanged) under small enough perturbations of the transition probabilities? We have an affirmative answer to this question in the case of actual automata (Theorem 11) and we discuss the problem for the general case."
1963,Words in the History of a Turing Machine with a Fixed Input.,n/a
1963,The Theory of Definite Automata.,"Abstract:
A definite automaton is, roughly speaking, an automaton (sequential circuit) with the property that for some fixed integer k its action depends only on the last k inputs. The notion of a definite event introduced by Kleene, as well as the related concepts of definite automata and tables, are studied here in detail. Basic results relating to the minimum number of states required for synthesizing an automaton of a given degree of definiteness are proved. We give a characterization of all k-definite events definable by k+1 state automata. Various decision problems pertaining to definite automata are effectively solved. We also solve effectively the problem of synthesizing a minimal automaton defining a given definite event. The solutions of decision and synthesis problems given here are practical in the sense that if the problem is presented by n units of information, then the algorithm in question requires about n3 steps of a very elementary nature (rather than requiring about 2n steps as some algorithms for automata do, which puts them beyond the capacity of the largest computers even for relatively small values of n). A notion of equivalence of definite events is introduced and the uniqueness of the minimal automaton defining an event in an equivalence class is proved."
1959,Finite Automata and Their Decision Problems.,"Abstract:
Finite automata are considered in this paper as instruments for classifying finite tapes. Each one-tape automaton defines a set of tapes, a two-tape automaton defines a set of pairs of tapes, et cetera. The structure of the defined sets is studied. Various generalizations of the notion of an automaton are introduced and their relation to the classical automata is determined. Some decision problems concerning automata are shown to be solvable by effective algorithms; others turn out to be unsolvable by algorithms."
1959,On Codes for Checking Logical Operations.,"Abstract:
Two types of codes for checking logical operations digit by digit on two vectors of binary digits are studied. The first type attaches a check symbol to each vector of binary digits and requires that the check symbol for the logical function of two vectors can be determined from the check symbols of the two input vectors. The second type of coding is ordinary block coding into vectors of binary digits, with the added requirement that the coded vectors be processed digit by digit. The constraints on the codes resulting from the assumptions for the coding system are studied by typical algebraic arguments. It is shown that for both types of coding and for all nontrivial logical functions of two variables, except ‚Äúexclusive or‚Äù and its complement, there is no system of checking simpler than duplication. For ‚Äúexclusive or‚Äù and its complement, group alphabets can be used, and for the block coding these are the only codes which can be used."
1958,On Recursively Enumerable and Arithmetic Models of Set Theory.,"In this note we shall prove a certain relative recursiveness lemma concerning countable models of set theory (Lemma 5). From this lemma will follow two results about special types of such models.

Kreisel [5] and Mostowski [6] have shown that certain (finitely axiomatized) systems of set theory, formulated by means of the ? relation and certain additional non-logical constants, do not possess recursive models. Their purpose in doing this was to construct consistent sentences without recursive models. As a first corollary of Lemma 5, we obtain a very simple proof, not involving any formal constructions within the system of the notions of truth and satisfiability, of an extension of the Kreisel-Mostowski theorems. Namely, set theory with the single non-logical constant ? does not possess any recursively enumerable model. Thus we get, as a side product, an easy example of a consistent sentence containing a single binary relation which does not possess any recursively enumerable model; this sentence being the conjunction of the (finitely many) axioms of set theory."
