2000,Five seconds or sixty? Presentation time in expert memory.,"For many years, the game of chess has provided an invaluable task environment for research on cognition, in particular on the differences between novices and experts and the learning that removes these differences, and upon the structure of human memory and its paramaters. The template theory presented by Gobet and Simon based on the EPAM theory offers precise predictions on cognitive processes during the presentation and recall of chess positions. This article describes the behavior of CHREST, a computer implementation of the template theory, in a memory task when the presentation time is varied from one second to sixty, on the recall of game and random positions, and compares the model to human data. Strong players are better than weak players in both types of positions, especially with long presentation times, but even after brief presentations. CHREST predicts the data, both qualitatively and quantitatively. Strong players' superiority with random positions is explained by the large number of chunks they hold in LTM. Their excellent recall with short presentation times is explained by templates, a special class of chunks. CHREST is compared to other theories of chess skill, which either cannot account for the superiority of Masters in random positions or predict too strong a performance of Masters in such positions.
"
1999,editorial: Cognitive modeling in perspective.,n/a
1999,A Time for Talk and a Time for Silence.,"Abstract
The computer stores mountains of information which it communicates worldwide through an enormous bandwidth. We must learn to exercise severe, intelligent selectivity in mining our data mountains, and to communicate information in ways that will inform and not bury the recipients.
This is todayâ€™s task of organizational design. Organizing combines human efforts efficiently, dividing the undertaking into separate but interdependent tasks and securing good coordination in their performance. An effective organization and its buildings balance opportunity for reflective deliberation against opportunity for mutual exchange of ideas and information. That balance is lost if talk drowns out silence. In our time, silence is unlikely to drown out talk."
1998,Discovering Explanations.,n/a
1998,Human and Machine Interpretation of Expressions in Formal Systems.,"Abstract
This paper uses a proof of GÃ¶dels theorem, implemented on a computer, to explore how a person or a computer can examine such a proof, understand it, and evaluate its validity. It is argued that, in order to recognize it (1) as GÃ¶del's theorem, and (2) as a proof that there is an undecidable statement in the language of PM, a person must possess a suitable semantics. As our analysis reveals no differences between the processes required by people and machines to understand GÃ¶del's theorem and manipulate it symbolically, an effective way to characterize this semantics is to model the human cognitive system as a Turing Machine with sensory inputs.
La logistique n'est plus stÃ©rile: elle engendre la contradicion! â€“ Henri PoincarÃ© â€˜Les mathematiques et la logiqueâ€™"
1997,Scientific Discovery and Simplicity of Method.,n/a
1997,The future of information systems.,"Abstract
Today, the improvement of organizations and the information systems in them is not a matter of making more information available, but of conserving scarce human attention so that it can focus on the information that is most important and most relevant to the decisions that have to be made. To do this, we must draw upon OR and artificial intelligence, but also on what has been learned in the past forty years of cognitive psychology about the informa-tion capabilities of human beings."
1997,The Implications of Kasparov vs. Deep Blue.,
1997,Collaborative Discovery in a Scientific Domain.,n/a
1997,CaMeRa: A Computational Model of Multiple Representations.,n/a
1997,Logic and Thought.,n/a
1995,Artificial Intelligence: An Empirical Science.,n/a
1995,Applications of Machine Learning and Rule Induction.,"Machine learning is the study of computational methods for improving performance by mechanizing the acquisition of knowledge from experience. Expert performance requires much domain-specific knowledge, and knowledge engineering has produced hundreds of AI expert systems that are now used regularly in industry. Machine learning aims to provide increasing levels of automation in the knowledge engineering process, replacing much time-consuming human activity with automatic techniques that improve accuracy or efficiency by discovering and exploiting regularities in training data. The ultimate test of machine learning is its ability to produce systems that are used regularly in industry, education, and elsewhere."
1995,Internal Representation and Rule Development in Object-Oriented Design.,"This article proposes a cognitive framework describing the software development process in object-oriented design (OOD) as building internal representations and developing rules. Rule development (method construction) is performed in two problem spaces: a rule space and an instance space. Rules are generated, refined, and evaluated in the rule space by using three main cognitive operations: Infer, Derive, and Evoke. Cognitive activities in the instance space are called mental simulations and are used in conjunction with the Infer operation in the rule space. In an empirical study with college students, we induced different representations to the same problem by using problem isomorphs. Initially, subjects built a representation based on the problem description. As rule development proceeded, the initial internal representation and designed objects were refined, or changed if necessary, to correspond to knowledge gained during rule development. Differences in rule development processes among groups created final designs that are radically different in terms of their level of abstraction and potential reusability. The article concludes by discussing the implications of these results for object-oriented design."
1995,Models of test selection.,"Abstract:
Complex systems such as computers, aerospace systems, etc., are often tested by using a sequence of tests to exercise the functionality of the system. If the system fails a test, an error message is generated, initiating the test selection (TS) phase. The troubleshooter must decide whether or not to run more tests. Should the troubleshooter decide to conduct more tests, a test must be chosen as it may no longer be useful to conform to the predefined sequence. While in the TS phase, the troubleshooter will repeatedly make these decisions until he is done. The authors present a domain-independent framework to automate TS that is based on two computational models, TI and TP. Both models are needed for the authors show there are applications for which one model performs well, while the other model performs poorly. The use of the appropriate model for automating TS is indicated by certain characteristics of the testing sequence and the system under test.< >"
1995,"Explaining the Ineffable: AI on the Topics of Intuition, Insight and Inspiration.","Artificial intelligence methods may be used to model human intelligence or to build intelligent (expert) computer systems. AI has already reached the stage of human simulation where it can model such ""ineffable"" phenomena as intuition, insight and inspiration. This paper reviews the empirical evidence for these capabilities."
1994,Causality and Model Abstraction.,"Much of science and engineering is concerned with characterizing processes by equations describing the relations that hold among parameters of objects and govern their behavior over time. In formal descriptions of processes in terms of parameters and equations, the notion of causality is rarely made explicit. Formal treatments of the foundations of sciences have avoided discussions of causation and spoken only of functional relations among variables. Nevertheless, the notion of causality plays an important role in our understanding of phenomena. Even when we describe the behavior of a system formally in terms of acausal, mathematical relations, we often give an informal, intuitive explanation of why the system behaves the way it does in terms of cause-effect relations. In this paper, we will present an operational definition of causal ordering. The definition allows us to extract causal dependency relations among variables implicit in a model of a system, when a model is represented as a set of acausal, mathematical relations. Our approach is based on the theory of causal ordering first presented by Simon [22]. The paper shows how to use the theory and its extension in reasoning about physical systems. Further, the paper studies the relation of the theory to the problems of model aggregation.


"
1994,Reply to Touretzky and Pomerleau: Reconstructing Physical Symbol Systems.,n/a
1994,What you see is what you get - but do you get what you see?,"Visualization can be very powerful. But can we over-visualize? This paper describes three experiments that show that visualization without proper grounding in the underlying knowledge base could be detrimental to understanding. Although we concentrate on graphs, the conclusions should hold for diagrams and icons as well. Visualization needs to be seen as but one aspect of what is needed to understand a concept. The visual aspect of a concept can be extremely helpful and enlightening -but without thorough connections to its non-visual aspects, such as verbally expressed causat mechanisms, it can be but so many lines on paper"
1993,"Retrospective on ""Causality in Device Behavior"".",n/a
1993,Anecdotes-a very early expert system.,"Abstract:
A seldom-mentioned set of expert systems that have some claim to be regarded as among the first artificial intelligence programs that were implemented and used to perform a professional task at an expert level is described. The set of programs was developed for electric motor, generator, and transformer design. They were written and used by the Westinghouse Electric Corporation at least as early as 1956.< >"
1993,Situated Action: A Symbolic Interpretation.,"The congeries of theoretical views collectively referred to as “situated action” (SA) claim that humans and their interactions with the world cannot be understood using symbol?system models and methodology, but only by observing them within real?world contexts or building nonsymbolic models of them. SA claims also that rapid, real?time interaction with a dynamically changing environment is not amenable to symbolic interpretation of the sort espoused by the cognitive science of recent decades. Planning and representation, central to symbolic theories, are claimed to be irrelevant in everyday human activity. We will contest these claims, as well as their proponents' characterizations of the symbol?system viewpoint. We will show that a number of existing symbolic systems perform well in temporally demanding tasks embedded in complex environments, whereas the systems usually regarded as exemplifying SA are thoroughly symbolic (and representational), and, to the extent that they are limited in these respects, have doubtful prospects for extension to complex tasks. As our title suggests, we propose that the goals set forth by the proponents of SA can be attained only within the framework of symbolic systems. The main body of empirical evidence supporting our view resides in the numerous symbol systems constructed in the past 35 years that have successfully simulated broad areas of human cognition.

"
1993,Situated Action: Reply to Reviewers.,n/a
1993,Situated Action: Reply to William Clancey.,n/a
1993,Scientific Model-Building as Search in Matrix Spaces.,"Many reported discovery systems build discrete models of hidden structure, properties, or processes in the diverse fields of biology, chemistry, and physics. We show that the search spaces underlying many well-known systems are remarkably similar when re-interpreted as search in matrix spaces. A small number of matrix types are used to represent the input data and output models. Most of the constraints can be represented as matrix constraints; most notably, conservation laws and their analogues can be represented as matrix equations. Typically, one or more matrix dimensions grow as these systems consider more complex models after simpler models fail, and we introduce a notation to express this. The novel framework of matrix-space search serves to unify previous systems and suggests how at least two of them can be integrated. Our analysis constitutes an advance toward a generalized account of model-building in science.
"
1993,Artificial Intelligence as an Experimental Science (Abstract).,"The journal Artificial Intelligence has experienced a rather steady drift, in recent years, from articles describing and evaluating specific computer programs that exhibit intelligence to formal articles that prove theorems about intelligence. This trend raises basic questions about the nature of theory in artificial intelligence and the appropriate form for a mature science of this discipline. During the past 35 years of AI’s history, the vast bulk of our understanding of machine intelligence has derived from experimenting: constructing innumerable programs that exhibit such intelligence, and examining and analyzing their performance. Theory has been induced by identifying components and processes that are common to many of the programs, and broad generalizations about them. Some of this theory is formal, but most takes the form of laws of qualitative structure. In this respect, artificial intelligence resembles other empirical sciences like molecular biology or geophysics much more than mathematics. Computers, however ""artificial,"" are real objects the complexity of whose behavior cannot be captured fully in simple formalisms. There are no ""Three Laws of Motion"" of AI. This talk examines the forms that theory has taken (and will take) in artificial intelligence, and shows why the progress of the discipline would be stifled by a premature or excessive preoccupation with formalizations derivable from logic and mathematics.
"
1993,Causality in Bayesian Belief Networks.,"We address the problem of causal interpretation of the graphical structure of Bayesian belief networks (BBNs). We review the concept of causality explicated in the domain of structural equations models and show that it is applicable to BBNs. In this view, which we call mechanism-based, causality is defined within models and causal asymmetries arise when mechanisms are placed in the context of a system. We lay the link between structural equations models and BBNs models and formulate the conditions under which the latter can be given causal interpretation."
1992,Directions for Qualitative Reasoning.,"Sacks & Doyle provide an excellent overview of the fundamental limitations of the SPQR representations for reasoning about the qualitative properties of dynamic systems. We take this opportunity to outline some new directions for qualitative reasoning. In this paper, we provide a rigorous mathematical characterization for the term “qualitative property” in the context of static and dynamic systems. Based on these characterizations, we show that interval representations are well suited for reasoning about the qualitative properties of static systems such as qualitative comparative statics and qualitative stability. Moreover, we also show that symbolic computations help in the derivation of useful global properties of dynamic systems which can be used to guide numerical sampling of differential equations. The integration of symbolic and numeric methods provides a powerful approach for automating the qualitative analysis of differential equations.
"
1992,The Right Representation for Discovery: Finding the Conservation of Momentum.,"The representation of knowledge has a central role in the processes of discovery. A representation includes both the declarative formalisms that express knowledge and the procedural operators that are used to manipulate the formalisms. The role of representations in discovery is investigated by examining a case of law induction, the discovery of the conservation of momentum, using two different representations: (i) mathematical sentences and (ii) diagrams. The discovery is unlikely under the mathematical approach but feasible under the diagrammatic approach, as demonstrated by a computational model that simulates the discovery. Comparison of the approaches demonstrates again that the right representations are important for success in discovery and highlights the advantages in this instance of the diagrammatic representation, which include: the ability to encode information conveniently about the geometry of experimental setups; methods for comparing magnitudes of different properties; the possibility of organizing items of information deliberately to aid in making inferences; and, permitting the use of computationally inexpensive perceptual inferences.
"
1991,Nonmonotonic Reasoning and Causation: Comment.,n/a
1991,The Mathematical Bases for Qualitative Reasoning.,"Abstract:
The practices of researchers in many fields who use qualitative reasoning are summarized and explained. The goal is to gain an understanding of the formal assumptions and mechanisms that underlie this kind of analysis. The explanations given are based on standard mathematical formalisms, particularly on ordinal properties, continuous differentiable functions, and the mathematics of nonlinear dynamic systems."
1991,"Artificial Intelligence: Where Has It Been, Where is it Going?","Abstract:
The directions for near-future development of artificial intelligence (AI) can be described in terms of four dichotomies: the use of reasoning versus the use of knowledge; the roles of parallel and of serial systems; systems that perform and systems that learn to perform; and programming languages derived from the search metaphor versus languages derived from the logical reasoning metaphor. Although the author believes that there are reasons for emphasizing knowledge systems (production systems) that are serial, capable of expert performance, and designed in terms of the search metaphor, the other pathways are also important and should not be ignored. In particular, empirical work is needed in the construction and empirical testing of the performance of large systems to explore all of these branching pathways."
1990,Laboratory Replication of Scientific Discovery Processes.,"Fourteen subjects were tape?recorded while they undertook to find a law to summarize numerical data they were given. The source of the data was not identified, nor were the variables labeled semantically. Unknown to the subjects, the data were measurements of the distances of the planets from the sun and the periods of their revolutions about it—equivalent to the data used by Johannes Kepler to discover his third law of planetary motion.
Four of the 14 subjects discovered the same law as Kepler did (the period varies as the 3/2 power of the distance), and a fifth came very close to the answer. The subiects' protocols provide a detailed picture of the problem?solving searches they engaged in, which were mainly, but not exclusively, in the space of possible functions for fitting the data, and provide explanations as to why some succeeded and the others failed.
The search heuristics used by the subjects are similar to those embodied in the BACON program, computer simulation of certain scientific discovery processes. The experiment demonstrates the feasibility of examining some of the processes of scientific discovery by recreating, in the laboratory, discovery situations of substantial historical relevance. It demonstrates also, that under conditions rather similar to those of the original discoverer, a law can be rediscovered by persons of ordinary intelligence (i.e., the intelligence needed for academic success in a good university). The data for the successful subjects reveal no “creative” processes in this kind of a discovery situation different from those that are regularly observed in all kinds of problem?solving settings.
"
1990,Prediction and Prescription in Systems Modeling.,"Modeling is a principal tool for studying complex systems. Since models may be used for predictions, for analysis, or for prescription, we must ask what our goals are before we build our models. Historically, predictive numerical models have dominated our practice. Since the world we are modeling is orders of magnitude more complex than even the largest models our computers can handle, we must conserve computational power, first, by asking how much temporal detail we need and how much can be supported by available data and theories, second, by asking whether knowledge of steady states may not be more important than knowledge of temporal paths, third, by using the hierarchical properties of systems to aggregate and thereby simplify them, and, fourth, by substituting symbolic modeling, where appropriate, for numerical modeling."
1990,Optimal probe selection in diagnostic search.,"Abstract:
Probe selection (PS) in machine diagnosis is viewed as a collection of models that apply under specific conditions. This makes it possible for three polynomial-time optimal algorithms to be developed for simplified PS models that allow different probes to have different costs. The work is compared with previous research, wherein H.A. Simon and J.B. Kadane (1975) review and develop a collection of models for optimal problem-solving search. The relationship between these models and the three newly developed algorithms for PS is explored. Two of the algorithms are unlike the ones discussed by Simon and Kadane. The third cannot be related to the problem-solving models.< >"
1989,The Role of Experimentation in Scientific Theory Revision.,"In this research, we produce a program KEKADA capable of carrying out intelligent experimental programs on problems similar to those faced by a number of experimental scientists. KEKADA has a set of experimentation strategies, that were detected from the traces of the behaviors of scientists. KEKADA strategies include : focusing on a surprising phenomenon, characterizing the surprising phenomenon by general strategies such as magnification, applying divide-and-conquer, determining the scope of phenomenon, factor-analysis, relating to similar phenomena, and domain-specific strategies and hypotheses. The domain-specific heuristics in KEKADA are efficient and practical instantiations of general strategies such as - controlled experimentation, determination of complexity of a process, testing of a causal chain, componential analysis, differencing and divide-and-conquer."
1989,Rule Creation and Rule Learning Through Environmental Exploration.,"The task of learning from environment is specified. It requires the learner to infer the laws of the environment in terms of its percepts and actions, and use the laws to solve problems. Based on research on problem space creation and discrimination learning, this paper reports an approach in which exploration, rule creation and rule learning are coordinated in a single framework. With this approach, the system LIVE creates STRIPS-like rules by noticing the changes in the environment when actions are taken, and later refines the rules by explaining the failures of their predictions. Unlike many other learning systems, since LIVE treats learning and problem solving as interleaved activities, no training instance nor any concept hierarchy is necessary to start learning. Furthermore, the approach is capable of discovering hidden features from the environment when normal discrimination process fails to make any progress. The relative generality of KEKADA allows us to view the control structure of KEKADA and its domain?independent heuristics as a model of scientific experimentation that should apply over a broad domain.
"
1988,The Processes of Scientific Discovery: The Strategy of Experimentation.,"Hans Krebs' discovery, in 1932, of the urea cycle was a major event in biochemistry. This article describes a program, KEKADA, which models the heuristics Hans Krebs used in this discovery. KEKADA reacts to surprises, formulates explanations, and carries out experiments in the same manner as the evidence in the form of laboratory notebooks and interviews indicates Hans Krebs did. Furthermore, we answer a number of questions about the nature of the heuristics used by Krebs, in particular: How domain?specific are the heuristics? To what extent are they idiosyncratic to Krebs? To what extent do they represent general strategies of problem?solving search?
"
1988,Prospects for Cognitive Science.,n/a
1987,Why a Diagram is (Sometimes) Worth Ten Thousand Words.,"We distinguish diagrammatic from sentential paper?and?pencil representations of information by developing alternative models of information?processing systems that are informationally equivalent and that can be characterized as sentential or diagrammatic. Sentential representations are sequential, like the propositions in a text. Diagrammatic representations are indexed by location in a plane. Diagrammatic representations also typically display information that is only implicit in sentential representations and that therefore has to be computed, sometimes at great cost, to make it explicit for use. We then contrast the computational efficiency of these representations for solving several illustrative problems in mathematics and physics.
When two representations are informationally equivalent, their computational efficiency depends on the information?processing operators that act on them. Two sets of operators may differ in their capabilities for recognizing patterns, in the inferences they can carry out directly, and in their control strategies (in particular, the control of search). Diagrammatic and sentential representations support operators that differ in all of these respects. Operators working on one representation may recognize features readily or make inferences directly that are difficult to realize in the other representation. Most important, however, are differences in the efficiency of search for information and in the explicitness of information. In the representations we call diagrammatic, information is organized by location, and often much of the information needed to make an inference is present and explicit at a single location. In addition, cues to the next logical step in the problem may be present at an adjacent location. Therefore problem solving can proceed through a smooth traversal of the diagram, and may require very little search or computation of elements that had been implicit.
"
1986,Causality in Device Behavior.,"This paper shows how formal characterizations of causality and of the method of comparative statics, long used in econometrics, thermodynamics and other domains, can be applied to clarify and make rigorous the qualitative causal calculus recently proposed by de Kleer and Brown [2]. The formalization shows exactly what assumptions are required to carry out causal analysis of a system of interdependent variables in equilibrium and to propagate disturbances through such a system. The intuitive concepts of causality captured by de Kleer and Brown provide a rough approximation to the standard analytic techniques that are used in the treatment of simultaneous algebraic and differential equations.
"
1986,Theories of Causal Ordering: Reply to de Kleer and Brown.,"In their reply to our paper, “Causality in Device Behavior,” de Kleer and Brown seek to establish a clear product differentiation between the well-known concepts of causal ordering and comparative statics, on the one side, and their “mythical causality” and qualitative physics, on the other. Most of the differences they see, however, are invisible to our eyes. Contrary to their claim, the earlier notion of causality, quite as much as the later one, is qualitative and “derives from the relationship between the equations and their underlying components which comprise the modeled system.” The concepts of causal ordering and comparative statics offer the advantage of a formal foundation that makes clear exactly what is being postulated. Hence, they can contribute a great deal to the clarification of the causal approaches to system analysis that de Kleer and Brown are seeking to develop.
In this brief response to their comments, we discuss the source of the structural equations in the causal ordering approach, and we challenge more generally the claim that there are inherent differences (e.g., in the case of feedback) between the “engineer's” and the “economist's” approach to the study of system behavior.
"
1986,Information Processing Language V on the IBM 650.,
1986,A Theory of Historical Discovery: The Construction of Componential Models.,n/a
1986,Whether Software Engineering Needs to Be Artificially Intelligent.,"Abstract:
The author discusses the roles that humans now play versus the roles that could be taken over by artificial intelligence in developing computer systems. Also discussed is how the intelligent part of the automatic system can communicate effectively with humans. Topics covered include an artificial intelligence overview; weak methods; the heuristic search; the problem space; the knowledge base; expert systems; and conclusions drawn from a description of the general artificial intelligence paradigm."
1984,EPAM-like Models of Recognition and Learning.,"A description is provided of EPAM?III, a theory in the form of a computer program for simulating human verbal learning, along with a summary of the empirical evidence for its validity. Criticisms leveled against the theory in a recent paper by Barsalou and Bower are shown to derive largely from their misconception that EPAM?III employed a binary, rather than n?ary branching discrimination net. It is shown that Barsalou and Bower also failed to understand how the recursive structure of EPAM?III eliminates the need to duplicate test nodes that are used to recognize subobjects, and how the possibility of redundant recognition paths controls the sensitivity of EPAM to noticing order. EPAM is also compared briefly with other theories of human discrimination and discrimination learning, including PANDEMONIUM?like systems and dataflow nets.
"
1983,Search and Reasoning in Problem Solving.,n/a
1983,Three Facets of Scientific Discovery.,"Scientific discovery is a complex process, and in this paper we consider three of its many facets - discovering laws of qualitative structure, finding quantitative relations between variables, and formulating structural models of reactions. We describe three discovery systems - GLAUBER, BASCON, and DALTON - that address these three aspects of the scientific process. GLAUBER forms classes of objects based on regularities in qualitative data, and states abstract laws in terms of these classes. BACON includes heuristics for finding numerical laws, for postulating intrinsic properties, and for noting common divisors. DALTON formulates molecular models that account for observed reactions, taking advantage of theoretical assumptions to direct its search if they are available. We show how each of the programs is capable of rediscovering laws or models that were found in the early days of chemistry. Finally, we consider some possible interactions between these systems, and the need for an integrated theory of discovery"
1981,Prometheus or Pandora: The Influence of Automation on Society.,"Abstract:
Even though computer technology may challenge one of man's last and most cherished illusions that he alone is capable of thought it can also show him how to live in harmony with nature."
1981,Information-processing models of cognition.,"This article reviews recent progress in modeling human cognitive processes. Particular attention is paid to the use of computer programming languages as a formalism for modeling, and to computer simulation of the behavior of the systems modeled. Theories of human cognitive processes can be attempted at several levels: at the level of neural processes, at the level of elementary information processes (e.g., retrieval from memory, scanning down lists in memory, comparing simple symbols, etc.), or at the level of higher mental processes (e.g., problem solving, concept attainment). This article will not deal at all with neural models; it focuses mainly upon higher mental processes, but not without some attention to modeling the elementary processes and especially to the relationships between elementary and complex processes.
"
1981,BACON.5: The Discovery of Conservation Laws.,"BACON5 is a program that discovers empirical laws. The program represents information at varying levels of description, with higher levels summarizing the levels below them. The system applies a small set of data-driven heuristics to detect regularities in numeric and nominal data. These heuristics note constancies and trends, leading BACON5 to formulate hypotheses, define theoretical terms, and postulate intrinsic properties. Once the program has formulated an hypothesis, it uses this to reduce the amount of data it must consider at later times. A simple type of reasoning by analogy also simplifies the discovery of laws containing symmetric forms. These techniques have allowed the system to rediscover Snail's law of refraction, conservation of momentum, Black's specific heat law, and Joule's formulation of conservation of energy. Thus, BACON.5's heuristics appear to be general mechanisms applicable to discovery in diverse domains."
1980,Cognitive Science: The Newest Science of the Artificial.,"Cognitive science is, of course, not really a new discipline, but a recognition of a fundamental set of common concerns shared by the disciplines of psychology, computer science, linguistics, economics, epistemology, and the social sciences generally. All of these disciplines are concerned with information processing systems, and all of them are concerned with systems that are adaptive—that are what they are from being ground between the nether millstone of their physiology or hardware, as the case may be, and the upper millstone of a complex environment in which they exist. Systems that are adaptive may equally well be described as “artificial,” for as environments change, they can be expected to change too, as though they were deliberately designed to fit those environments (as indeed they sometimes are).
The task of empirical science is to discover and verify invariants in the phenomena under study. The artificiality of information processing systems creates a subtle problem in defining empirical invariants in such systems. For observed regularities are very likely invariant only within a limited range of variation in their environments, and any accurate statement of the laws of such systems must contain reference to their relativity to environmental features. It is a common experience in experimental psychology, for example, to discover that we are studying sociology—the effects of the past histories of our subjects—when we think we are studying physiology—the effects of properties of the human nervous system. Similarly, business cycle economists are only now becoming aware of the extent to which the parameters of the system they are studying are dependent on the experiences of a population with economic events over the previous generation.
In artificial sciences, the descriptive and the normative are never far apart. Thus, in economics, the “principle of rationality” is sometimes asserted as a descriptive invariant, sometimes as advice to decision makers. Similarly, in psychology, the processes of adaptation (learning) have always been a central topic, at one time a topic that dominated the whole field of research. Linguistics, too, has suffered its confusions between descriptive and normative attitudes towards its subject. But we must avoid the error, in studying information processing systems, of thinking that the adaptive processes themselves must be invariant; and we must be prepared to face the complexities of regression in the possibility that they themselves may be subject to improvement and adaptation.
It might have been necessary a decade ago to argue for the commonality of the information processes that are employed by such disparate systems as computers and human nervous systems. The evidence for that commonality is now overwhelming, and the remaining questions about the boundaries of cognitive science have more to do with whether there also exist nontrivial commonalities with information processing in genetic systems than with whether men and machines both think.
"
1980,Models of Competence in Solving Physics Problems.,"We describe a set of two computer?implemented models that solve physics problems in ways characteristic of more and less competent human solvers. The main features accounting for different competences are differences in strategy for selecting physics principles, and differences in the degree of automation in the process of applying a single principle. The models provide a good account of the order in which principles are applied by human solvers working problems in kinematics and dynamics. They also are sufficiently flexible to allow easy extension to several related domains of physics problems.
"
1977,Problem Solving in Semantically Rich Domains: An Example from Engineering Thermodynamics.,"Recent research on human problem solving has largely focused on laboratory tasks that do not demand from the subject much prior, task?related information. This study seeks to extend the theory of human problem solving to semantically richer domains that are characteristic of professional problem solving. We discuss the behavior of a single subject solving problems in chemical engineering thermodynamics. We use as a protocol?encoding device a computer program called SAPA which also doubles as a theory of the subject's problem?solving behavior. The subject made extensive use of means?ends analysis, similar to that observed in semantically less rich domains, supplemented by recognition mechanisms for accessing information in semantic memory.
"
1977,History of Artificial Intelligence.,n/a
1977,Artificial Intelligence Systems That Understand.,n/a
1976,Computer Science as Empirical Inquiry: Symbols and Search.,"Computer science is the study of the phenomena surrounding computers. The founders of this society understood this very well when they called themselves the Association for Computing Machinery. The machineâ€”not just the hardware, but the programmed, living machineâ€”is the organism we study."
1975,Optimal Problem-Solving Search: All-Oor-None Solutions.,"Optimal algorithms are derived for satisficing problem-solving search, that is, search where the goal is to reach any solution, no distinction being made among different solutions. This task is quite different from search for best solutions or shortest path solutions.
Constraints may be placed on the order in which sites may be searched. This paper treats satisficing searches through partially ordered search spaces where there are multiple alternative goals.
"
1974,'Losing move': an information processing concept.,"Chess, like all games of perfect information, is, from a game-theoretic point of view, trivial. The value of the game, as well as optimal strategies, can be calculated - in principle, of course - by minimaxing backwards from terminal positions. If, as generally believed, the game's value is <u>Drawn</u>, then it is possible for a player to make a losing move - one that changes the value from D to L - but not to make a winning move - one that changes the value from D to W. (W, D and L mean 'won', 'drawn', and 'lost' from the standpoint of the player in question.)"
1973,The Structure of Ill Structured Problems.,"The boundary between well structured and ill structured problems is vague, fluid and not susceptible to formalization. Any problem solving process will appear ill structured if the problem solver is a serial machine that has access to a very large long-term memory of potentially relevant information, and/or access to a very large external memory that provides information about the actual real-world consequences of problem-solving actions. There is no reason to suppose that new and hitherto unknown concepts or techniques are needed to enable artificial intelligence systems to operate successfully in domains that have these characteristics.
"
1973,Lessons from Perception for Chess-Playing Programs (and vice versa).,
1971,The Theory of Problem Solving.,"It is now about fifteen years since the first computer programs were written and tested that used the method of heuristic search to solve problems. Dozens of such programs, some designed for specific task domains, others claiming various degrees of generality, have now been described in the literature, and many experiments with their performance have been reported and analysed. It is an appropriate time to ask what has been learned from these experiments about the general theory of problem solving, and to try to summarize the present state of that theory.
"
1966,A chess mating combinations program.,"The program reported here is not a complete chess player; it does not play games. Rather, it is a chess analyst limited to searching for checkmating combinations in positions containing tactical possibilities. A combination in chess is a series of forcing moves with sacrifice that ends with an objective advantage for the active side. A checkmating combination, then, is a combination in which that objective advantage is checkmate. Thus the program described here---dubbed MATER---given a position, proceeds by generating that class of forcing moves that put the enemy King in check or threaten mate in one move, and then by analyzing first those moves that appear most promising."
1963,Experiments with a Heuristic Compiler.,"This report describes some experiments in constructing a compiler that makes use of heuristic problem~solving techniques such as those incorporated in the General Problem Solver (GPS) [1]. The experiments were aimed at the dual objectives of throwing light on some of the problems of constructing more powerful programming languages and compilers, and of testing whether the task of writing a computer program can be regarded as a ""problem"" in the sense in which that term is used in GPS. The present paper is concerned primarily with the second objective--with analyzing some of the problem-solving processes that are involved in writing computer programs. At the present stage of their development, no claims will be made for the heuristic programming procedures described here as practical approaches to the construction of compilers. Their interest lies in what they teach us about the nature of the programming task."
1962,Simulation of human verbal learning behavior.,n/a
1962,Generalization of an Elementary Perceiving and Memorizing Machine.,"The Elementary Perceiver and Memorizer (EPAM) is a computer model of human associative memory and the processes of verbal learning. Analysis of the failures of one earlier version of the model (EPAM II) to simulate certain features of human verbal learning behaviour led the authors to formulate a more general model of verbal learning processes (EPAM III), which is discussed in this paper. In this model, EPAM information processes and structures have been generalized to deal with stimulus objects of arbitrary complexity. Discrimination processes discriminate objects on the basis of properties of the objects themselves or on the basis of properties of constituent subobjects. The cue-token is an associative link between the references to an object and the image of that object stored in the association memory. The learning processes of EPAM III provide an associative mechanism by means of which earlier learning can be brought to bear in a useful way on later learning. Exploration of the EPAM III model is concerned with simulating the behaviour observed in psychological experiments on meaningfulness in verbal learning. "
1961,"Reply to ""Final Note"" by Benoit Mandelbrot.","Dr. Mandelbrot's original objections (1959) to using the Yule process to explain the phenomena of word frequencies were refuted in Simon (1960), and are now mostly abandoned. The present “Reply” refutes the almost entirely new arguments introduced by Dr. Mandelbrot in his “Final Note,” and demonstrates again the adequacy of the models in 1955.
"
1961,Reply to Dr. Mandelbrot's Post Scriptum.,"Dr. Mandelbrot has proposed a new set of objections to my 1955 models of the Yule distribution. Like his earlier objections, these are invalid.
"
1960,Some Further Notes on a Class of Skew Distribution Functions.,"This note takes issue with a recent criticism by Dr. B. Mandelbrot of a certain stochastic model to explain word-frequency data. Dr. Mandelbrot's principal empirical and mathematical objections to the model are shown to be unfounded. A central question is whether the basic parameter of the distributions is larger or smaller than unity. The empirical data show it is almost always very close to unity, sometimes slightly larger, sometimes smaller. Simple stochastic models can be constructed for either case, and give a special status, as a limiting case, to instances where the parameter is unity. More generally, the empirical data can be explained by two types of stochastic models as well as by models assuming efficient information coding. The three types of models are briefly characterized and compared."
1959,Report on a general problem-solving program.,"This paper reports on a computer program, called GPS-I for General Problem Solving Program I. Construction and investigation of this program is part of a research effort by the authors to understand the information processes that underlie human intellectual, adaptive, and creative abilities. The approach is synthetic - to construct computer programs that can solve problems requiring intelligence and adaptation, and to discover which varieties of these programs can be matched to data on human problem solving. GPS - I grew out of an earlier program, the Logic Theorist, which discovers proofs to theorems in the sentential calculus. GPS-I is an attempt to fit the recorded behavior of college students trying to discover proofs. The purpose of this paper is not to relate the program to human behaviour, but to describe its main characteristics and to assess its capacities as a problem-solving mechanism. The paper will present enough theoretical discussion of problem-solving activity so that the program can be seen as an attempt to advance our basic knowledge of intellectual activity. The program will be assessed from this point of view, rather than whether it offers an economical solution to a significant class of problems. The major features of the program that are worthy of discussion are: the recursive nature of its prolem-solving activity, the separation of problem content from problem-solving technique as a way of increasing the generality of the program, the two general problem-solving techniques that now constitute its repertoire: means-ends analysis, and planning. The memory and program organization used to mechanize the program (this will be noted only briefly, since there will be no space to describe the computer languages (IPL's) used to code GPS-I). Examples will be given of how GPS solves problems in the areas of elementary symbolic logic and elementary algebra."
1958,Chess-Playing Programs and the Problem of Complexity.,"Abstract:
This paper traces the development of digital computer programs that play chess. The work of Shannon, Turing, the Los Alamos group, Bernstein, and the authors is treated in turn. The efforts to program chess provide an indication of current progress in understanding and constructing complex and intelligent mechanisms."
1956,The logic theory machine-A complex information processing system.,"Abstract:
In this paper we describe a complex information processing system, which we call the logic theory machine, that is capable of discovering proofs for theorems in symbolic logic. This system, in contrast to the systematic algorithms that are ordinarily employed in computation, relies heavily on heuristic methods similar to those that have been observed in . human problem solving activity. The specification is written in a formal language, of the nature of a pseudo-code, that is suitable for coding for digital computers. However, the present paper is concerned exclusively with specification of the system, and not with its realization in a computer. The logic theory machine is part of a program of research to understand complex information processing systems by specifying and synthesizing a substantial variety of such systems for empirical study."
1954,The Control of Inventories and Production Rates - A Survey.,"The inventory and production control problem encompasses purchasing and ordering decisions, production-rate decisions, and scheduling decisions. Over the past twenty-five years there has been considerable study of ordering decisions, under “static” assumptions, and workable decision rules are available. Work on “dynamic” rules is still in an early stage, but a general framework of analysis, and several methods of approach have been developed. Production rate decisions have been investigated from the standpoint of servomechanism “feedback” theory, the calculus of variations, and several forecasting techniques. A combination of these approaches promises to yield simple, practical decision rules. The most urgent current research problem is to devise workable decision-making procedures for complex interrelated industrial situations by finding methods for “coupling” the decision rules that apply to the individual parts of the system. Industry has made some use of the “static” decision rules now available in the literature; there has yet been little application of the dynamic ordering and production rate rules.
"
