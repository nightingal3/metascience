2010,Wireless sensor networks for soil science.,"Wireless sensor networks can revolutionise soil ecology by providing measurements at temporal and spatial granularities previously impossible. This paper presents our first steps towards fulfilling that goal by developing and deploying two experimental soil monitoring networks at urban forests in Baltimore, MD. The nodes of these networks periodically measure soil moisture and temperature and store the measurements in local memory. Raw measurements are incrementally retrieved by a sensor gateway and persistently stored in a database. The database also stores calibrated versions of the collected data. The measurement database is available to third-party applications through various Web Services interfaces. At a high level, the deployments were successful in exposing high-level variations of soil factors. However, we have encountered a number of challenging technical problems: need for low-level programming at multiple levels, calibration across space and time, and sensor faults. These problems must be addressed before sensor networks can fulfil their potential as high-quality instruments that can be deployed by scientists without major effort or cost."
2008,The Catalog Archive Server Database Management System.,"Abstract:
The multiterabyte Sloan Digital Sky Survey's (SDSS's) catalog data is stored in a commercial relational database management system with SQL query access and a built-in query optimizer. The SDSS catalog archive server adds advanced data mining features to the DBMS to provide fast online access to the data."
2008,The sqlLoader Data-Loading Pipeline.,"Abstract:
Using a database management system (DBMS) is essential to ensure the data integrity and reliability of large, multidimensional data sets. However, loading multiterabyte data into a DBMS is a time-consuming and error-prone task that the authors have tried to automate by developing the sqlLoader pipeline-a distributed workflow system for data loading."
2008,Distributed Computing Economics.,"Computing economics are changing. Today there is rough price parity between: (1) one database access; (2) 10 bytes of network traffic; (3) 100,000 instructions; (4) 10 bytes of disk storage; and (5) a megabyte of disk bandwidth. This has implications for how one structures Internet-scale distributed computing: one puts computing as close to the data as possible in order to avoid expensive network traffic.
"
2008,Flash Disk Opportunity for Server Applications.,"Future flash-based disks could provide breakthroughs in IOPS, power, reliability, and volumetric capacity when compared with conventional disks.
"
2007,Data Management in the Worldwide Sensor Web.,"Abstract:
Harvesting the benefits of a sensor-rich world presents many data management challenges. Recent advances in research and industry aim to address these challenges. With the rapidly increasing number of large-scale sensor network deployments, the vision of a worldwide sensor Web is close to becoming a reality."
2006,Petascale Computational Systems.,"Abstract:
A balanced cyberinfrastructure is necessary to meet growing data-intensitive scientific needs. We believe that available resources should be allocated to benefit the broadest cross-section of the scientific community. Given the power-law distribution of problem sizes, this means that about half of funding agency resources should be spent on tier-1 centers at the petascale level and the other half dedicated to tier-2 and tier-3 centers on a cost-sharing basis. Funding agencies should support balanced systems, not just CPU farms, as well as petascale IO and networking. They should also allocate resources for a balanced tier-1 through tier-3 cyberinfrastructure."
2006,Designing and Building TerraService.,"Abstract:
A few simple rules guide the design of Web services such as TerraService, a geospatial service added to Microsoft's popular TerraServer database. By sticking to standards-based tools, the authors were able to implement the Web service with no major structural changes to the database. Because it conforms to the ""1-to-10 Kbytes in one second"" guideline, TerraService offers a highly acceptable user experience. The success of two US Department of Agriculture applications that bridge the Web service and the database demonstrates the astuteness of the authors' design principles"
2006,Consensus on transaction commit.,"The distributed transaction commit problem requires reaching agreement on whether a transaction is committed or aborted. The classic Two-Phase Commit protocol blocks if the coordinator fails. Fault-tolerant consensus algorithms also reach agreement, but do not block whenever any majority of the processes are working. The Paxos Commit algorithm runs a Paxos consensus algorithm on the commit/abort decision of each participant to obtain a transaction commit protocol that uses 2F + 1 coordinators and makes progress if at least F + 1 of them are working properly. Paxos Commit has the same stable-storage write delay, and can be implemented to have the same message delay in the fault-free case as Two-Phase Commit, but it uses more messages. The classic Two-Phase Commit algorithm is obtained as the special F = 0 case of the Paxos Commit algorithm.
"
2006,"Triumphs, Sins, and Challenges of Database Benchmarking.",n/a
2006,Memory - A memory model for scientific algorithms on graphics processors.,"ABSTRACT
We present a memory model to analyze and improve the performance of scientific algorithms on graphics processing units (GPUs). Our memory model is based on texturing hardware, which uses a 2D block-based array representation to perform the underlying computations. We incorporate many characteristics of GPU architectures including smaller cache sizes, 2D block representations, and use the 3C's model to analyze the cache misses. Moreover. we present techniques to improve the performance of nested loops on GPUs. In order to demonstrate the effectiveness of our model, we highlight its performance on three memory-intensive scientific applications - sorting, fast Fourier transform and dense matrix-multiplication. In practice, our cache-efficient algorithms for these applications are able to achieve memory throughput of 30-50 GB/s on a NVIDIA 7900 GTX GPU. We also compare our results with prior GPU-based and CPU-based implementations on high-end processors. In practice, we are able to achieve 2-5 x performance improvement."
2006,Data analysis tools for sensor-based science.,"ABSTRACT
Science is increasingly driven by data collected automatically from arrays of inexpensive sensors. The collected data volumes require a different approach from the scientists' current Excel spreadsheet storage and analysis model. Spreadsheets work well for small data sets; but scientists want high level summaries of their data for various statistical analyses without sacrificing the ability to drill down to every bit of the raw data. This demonstration describes our prototype data analysis system that is suitable for browsing and visualization - like a spreadsheet - but scalable to much larger data sets."
2006,GPUTeraSort: high performance graphics co-processor sorting for large database management.,"ABSTRACT
We present a novel external sorting algorithm using graphics processors (GPUs) on large databases composed of billions of records and wide keys. Our algorithm uses the data parallelism within a GPU along with task parallelism by scheduling some of the memory-intensive and compute-intensive threads on the GPU. Our new sorting architecture provides multiple memory interfaces on the same PC -- a fast and dedicated memory interface on the GPU along with the main memory interface for CPU computations. As a result, we achieve higher memory bandwidth as compared to CPU-based algorithms running on commodity PCs. Our approach takes into account the limited communication bandwidth between the CPU and the GPU, and reduces the data communication between the two processors. Our algorithm also improves the performance of disk transfers and achieves close to peak I/O performance. We have tested the performance of our algorithm on the SortBenchmark and applied it to large databases composed of a few hundred Gigabytes of data. Our results on a 3 GHz Pentium IV PC with $300 NVIDIA 7800 GT GPU indicate a significant performance improvement over optimized CPU-based algorithms on high-end PCs with 3.6 GHz Dual Xeon processors. Our implementation is able to outperform the current high-end PennySort benchmark and results in a higher performance to price ratio. Overall, our results indicate that using a GPU as a co-processor can significantly improve the performance of sorting algorithms on large databases."
2005,The Lowell database research self-assessment.,"Database needs are changing, driven by the Internet and increasing amounts of scientific and sensor data. In this article, the authors propose research into several important new directions for database management systems.
"
2005,"A ""Measure of Transaction Processing"" 20 Years Later.",n/a
2005,A call to arms.,"Long anticipated, the arrival of radically restructured database architectures is now finally at hand.
"
2005,Scientific data management in the coming decade.,"Scientific instruments and computer simulations are creating vast data stores that require new scientific methods to analyze and organize the data. Data volumes are approximately doubling each year. Since these new instruments have extraordinary precision, the data quality is also rapidly improving. Analyzing this data to find the subtle effects missed by previous studies requires algorithms that can simultaneously deal with huge datasets and that can find very subtle effects --- finding both needles in the haystack and finding very small haystacks that were undetected in previous measurements.
"
2005,When Database Systems Meet the Grid.,"We illustrate the benefits of combining database systems and Grid technologies for data-intensive applications. Using a cluster of SQL servers, we reimplemented an existing Grid application that finds galaxy clusters in a large astronomical database. The SQL implementation runs an order of magnitude faster than the earlier Tcl-C-file-based implementation. We discuss why and how Grid applications can take advantage of database systems. "
2004,Where the Rubber Meets the Sky: Bridging the Gap between Databases and Science.,n/a
2004,The Revolution in Database System Architecture.,"Database system architectures are undergoing revolutionary changes. Algorithms and data are being unified by integrating programming languages with the database system. This gives an extensible object-relational system where non-procedural relational operators manipulate object sets. Coupled with this, each DBMS is now a web service. This has huge implications for how we structure applications. DBMSs are now object containers. Queues are the first objects to be added. These queues are the basis for transaction processing and workflow applica-tions. Future workflow systems are likely to be built on this core. Data cubes and online analytic processing are now baked into most DBMSs. Beyond that, DBMSs have a framework for data mining and machine learning algorithms. Decision trees, Bayes nets, clustering, and time series analysis are built in; new algorithms can be added. Text, temporal, and spatial data access methods, along with their probabilistic reasoning have been added to database systems. Allowing approximate and probabilistic answers is essential for many applications. Many believe that XML and xQuery will be the main data structure and access pattern. Database systems must accommodate that perspective.These changes mandate a much more dynamic query optimization strategy. Intelligence is moving to the periphery of the network. Each disk and each sensor will be a competent database machine. Relational algebra is a convenient way to program these systems. Database systems are now expected to be self-managing, self-healing, and always-up."
2004,The Next Database Revolution.,"ABSTRACT
Database system architectures are undergoing revolutionary changes. Most importantly, algorithms and data are being unified by integrating programming languages with the database system. This gives an extensible object-relational system where non-procedural relational operators manipulate object sets. Coupled with this, each DBMS is now a web service. This has huge implications for how we structure applications. DBMSs are now object containers. Queues are the first objects to be added. These queues are the basis for transaction processing and workflow applications. Future workflow systems are likely to be built on this core. Data cubes and online analytic processing are now baked into most DBMSs. Beyond that, DBMSs have a framework for data mining and machine learning algorithms. Decision trees, Bayes nets, clustering, and time series analysis are built in; new algorithms can be added. There is a rebirth of column stores for sparse tables and to optimize bandwidth. Text, temporal, and spatial data access methods, along with their probabilistic reasoning have been added to database systems. Allowing approximate and probabilistic answers is essential for many applications. Many believe that XML and xQuery will be the main data structure and access pattern. Database systems must accommodate that perspective. External data increasingly arrives as streams to be compared to historical data; so stream-processing operators are being added to the DBMS. Publish-subscribe systems invert the data-query ratios; incoming data is compared against millions of queries rather than queries searching millions of records. Meanwhile, disk and memory capacities are growing much faster than their bandwidth and latency, so the database systems increasingly use huge main memories and sequential disk access. These changes mandate a much more dynamic query optimization strategy - one that adapts to current conditions and selectivities rather than having a static plan. Intelligence is moving to the periphery of the network. Each disk and each sensor will be a competent database machine. Relational algebra is a convenient way to program these systems. Database systems are now expected to be self-managing, self-healing, and always-up. We researchers and developers have our work cut out for us in delivering all these features."
2004,Where the Rubber Meets the Sky: The Semantic Gap between Data Producers and Data Consumers.,"Abstract:
Summary form only given. Historically, scientists gatherer and analyzed their own data. But technology has created functional specialization where some scientists gather or generate data, and others analyze it. Technology allows us to easily capture vast amounts of empirical data and to generate vast amounts of simulated data. Technology also allows us to store these bytes almost indefinitely. But there are few tools to organize scientific data for easy access and query, few tools to curate the data, and few tools to federate science archives. Domain scientists, notably NCBI and the Virtual Observatory, are making heroic efforts to address these problems. But this is a generic problem that cuts across all scientific disciplines. It requires a coordinated effort by the computer science community to build generic tools that will help all the sciences. Our current database products are a start, but much more is needed."
2003,Migrating a multiterabyte archive from object to relational databases.,"Abstract:
A commercial, object-oriented database engine with custom tools for data-mining the multiterabyte Sloan Digital Sky Survey archive did not meet its performance objectives. We describe the problems, technical issues, and process of migrating this large data set project, to relational database technology."
2003,What next?: A dozen information-technology research goals.,n/a
2003,On-line science: the world-wide telescope as a prototype for the new computational science.,n/a
2003,The Lowell Report.,n/a
2002,What's next in high-performance computing?,"We can trace the evolution from Crays, to clusters, to supercomputing centers. But where does it go from here?
"
2002,The world-wide telescope.,"Mining vast databases of astronomical data, this new online way to see the global structure of the universe promises to be not only a wonderful virtual telescope but an archetype for the evolution of computational science.
"
2002,The SDSS skyserver: public access to the sloan digital sky server data.,"ABSTRACT
The SkyServer provides Internet access to the public Sloan Digital Sky Survey (SDSS) data for both astronomers and for science education. This paper describes the SkyServer goals and architecture. It also describes our experience operating the SkyServer on the Internet. The SDSS data is public and well-documented so it makes a good test platform for research on database algorithms and performance."
2002,Data Mining the SDSS SkyServer Database.,"An earlier paper (Szalay et. al. ""Designing and Mining MultiTerabyte Astronomy Archives: The Sloan Digital Sky Survey,"" ACM SIGMOD 2000) described the Sloan Digital Sky Survey's (SDSS) data management needs by defining twenty database queries and twelve data visualization tasks that a good data management system should support. We built a database and interfaces to support both the query load and also a website for ad-hoc access. This paper reports on the database design, describes the data loading pipeline, and reports on the query implementation and performance. The queries typically translated to a single SQL statement. Most queries run in less than 20 seconds, allowing scientists to interactively explore the database. This paper is an in-depth tour of those queries. Readers should first have studied the companion overview paper Szalay et. al. ""The SDSS SkyServer, Public Access to the Sloan Digital Sky Server Data"" ACM SIGMOND 2002."
2001,Digital immortality.,n/a
2000,Rules of Thumb in Data Engineering.,"Abstract:
This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for system design need only slight revision after 35 years-the major change being the increased use of RAM. An analysis also indicates storage should be used to cache both database and Web data to save disk bandwidth, network bandwidth, and people's time. Surprisingly, the 5-minute rule for disk caching becomes a cache-everything rule for Web caching."
2000,TerraServer: A Spatial Data Warehouse.,"ABSTRACT
Microsoft¬Æ TerraServer stores aerial, satellite, and topographic images of the earth in a SQL database available via the Internet. It is the world's largest online atlas, combining eight terabytes of image data from the United States Geological Survey (USGS) and SPIN-2. Internet browsers provide intuitive spatial and text interfaces to the data. Users need no special hardware, software, or knowledge to locate and browse imagery. This paper describes how terabytes of ‚ÄúInternet unfriendly‚Äù geo-spatial images were scrubbed and edited into hundreds of millions of ‚ÄúInternet friendly‚Äù image tiles and loaded into a SQL data warehouse. All meta-data and imagery are stored in the SQL database.
TerraServer demonstrates that general-purpose relational database technology can manage large scale image repositories, and shows that web browsers can be a good geo-spatial image presentation system."
2000,Designing and Mining Multi-Terabyte Astronomy Archives: The Sloan Digital Sky Survey.,"The next-generation astronomy digital archives will cover most of the sky at fine resolution in many wavelengths, from X-rays, through ultraviolet, optical, and infrared. The archives will be stored at diverse geographical locations. One of the first of these projects, the Sloan Digital Sky Survey (SDSS) is creating a 5-wavelength catalog over 10,000 square degrees of the sky (see http://www.sdss.org/). The 200 million objects in the multi-terabyte database will have mostly numerical attributes in a 100+ dimensional space. Points in this space have highly correlated distributions.

The archive will enable astronomers to explore the data interactively. Data access will be aided by multidimensional spatial and attribute indices. The data will be partitioned in many ways. Small tag objects consisting of the most popular attributes will accelerate frequent searches. Splitting the data among multiple servers will allow parallel, scalable I/O and parallel data analysis. Hashing techniques will allow efficient clustering, and pair-wise comparison algorithms that should parallelize nicely. Randomly sampled subsets will allow de-bugging otherwise large queries at the desktop. Central servers will operate a data pump to support sweep searches touching most of the data. The anticipated queries will require special operators related to angular distances and complex similarity tests of object properties, like shapes, colors, velocity vectors, or temporal behaviors. These issues pose interesting data management challenges."
1999,System R: An Architectural Overview.,"Abstract:
We have described the architecture of System R, including the Relational Data System and the Research Storage System. The RDS supports a flexible spectrum of binding times, ranging from precompilmion of ‚Äúcanned transactions‚Äù to on-line execution of ad hoc queries. The advantages of this approach may be summarized as follows: 1. For repetitive transactions, all the work of parsing, name binding, and access path selection is done once at precompilation time and need not be repeated. 2. Ad hoc queries are compiled on line into small machine-language routines that execute more efficiently than an interpreter. 3. Users are given a single language, SQL, for use in ad hoc queries as well as in writing PL/I and COBOL transaction programs. 4. The SQL parser, access path selection routines, and machine language code generator are used in common between query processing and precompilation of transaction programs. 5. When an index used by a transaction program is dropped, a new access path is automatically selected for the transaction without user intervention."
1999,"Fcast Multicast File Distribution: ""Tune in, Download, and Drop Out"".","Reliable data multicast is difficult to scale. Fcast, ìfile multicastingî, combines multicast with Forward Error Correction (FEC) to solve this problem. Like classic multicast, Fcast scales to large audiences, and like other FEC schemes, it uses bandwidth very efficiently. Some of the benefits of this combinati on were known previously, but Fcast contributes new caching methods that improve disk throughput and new optimizations for small file transfers. "
1998,The Asilomar Report on Database Research.,"The database research community is rightly proud of success in basic research, and its remarkable record of technology transfer. Now the field needs to radically broaden its research focus to attack the issues of capturing, storing, analyzing, and presenting the vast array of online data. The database research community should embrace a broader research agenda ó broadening the definition of database management to embrace all the content of the Web and other online data stores, and rethinking our fundamental assumptions in light of technology shifts. To accelerate this transition, we recommend changing the way research results are evaluated and presented. In particular, we advocate encouraging more speculative and long-range work, moving conferences to a poster format, and publishing all research literature on the Web.
"
1998,The Design and Architecture of the Microsoft Cluster Service - A Practical Approach to High-Availability and Scalability.,"Abstract:
Microsoft Cluster Service (MSCS) extends the Windows NT operating system to support high-availability services. The goal is to offer an execution environment where off-the-shelf server applications can continue to operate, even in the presence of node failures. Later versions of MSCS will provide scalability via a node and application management system which allows applications to scale to hundreds of nodes. In this paper we provide a detailed description of the MSCS architecture and the design decisions that have driven the implementation of the service. The paper also describes how some major applications use the MSCS features, and describes features added to make it easier to implement and manage fault-tolerant applications on MSCS."
1997,"Data Cube: A Relational Aggregation Operator Generalizing Group-by, Cross-Tab, and Sub Totals.","Data analysis applications typically aggregate data across manydimensions looking for anomalies or unusual patterns. The SQL aggregate functions and the GROUP BY operator produce zero-dimensional orone-dimensional aggregates. Applications need the N-dimensional generalization of these operators. This paper defines that operator, called the data cube or simply cube. The cube operator generalizes the histogram,cross-tabulation, roll-up,drill-down, and sub-total constructs found in most report writers.The novelty is that cubes are relations. Consequently, the cube operator can be imbedded in more complex non-procedural dataanalysis programs. The cube operator treats each of the N aggregation attributes as a dimension of N-space. The aggregate of a particular set of attribute values is a point in this space. Theset of points forms an N-dimensional cube. Super-aggregates arecomputed by aggregating the N-cube to lower dimensional spaces.This paper (1) explains the cube and roll-up operators, (2) showshow they fit in SQL, (3) explains how users can define new aggregate functions for cubes, and (4) discusses efficient techniques tocompute the cube. Many of these features are being added to the SQLStandard.
"
1997,"The Five-Minute Rule Ten Years Later, and Other Computer Storage Rules of Thumb.","Simple economic and performance arguments suggest appropriate lifetimes for main memory pages and suggest optimal page sizes. The fundamental tradeoffs are the prices and bandwidths of RAMs and disks. The analysis indicates that with today's technology, five minutes is a good lifetime for randomly accessed pages, one minute is a good lifetime for two-pass sequentially accessed pages, and 16 KB is a good size for index pages. These rules-of-thumb change in predictable ways as technology ratios change. They also motivate the importance of the new Kaps, Maps, Scans, and $/Kaps, $/Maps, $/TBscan metrics.
"
1996,Evolution of Data Management.,"Abstract:
Computers can now store all forms of information: records, documents, images, sound recordings, videos, scientific data, and many new data formats. Society has made great strides in capturing, storing, managing, analyzing, and visualizing this data. These tasks are generically called data management. This article sketches the evolution of data management systems. There have been six distinct phases in data management. Initially, data was manually processed. The next step used punched-card equipment and electromechanical machines to sort and tabulate millions of records. The third phase stored data on magnetic tape and used stored-program computers to perform batch processing on sequential files. The fourth phase introduced the concept of a database schema and on-line navigational access to the data. The fifth step automated access to relational databases and added distributed and client server processing. We are now in the early stages of sixth-generation systems that store richer data types, notably documents, images, voice, and video data. These sixth-generation systems are the storage engines for the emerging Internet and intranets. Early data management systems automated traditional information processing. Today they allow fast, reliable, and secure access to globally distributed data. Tomorrow's systems will access and summarize richer forms of data. It is argued that multimedia databases will be a cornerstone of cyberspace."
1996,"Data Cube: A Relational Aggregation Operator Generalizing Group-By, Cross-Tab, and Sub-Total.","Abstract:
Data analysis applications typically aggregate data across many dimensions looking for unusual patterns. The SQL aggregate functions and the GROUP BY operator produce zero-dimensional or one-dimensional answers. Applications need the N-dimensional generalization of these operators. The paper defines that operator, called the data cube or simply cube. The cube operator generalizes the histogram, cross-tabulation, roll-up, drill-down, and sub-total constructs found in most report writers. The cube treats each of the N aggregation attributes as a dimension of N-space. The aggregate of a particular set of attribute values is a point in this space. The set of points forms an N-dimensionaI cube. Super-aggregates are computed by aggregating the N-cube to lower dimensional spaces. Aggregation points are represented by an ""infinite value"": ALL, so the point (ALL,ALL,...,ALL, sum(*)) represents the global sum of all items. Each ALL value actually represents the set of values contributing to that aggregation."
1996,The Dangers of Replication and a Solution.,"Update anywhere-anytime-anyway transactional replication has unstable behavior as the workload scales up: a ten-fold increase in nodes and traffic gives a thousand fold increase in deadlocks or reconciliations. Master copy replication (primary copy) schemes reduce this problem. A simple analytic model demonstrates these results. A new two-tier replication algorithm is proposed that allows mobile (disconnected) applications to propose tentative update transactions that are later applied to a master copy. Commutative update transactions avoid the instability of other replication schemes.
"
1995,AlphaSort: A Cache-Sensitive Parallel External Sort.,"A new sort algorithm, called AlphaSort, demonstrates that commodity processors and disks can handle commercial batch workloads. Using commodity processors, memory, and arrays of SCSI disks, AlphaSort runs the industrystandard sort benchmark in seven seconds. This beats the best published record on a 32-CPU 32-disk Hypercube by 8:1. On another benchmark, AlphaSort sorted more than a gigabyte in one minute. AlphaSort is a cache-sensitive, memoryintensive sort algorithm. We argue that modern architectures require algorithm designers to re-examine their use of the memory hierarchy. AlphaSort uses clustered data structures to get good cache locality, file striping to get high disk bandwidth, QuickSort to generate runs, and replacement-selection to merge the runs. It uses shared memory multiprocessors to break the sort into subsort chores. Because startup times are becoming a significant part of the total time, we propose two new benchmarks: (1) MinuteSort: how much can you sort in one minute, and (2) PennySort: how much can you sort for one penny."
1995,Super Servers: Commodity Computer Clusters Pose a Software Challenge.,"Abstract
Technology is pushing the fastest processors onto single mass-produced chips. Standards are defining a new level of integration: the Pizza Box - a one board computer with memory, disk, base ware, and middleware. These developments fundamentally change the way we will build computers. Future designs must leverage commodity products. Clusters of computers are the natural way to build future mainframes. A simple analysis suggests that such machines will have thousands of processors giving a tera-op processing rate, terabytes of RAM storage, many terabytes of disc storage, and terabits-per-second of communications bandwidth. This presages 4T clusters. To an iron monger or software house: the T stands for Terror! To customers it stands for Tremendous! These computers will be ideally suited to be super-servers in future networks. Software that extracts parallelism from applications is the key to making clusters useful. Client-server computing has natural parallelism: many clients submit many independent requests that can be processed in parallel. Database, visualization, and scientific computing applications have also made great strides in extracting and exploiting parallelism within a single application. These promising first steps bode well for cluster architectures. The challenge remains to extend these techniques to general purpose systems."
1995,A Critique of ANSI SQL Isolation Levels.,"ANSI SQL-92 [MS, ANSI] defines Isolation Levels in terms of phenomena: Dirty Reads, Non-Repeatable Reads, and Phantoms. This paper shows that these phenomena and the ANSI SQL definitions fail to properly characterize several popular isolation levels, including the standard locking implementations of the levels covered. Ambiguity in the statement of the phenomena is investigated and a more formal statement is arrived at; in addition new phenomena that better characterize isolation types are introduced. Finally, an important multiversion isolation type, called Snapshot Isolation, is defined.
"
1995,Parallel Database Systems 101.,n/a
1995,Databases and Workflow Management: What is it All About? (Panel).,n/a
1994,Loading Databases Using Dataflow Parallelism.,"This paper describes a parallel database load prototype for Digital's Rdb database product. The prototype takes a dataflow approach to database parallelism. It includes an explorer that discovers and records the cluster configuration in a database, a client CUI interface that gathers the load job description from the user and from the Rdb catalogs, and an optimizer that picks the best parallel execution plan and records it in a web data structure. The web describes the data operators, the dataflow rivers among them, the binding of operators to processes, processes to processors, and files to discs and tapes. This paper describes the optimizer's cost-based hierarchical optimization strategy in some detail. The prototype executes the web's plan by spawning a web manager process at each node of the cluster. The managers create the local executor processes, and orchestrate startup, phasing, checkpoint, and shutdown. The execution processes perform one or more operators. Data flows among the operators are via memory-to-memory streams within a node, and via web-manager multiplexed tcp/ip streams among nodes. The design of the transaction and checkpoint/restart mechanisms are also described. Preliminary measurements indicate that this design will give excellent scaleups.
"
1994,Desktop Batch Processing.,"Abstract:
Today, online transaction processing (OLTP) applications can downsize from mainframes to microprocessors. Commodity database systems, operating systems, and hardware came of age in 1993/spl minus/they surpassed the online transaction processing performance of proprietary solutions. There are lingering doubts about downsizing batch transaction processing applications. The doubts center on the ability of microprocessor hardware to handle the high I/O bandwidth required by batch processing, and on doubts that microprocessor systems offer the software services and utilities key to batch processing applications. This paper reviews the impressive progress of made by commodity software and hardware in processing OLTP workloads. The discussion is quantitative because the Transaction Processing Performance Council defined a set of benchmarks that characterize OLTP and that quantify price and performance. Discussion then turns to batch transaction processing. There is less consensus on the characteristics of batch transaction processing. Consequently, much of the discussion focuses on requirements. The discussion ends with some performance measurements of utilities running on DEC Alpha AXP microprocessors and on commodity disks. These results indicate that microprocessors today have the capacity to process batch workloads at mainframe speeds. We predict that over the next few years, batch-processing software exploiting parallel processing will emerge. This, combined with commodity hardware, will provide both superior performance and price/performance ratio.< >"
1994,AlphaSort: A RISC Machine Sort.,"ABSTRACT
A new sort algorithm, called AlphaSort, demonstrates that commodity processors and disks can handle commercial batch workloads. Using Alpha AXP processors, commodity memory, and arrays of SCSI disks, AlphaSort runs the industry-standard sort benchmark in seven seconds. This beats the best published record on a 32-cpu 32-disk Hypercube by 8:1. On another benchmark, AlphaSort sorted more than a gigabyte in a minute.
AlphaSort is a cache-sensitive memory-intensive sort algorithm. It uses file striping to get high disk bandwidth. It uses QuickSort to generate runs and uses replacement-selection to merge the runs. It uses shared memory multiprocessors to break the sort into subsort chores.
Because startup times are becoming a significant part of the total time, we propose two new benchmarks: (1) Minutesort: how much can you sort in a minute, and (2) DollarSort: how much can you sort for a dollar."
1994,Quickly Generating Billion-Record Synthetic Databases.,"ABSTRACT
Evaluating database system performance often requires generating synthetic databases‚Äîones having certain statistical properties but filled with dummy information. When evaluating different database designs, it is often necessary to generate several databases and evaluate each design. As database sizes grow to terabytes, generation often takes longer than evaluation. This paper presents several database generation techniques. In particular it discusses: (1) Parallelism to get generation speedup and scaleup. (2) Congruential generators to get dense unique uniform distributions. (3) Special-case discrete logarithms to generate indices concurrent to the base table generation. (4) Modification of (2) to get exponential, normal, and self-similar distributions.
The discussion is in terms of generating billion-record SQL databases using C programs running on a shared-nothing computer system consisting of a hundred processors, with a thousand discs. The ideas apply to smaller databases, but large databases present the more difficult problems."
1993,Why TP-Lite will Dominate the TP Market.,n/a
1992,Parallel Database Systems: The Future of High Performance Database Systems.,"The success of these systems refutes a 1983 paper predicting the demise of database machines [3]. Ten years ago the future of highly parallel database machines seemed gloomy, even to their staunchest advocates. Most database machine research had focused on specialized, often trendy, hardware such as CCD memories, bubble memories, head-per-track disks, and optical disks. None of these technologies fulfilled their promises; so there was a sense that conventional CPUs , electronic RAM, and mcving-head magnetic disks would dominate the scene for many years to come. At that time, disk throughput was predicted to double while processor speeds were predicted to increase by much larger factors. Consequently , critics predicted that multiprocessor systems would scxm be I/O limited unless a solution to the I/O bottleneck was found. Whiie these predictions were fairly accurate about the future of hardware, the critics were certainly wrong about the overall future of parallel database systems. Over the last decade 'Eradata, Tandem, and a host of startup companies have successfully developed and marketed highly parallel machines."
1992,Increasing the Effectiveness of OS Research.,n/a
1992,Database and Transaction Processing Benchmarks.,n/a
1991,High-Availability Computer Systems.,"Abstract:
The techniques used to build highly available computer systems are sketched. Historical background is provided, and terminology is defined. Empirical experience with computer failure is briefly discussed. Device improvements that have greatly increased the reliability of digital electronics are identified. Fault-tolerant design concepts and approaches to fault-tolerant hardware are outlined. The role of repair and maintenance and of design-fault tolerance is discussed. Software repair is considered. The use of pairs of computer systems at separate locations to guard against unscheduled outages due to outside sources (communication or power failures, earthquakes, etc.) is addressed.< >"
1990,Third-Generation Database System Manifesto - The Committee for Advanced DBMS Function.,n/a
1990,Parallel Database Systems: The Future of Database Processing or a Passing Fad?,"The concept of parallel database machines consisting of exotic hardware has been replaced by a fairly conventional shared-nothing hardware base along with a highly parallel dataflow software architecture. Such a design provides speedup and scaleup in processing relational database queries. This paper reviews the techniques used by such systems, and surveys current commercial and research systems.
"
1990,Third-Generation Database System Manifesto - The Committee for Advanced DBMS Function.,n/a
1990,A Benchmark of NonStop SQL Release 2 Demonstrating Near-Linear Speedup and Scaleup on Large Databases.,n/a
1990,FastSort: A Distributed Single-Input Single-Output External Sort.,"External single-input single-output sorts can use multiple processors each with a large tournament replacement-selection in memory, and each with private disks to sort an input stream in linear elapsed time. Of course, increased numbers of processors, memories, and disks are required as the input file size grows. This paper analyzes the algorithm and reports the performance of an implementation.
"
1990,"""The Committee for Advanced DBMS Function"": Third Generation Data Base System Manifesto.",n/a
1990,Parity Striping of Disk Arrays: Low-Cost Reliable Storage with Acceptable Throughput.,"An analysis of mirrored discs and of RAID5 shows that mirrors have considerably better throughput, measured as requests/second on random requests of arbitrary size (up to 1MB). Mirrors have comparable or better response time for requests of reasonable size (less than 100KB). But mirrors have a 100% storage penalty: storing the data twice. Parity striping is a data layout that stripes the parity across the discs, but does not stripe the data. Parity striping has throughput almost as good as mirrors, and has cost/GB comparable to RAID5 designs -combing the advantages of both for high-traffic disc resident data. Parity striping has additional fault containment and software benefits as well. Parity striping sacrifices the high data transfer rates of RAID designs for high throughput. It is argued that response time and throughput are preferable performance metrics."
1990,An Adaptive Hash Join Algorithm for Multiuser Environments.,"As main memory becomes a cheaper resource, hash joins are an alternative to the traditional methods of performing equi-joins: nested loop and merge joins. This paper introduces a modified, adaptive hash join method that is designed to work with dynamic changes in the amount of available memory. The general idea of the algorithm is to regulate resource usage of a hash join in a way that allows it to run concurrently with other applications. The algorithm provides good performance for a broad range of problem sizes, allows to join large tables in a small main memory, and uses advanced I/O controllers with tracksize I/O transfers. It has been implemented as a prototype in Nonstop SQL, a DBMS running on Tandem machines."
1989,Future Directions in DBMS Research - The Laguna Beach Participants.,"On February 4-5, 1988, the International Computer Science Institute sponsored a two day workshop at which 16 senior members of the data base research community discussed future research topics in the DBMS area. This paper summarizes the discussion which took place. "
1989,Database Performance Metrics.,n/a
1988,The Cost of Messages.,"Distributed systems can be modeled as processes communicating via messages. This model abstracts the three degrees of distribution: shared memory, local network, and wide area network. Although these three forms of distribution are qualitatively the same, there are huge quantitative differences in their message transport costs and message transport reliability. This paper quantifies these differences for past, current, and future technologies. "
1988,Disk Shadowing.,"Disk shadowing is a technique for maintaining a set of two or more identical disk images on separate disk devices. Its primary purpose is to enhance reliability and availability of secondary storage by providing multiple paths to redundant data. However, shadowing can also boost UO performance. In this paper, we contend that intelligent device scheduling of shadowed disks increases the I/O rate, by allowing parallel reads and by substantially reducing the average seek time for random reads. In particular, we develop an analytic model which shows that the seek time for a random read in a shadow set is a monotonic decreasing function of the number of disks in the set."
1987,A View of Database System Performance Measures.,"ABSTRACT
Database systems allow quick creation of performance problems. The goal of database systems is to allow the computer-illiterate to write complex and complete applications. It is the job of the system to translate a high-level description of data and procedures into efficient algorithms. The REAL performance metric of a system is how successfully it meets these goals.
Practitioners use a much narrower definition of system performance. They assume a standard workload and measure performance by peak throughput and by dollar cost per transaction.
Although many vendors have ‚Äúprivate‚Äù performance measures, Bitton, Dewitt, and Turbyfill were the first to publish a measure of database system performance [Bitton]. Their measure, here called the Wisconsin benchmark, consists of a database design, a set of 32 retrieval and update statements, and a script for multi-user tests. They give two performance metrics: the elapsed time for each statement and the throughput of the system when running sixteen simultaneous scripts. No response time requirement or cost measure is included in the definition. The Wisconsin benchmark is the most widely used database benchmark.
Largely in response to the Wisconsin benchmark, an informal group including Bitton and Dewitt, defined a benchmark more representative of transaction processing applications [Anon]. Its workload is:
SCAN - A mini-batch operation to sequentially copy 1000 records
SORT - A batch operation to sort one million records.
DebitCredit - A short transaction with terminal input and output via X.25, presentation services, and a mix of five database accesses.
The DebitCredit transaction has rules for scaling the terminal network and database size as the transaction rate increases, and also rules for distributing transactions if the system is decentralized.
The performance metrics for this benchmark are:
Elapsed time for the SCAN and SORT.
Peak throughput for the DebitCredit transaction at 1 second response time for 95% of the transactions. This gives a TPS (Transactions Per Second) rating.
Price per transaction where price is the 5-year cost of hardware, software and maintenance. This is sometimes called the vendors-view of price.
This benchmark has been adopted by several vendors to compare their performance and price performance from release to release and also to compare their performance to competitive products. MIPS, Whetstones and MegaFLOPs have served a similar role in the scientific community.
A system's TPS rating indicates not just processor speed, but also IO architecture, operating system, data communications and database software performance. Unfortunately, it does not capture ease-of-use.
Work continues on formalizing these benchmarks. At present they are written in English. Ultimately they should be defined by a file generator and a set of programs written in a standard database language such as COBOL-SQL.
When a vendor first measures his system against these benchmarks, the results are usually terrible. Both benchmarks are designed to expose generic performance bugs in frequently used transaction processing atoms. For example, the Wisconsin and SCAN benchmarks heavily penalize a system which is slow to read the next record in a file.
A system with poor performance on these benchmarks can be analyzed as follows: Most vendors have an ‚Äúatomic‚Äù model of their system which represents each transaction as a collection of atoms. The atoms are the primitives of the system. For example, the SCAN benchmark is represented by most vendors as: SCAN: BEGIN TRANSACTION PERFORM 1000 TIMES READ SEQUENTIAL INSERT SEQUENTIAL COMMIT TRANSACTION
The atomic weights for, BEGIN, READ SEQUENTIAL, INSERT SEQUENTIAL, and COMMIT are measured for each release. The atomic weight usually consists of CPU instructions, message bytes, and disc IOs for a ‚Äútypical‚Äù call to that operation. These weights can be converted to service times by knowing the speeds and utilizations of the devices (processors, discs, lines) used for the application. The molecular weight and service time of SCAN can then be computed as the sum of the atomic weights.
Defining and measuring a system's atoms is valuable. It produces a simple conceptual model of how the system is used. Atomic measurements also expose performance bugs. For example, based on the SCAN benchmark, most systems perform READ SEQUENTIAL in 1000 instructions and with .02 disc IO. If a system uses many more instructions or many more IO then it has a performance problem. Similarly, the DebitCredit transaction typically consumes about 2OOKi (thousand instructions) and five disc IO per transaction. One system is known to use 800Ki and 14 IO per transaction. The vendor could use atomic measurement to find the causes of such poor performance. When such problems are localized to an atom, solutions to the problem readily suggest themselves. So, atomic measurement is useful for performance assurance and performance improvement.
Atomic measurement also has a major role in system sizing and in capacity planning. If the customer can describe his application in terms of atoms, then a spreadsheet application can give him an estimate of the CPU, disc and line cost for the application. With substantially more effort (and assumptions) the system's response time can be predicted. With even more effort, a prototype system can be generated and benchmarked from the atomic transaction descriptions. Snapshot [Stewart] and Envision [Envison] are examples of systems which combine atomic modeling, queue modeling, and ultimately benchmarking of real systems generated from the atomic description of the application."
1987,Operating System Support for Data Management Systems.,n/a
1987,The 5 Minute Rule for Trading Memory for Disk Accesses and The 10 Byte Rule for Trading Memory for CPU Time.,"ABSTRACT
If an item is accessed frequently enough, it should be main memory resident. For current technology, ‚Äúfrequently enough‚Äù means about every five minutes.
Along a similar vein, one can frequently trade memory space for CPU time. For example, bits can be packed in a byte at the expense of extra instructions to extract the bits. It makes economic sense to spend ten bytes of main memory to save one instruction per second.
These results depend on current price ratios of processors, memory and disc accesses. These ratios are changing and hence the constants in the rules are changing."
1986,An Approach to Decentralized Computer Systems.,"Abstract:
The technology for distributed computing is available. However, it is argued that decentralized systems will always require more careful design, planning, and management than their centralized counterparts. The rationale for and against decentralization is given, and a technical approach to decentralized systems is sketched. This approach contrasts with the popular concept of a distributed integrated database which transparently provides remote IO against single system image. Rather, it proposes that functions be distributed as `servers' which abstract data as high-level operations on objects and communicate with `requestors' via a standard message protocol. This requestor-server approach has the advantages of modularity and performance."
1986,A Comparison of the Byzantine Agreement Problem and the Transaction Commit Problem.,"Abstract
Transaction Commit algorithms and Byzantine Agreement algorithms solve the problem of multiple processes reaching agreement in the presence of process and message failures. This paper summarizes the computation and fault models of these two kinds of agreement and discusses the differences between them. In particular, it explains that Byzantine Agreement is rarely used in practice because it involves significantly more hardware and messages, yet does not give predictable behavior if there are more than a few faults."
1986,Why Do Computers Stop and What Can Be Done About It?,"An analysis of the failure statistics of a commercially available fault -tolerant system shows that administration and software ar e the major contributors to failure. Various approaches to software fault-tolerance are then discussed -- notably process-pairs, transactions and reliable storage. It is pointed out that faults in production software are often soft (transient) and that a t ransaction mechanism combined with persistent processpairs provides fault-tolerant execution -- the key to software fault -tolerance."
1985,Transaction Acceleration.,n/a
1985,Why do Computer Stop and What Can be About it?,"An analysis of the failure statistics of a commercially available fault -tolerant system shows that administration and software ar e the major contributors to failure. Various approaches to software fault-tolerance are then discussed -- notably process-pairs, transactions and reliable storage. It is pointed out that faults in production software are often soft (transient) and that a t ransaction mechanism combined with persistent processpairs provides fault-tolerant execution -- the key to software fault -tolerance."
1985,One Thousand Transactions per Second.,Several companies intend to provide general-purpose transaction processing systems capable of one thousand transactions per second. This paper surveys the need for such systems and contrasts the approaches being taken by three different groups. 
1985,Fault Tolerance in Tandem Systems.,n/a
1983,Practical Problems in Data Management - A Position Paper.,n/a
1982,Transactions and Consistency in Distributed Database Systems.,"The concepts of transaction and of data consistency are defined for a distributed system. The cases of partitioned data, where fragments of a file are stored at multiple nodes, and replicated data, where a file is replicated at several nodes, are discussed. It is argued that the distribution and replication of data should be transparent to the programs which use the data. That is, the programming interface should provide location transparency, replica transparency, concurrency transparency, and failure transparency. Techniques for providing such transparencies are abstracted and discussed.
By extending the notions of system schedule and system clock to handle multiple nodes, it is shown that a distributed system can be modeled as a single sequential execution sequence. This model is then used to discuss simple techniques for implementing the various forms of transparency."
1981,A History and Evaluation of System R.,"System R, an experimental database system, was constructed to demonstrate that the usability advantages of the relational data model can be realized in a system with the complete function and high performance required for everyday production use. This paper describes the three principal phases of the System R project and discusses some of the lessons learned from System R about the design of relational systems and database systems in general.
"
1981,The Recovery Manager of the System R Database Manager.,"The recovery subsystem of an experimental data management system is described and evaluated. The transactmn concept allows application programs to commit, abort, or partially undo their effects. The DO-UNDO-REDO protocol allows new recoverable types and operations to be added to the recovery system Apphcation programs can record data m the transaction log to facilitate application-specific recovery. Transaction undo and redo are based on records kept in a transaction log. The checkpoint mechanism is based on differential fries (shadows). The recovery log is recorded on disk rather than tape."
1981,System R: An Architectural Overview.,"Abstract:
We have described the architecture of System R, including the Relational Data System and the Research Storage System. The RDS supports a flexible spectrum of binding times, ranging from precompilation of ‚Äúcanned transactions‚Äù to on-line execution of ad hoc queries. The advantages of this approach may be summarized as follows: 1. For repetitive transactions, all the work of parsing, name binding, and access path selection is done once at precompilation time and need not be repeated. 2. Ad hoc queries are compiled on line into small machine-language routines that execute more efficiently than an interpreter. 3. Users are given a single language, SQL, for use in ad hoc queries as well as in writing PL/I and COBOL transaction programs. 4. The SQL parser, access path selection routines, and machine language code generator are used in common between query processing and precompilation of transaction programs. 5. When an index used by a transaction program is dropped, a new access path is automatically selected for the transaction without user intervention."
1981,A Straw Man Analysis of the Probability of Waiting and Deadlock in a Database System.,n/a
1981,An Approach to End-Users Application Design.,"Abstract
Soon every desk will have a computer on it. Software to do mundane things such as payroll, mail, and text processing exists and as a by-product produces vast quantities of one-line information. Many users want to manipulate this data, often in unanticipated ways. These unexpected uses cannot justify substantial programming costs. This paper argues that the relational data model and operators combined with a screen-oriented forms design and display system answers many of the needs of such users. In such a system, all data are represented in terms of records and fields. The user defines the screens (forms) he wants to see, and then specifies the mapping between fields of these screens and fields of the database records in terms of predicates and relational operators."
1981,The Transaction Concept: Virtues and Limitations (Invited Paper).,"A transaction is a transformation of state which has the properties of atomicity (all or nothing), durability (effects survive failures) and consistency (a correct transformation). The transaction concept is key to the structuring of data management applications. The concept may have applicability to programming systems in general. This paper restates the transaction concepts and attempts to put several implementation approaches in perspective. It then describes some areas which require further study: (1) the integration of the transaction concept with the notion of abstract data type, (2) some techniques to allow transactions to be composed of sub- transactions, and (3) handling transactions which last for extremely long times (days or months)."
1980,A Transaction Model.,"Abstract
This paper is an attempt to tersely restate several theoretical results about transaction recovery and concurrency control. A formal model of entities, actions, transactions, entity failures, concurrency and distributed system is required to present these results. Included are theorems on transaction undo and redo, degrees of consistency, predicate locks, granularity of locks, deadlock, and two-phase commit."
1979,System R: A Relational Data Base Management System.,"Abstract:
A relational approach makes this experimental data base management system unusually easy to install and use. Some of the decisions made in System R design in order to enhance usability also offer major bonuses in other areas."
1979,The Convoy Phenomenon.,n/a
1978,Notes on Data Base Operating Systems.,"Abstract
This paper is a compendium of data base management operating systems folklore. It is an early paper and is still in draft form. It is intended as a set of course notes for a class on data base operating systems. After a brief overview of what a data management system is it focuses on particular issues unique to the transaction management component especially locking and recovery."
1976,The Notions of Consistency and Predicate Locks in a Database System.,"In database systems, users access shared data under the assumption that the data satisfies certain consistency constraints. This paper defines the concepts of transaction, consistency and schedule and shows that consistency requires that a transaction cannot request new locks after releasing a lock. Then it is argued that a transaction needs to lock a logical rather than a physical subset of the database. These subsets may be specified by predicates. An implementation of predicate locks which satisfies the consistency condition is suggested.
"
1976,System R: Relational Approach to Database Management.,"System R is a database management system which provides a high level relational data interface. The systems provides a high level of data independence by isolating the end user as much as possible from underlying storage structures. The system permits definition of a variety of relational views on common underlying data. Data control features are provided, including authorization, integrity assertions, triggered transactions, a logging and recovery subsystem, and facilities for maintaining data consistency in a shared-update environment.

This paper contains a description of the overall architecture and design of the system. At the present time the system is being implemented and the design evaluated. We emphasize that System R is a vehicle for research in database architecture, and is not planned as a product."
1976,Granularity of Locks and Degrees of Consistency in a Shared Data Base.,The problem of choosing the appropriate Hranularit~ (size) of lockable objects is introduced and the tradeoff between concurrency and overhead is discusseS. A locking protocol which allows simultaneous locking at various granularities by different transactions is presented. It is based on the introduction of additional lock modes besides the conventional share mode an5 exclusive mode. A proof is given of the equivalence of this protocol to a conventional one. 
1975,"Views, Authorization, and Locking in a Relational Data Base System.","ABSTRACT
In the interest of brevity we assume that the reader is familiar with the notion of a relational data base. In particular, we assume a familiarity with the work of Codd or Boyce and Chamberlin. The examples in this paper will be drawn from a data base which describes a department store and consists of three relations:
EMP(NAME, SAL, MGR, DEPT)
SALES(DEPT, ITEM, VOL)
LOC(DEPT, FLOOR)"
1975,Granularity of Locks in a Large Shared Data Base.,"ABSTRACT
This paper proposes a locking protocol which associates locks with sets of resources. This protocol allows simultaneous locking at various granularities by different transactions. It is based on the introduction of additional lock modes besides the conventional share mode and exclusive mode. The protocol is generalized from simple hierarchies of locks to directed acyclic graphs of locks and to dynamic graphs of locks. The issues of scheduling and granting conflicting requests for the same resource are then discussed. Lastly, these ideas are compared with the lock mechanisms provided by existing data management systems."
1973,Canonical Precedence Schemes.,"A general theory of canonical precedence analysis is defined and studied. The familiar types of precedence analysis such as operator precedence or simple precedence occur as special cases of this theory. Among the theoretical results obtained are a characterization of the structure of precedence relations and the relation of canonical precedence schemes to operator sets.
"
1972,On the Covering and Reduction Problems for Context-Free Grammars.,"A formal definition of one grammar ""covering"" another grammar is presented. It is argued that this definition has the property that G' covers G when and only when the ability to parse G' suffices for parsing G. I t is shown that every grammar may be covered by a grammar in canonical two form. Every A-free grammar is covered by an operator normal form grammar while there exist grammars which cannot be covered by any grammar in Greibach form. Any grammar may be covered by an invertible grammar. Each A-free and chain reduced LR(k) (bounded right context) grammar is covered by a precedence detectable, LR(k) (bounded right context) reducible grammar."
1969,Single Pass Precedence Analysis (Extended Abstract).,n/a
1968,Infinite Linear Sequential Machines.,"Abstract
Linear sequential machines (LSM's for short) are considered over arbitrary fields. Various finiteness conditions are given for, sequential machines and LSM's with these properties are characterized. It is shown that the set of all input/output pairs characterizes an LSM. The effect on realizations of varying the ground field is studied. A number of algorithms are given, for the solution of specific problems. Decision procedures are given for the equivalence problem for LSM's. The problem of determining if one state is accessible from another is discussed and a number of results are presented."
1967,Two-Way Pushdown Automata.,"In this paper, a new type of automation, called a two-way pushdown automaton is defined and studied. The model is a generalization of a pushdown automaton in that two-way motion is allowed on the input tape which is assumed to have endmarkers. The model is investigated in both the nondeterministic and deterministic cases. A number of basic results are obtained which include relationships with other families, closure and nonclosure results, and decidability properties. Certain special cases are studied such as the cases when the input alphabet has one letter and the device has no endmarkers."
1966,The Theory of Sequential Relations.,n/a
