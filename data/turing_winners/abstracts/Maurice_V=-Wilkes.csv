2007,High Performance Processor Chips.,"Abstract
The development during the last 15 years in single-chip computers has been striking. It is a development that I have followed in detail, and some of you may have heard me talk about it before. It started in 1989 when a desktop computer first outperformed the most powerful minicomputer on the market, namely the VAX 8600. The desktop computer in question was a workstation based on a processor chip developed and marketed by the MIPS Computer Company, of which Professor John Hennessy of Stanford University was a leading light. Subsequent developments were very rapid and led to the demise of the minicomputer as we used to know it and to love it."
2006,What I Remember of the ENIAC.,"Abstract:
This paper describes the experience of author regarding ENIAC. The ENIAC was indeed of very great size and the team, under the leadership of Presper Eckert, began to realize that by taking an improved approach, a much more powerful computer-one-tenth of the size of the ENIAC-could be built. This, as everyone now knows, led to the EDVAC report and, after a long evolution, to the modern computer."
2003,Biographies.,"Abstract:
I. Bernard Cohen was born on 1 March 1914 and died 20 June 2003 in his home in Waltham, Massachusetts, USA. He played a central role in the establishment of the history of computing as a topic of serious academic study. Even so, his contributions to the history of the physical sciences were so broad and fundamental that his obituary in the New York Times made no reference to computing. Cohen is best known as the author of classic works on Isaac Newton and Ben Franklin; as a rigorous yet readable popularizer of the history of science in The Birth of a New Physics (1960) and Revolution in Science (1985); as a lifelong teacher who brought his gifts to continuing education classes, cruise-ship passengers, and undergraduates; and as a mentor to generations of now eminent historians. He wrote more than 20 books, and 150 articles. He spent his adult life at Harvard, entering in 1933 as a freshman, staying for his graduate education and teaching there long after his official retirement. His life and works are reviewed."
2001,The memory gap and the future of high performance memories.,n/a
2001,High Performance Memory Systems.,n/a
2000,"Introduction to ""Babbage's Analytical Engine Plans 28 and 28a-The Programmer's Interface"".",n/a
1998,A Revisionist Account of Early Language Development.,"Abstract:
The author discusses how the practicality of Fortran and the elegance of Algol defined a fault line running through programming language study that we are just now beginning to bridge. He considers Algol's block structure and some alternatives to this structure."
1997,Arithmetic on the EDSAC MAURICE V.,"Abstract:
The Electronic Delay Storage Automatic Calculator (EDSAC) was a serial binary computer with an ultrasonic delay memory. It was designed and built at the University of Cambridge in England. Work on the project started at the end of 1946, and the machine began to work in May 1949. The paper describes how the programmer handled fixed point arithmetic and emphasizes the merits of having a double length accumulator."
1997,The collapsed LAN: a solution to a bandwidth problem?,n/a
1997,The impediments to technological advancement imposed by the laws of physics.,n/a
1996,Computers Then and Now - Part 2.,"By 1967, when I delivered my Turing lecture 'Computers Then and Now' [1], the computer field was well on its way to assuming its modem form. Progress had been made with high level languages (including the scientific study of their syntax), operating systems (including time-sharing systems), and CRT displays (including their application to Computer Aided Design). I referred to all these topics in my lecture. One feature was missing. There was as yet no computer networking, although it was in the air. Three years were to elapse before the ARPANET came into operation."
1996,Arithmetic on the EDSAC.,"The EDSAC was a serial binary computer with an ultrasonic delay memory. It was designed and built at the University of Cambridge in England. Work on the project started at the end of 1946 and the machine began to work in May 1949. The paper describes how the programmer handled (fixed point) arithmetic and emphasizes the merits of having a double length accumulator. The basic strategy underlying the EDSAC project was: first to build a machine and then to learn how to use it [1]. These two phases of building and using were of equal importance. We were quite sure that once we had a working machine, there would be much that we would have to learn before we could exploit it effectively. Architecturally, the machine was to be a simple one with no frills; on the other hand, the architecture was not to be so cut to the bone that the machine would be uncomfortable to use. There was to be no attempt to exploit fully the technology available and no attempt to optimize the implementation. It went \\'ithout saying that the EDSAC would be a fixedpoint machine. Floating point relay machines had been built and the merits of floating point operation were fully recognized. However, it would be a long time before any electronic engineer would feel sufficiently confident with vacuum tube technology to attempt the design of a floating point machine"
1995,The memory wall and the CMOS end-point.,n/a
1994,Operating Systems in a Changing World.,n/a
1994,The EDSAC: Origins and Design Decisions.,n/a
1993,Light Amplifiers and Solitons.,n/a
1993,From Fortran and Algol to Object-Oriented Languages.,n/a
1992,EDSAC 2.,"Abstract:
The principal hardware features of EDSAC 2, which was developed in the Cambridge University Mathematical Laboratory, are described. EDSAC 2, which came into operation early in 1958, was the first computer to have a microprogrammed control unit and established the viability of microprogramming as a basis for computer design. At the mechanical level of organization, EDSAC 2 was packaged in a bit-sliced manner, with interchangeable plug-in units. This method of packaging was well matched to the vacuum tube technology of the period, and its expected advantages, arising from the replication of units, were fully realized.< >"
1992,Charles Babbage - The Great Uncle of Computing?,n/a
1992,The Long-Term Future of Operating Systems.,n/a
1991,Babbage's Expectations for his Engines.,"Abstract:
Babbage's expectations for his Difference Engine were those of a young enthusiast. Although he failed to complete his version of the engine, an independent implementation of his ideas was carried through by Georg and Edvard Scheutz. Two Scheutz engines were built and put to work, one at the Registrar-General's Office in London and one at the Dudley Observatory in Albany, N. Y. They performed as intended, but failed to revolutionize the making of mathematical tables as Babbage had hoped they would. When Babbage was 45years old. he wrote, but did not publish, a description of the Analytical Engine. Here he showed vision verging on genius. His judgment on the design and utility of the Analytical Engine was as sound as his judgment on matters concerned with the Difference Engine was weak. Studies by A. G. Bromley, based on an examination of his notebooks, have brought out his remarkable achievements at what we would now call the microprogram level and also the insights that eluded him at the user level. His failure to publish may have been because he never arrived at what he regarded as a satisfactory system for programming at the user level."
1991,Pray Mr. Babbage-A character study in dramatic form.,"Abstract:
Babbage is noted for his technical, belatedly recognized, triumphs of computational technology; his personality has been the subject of scrutiny by several authors. Towards his end of life he is said to have wished that he could exchange the remainder of his life for three days in the future. This play provides us with the opportunity to step back to his day and to see the man separate (as far as he would allow) from his machines. That we could grant his wish..."
1991,Software and the Programmer.,"Engineering could not exist without mathematics and experimental science. Mathematics deals in pure thought and experimental science is concerned with the laws of nature. Within the same framework, it may be said that the aim of engineering is to secure control over nature. In some branches of engineering the dependence is very clear. Where, for example, would heat engines be without thermodynamics, radio antenna design without electromagnetic theory, or communications without Fourier analysis? It has long been accepted that the training of an engineer should include a serious study of mathematics and the relevant science, whatever use he or she may subsequently make of this learning."
1991,Revisiting Computer Security in the Business World.,"In my first column (Communications, Apr. 1990) I discussed computer security in the business world. I remarked that whereas the military had always attached great importance to security, business managers regarded it as yet another demand on their budgets, and have required convincing about its importance. As a result, the military made most of the advances in research on computer security for a long time. They have evolved what is now a well-developed doctrine."
1991,Could It Have Been Predicted?,"As everyone knows, the computer industry is passing through a period of great change. I was speaking recently to a senior executive in one of the large companies vigorously working to meet the developing situation. The question he posed was: ‚ÄúCould it have been predicted?‚Äù"
1990,Computer Security in the Business World.,n/a
1990,"Networks, Email, and FAX.",n/a
1990,The Bandwidth Famine.,"A number of years ago I visited a young and successful computer company and was given a tour of the facilities by one of the directors. We passed offices in which people were working at computer terminals; I was told they constituted the accounts department. A little further on we came to the software department; here people were also using terminals. ‚ÄúNow,‚Äù said my host, ‚ÄúI will show you the hardware department.‚Äù To my surprise, instead of the oscilloscopes and waveform analyzers that I had expected to find, I saw more computer terminals. This was a sign of the times.Today, the designer of printed circuit boards such as those intended to be plugged into PCs, has a great variety of software tools from which to choose: programs for layout and routing; programs for checking that the design rules have been followed correctly; logic simulators for checking that the design is functionally correct; and more elaborate simulators for checking the timing. These tools make it possible to produce a working prototype without performing any experimental work on a laboratory bench.In the dark ages of electronics, the debugging of circuits was carried out using experimental versions made by screwing the components down on a piece of wood and connecting them. The behavior of the circuit was checked using an oscilloscope. These experimental versions were known as breadboards, because someone had compared them to those boards on which bread was sliced. The wooden board has been obsolete for years, but the term has survived. A modern version of the breadboard (still used occasionally) is the wire-wrapped prototype. It is, however, costly in time and money and the effort spent on it does not advance the physical design of the final product. Moreover, in certain respects, particularly as regards electrical interference and cross talk, the wire-wrapped version may give misleading information."
1990,It's All Software Now.,n/a
1990,The Rise of the VLSI Processor.,"Around 1970 Intel discovered it could put 2,000 transistors‚Äîor perhaps a few more‚Äîon a single NMOS chip. In retrospect, this may be said to mark the beginning of very large-scale integration (VLSI), an event which had been long heralded, but had been seemingly slow to come. At the time, it went almost unnoticed in the computer industry. This was partly because 2,000 transistors fell far short of what was needed to put a processor on a chip, but also because the industry was busy exploiting medium-scale integration (MSI) in the logic family known as TTL. Based on bipolar transistors, and a wide range of parts containing a few logical elements‚Äîtypically two flip-flops or up to 16 gates in various combinations‚ÄîTTL was highly successful. It was fast and versatile, and established new standards for cost effectiveness and reliability. Indeed, in an improved form and with better process technology, TTL is still widely used. In 1970, NMOS seemed a step backward as far as speed was concerned.Intel did, however, find a customer for its new process; it was a company that was interested in a pocket calculator chip. Intel was able to show that a programmable device would be preferable on economic grounds to a special-purpose device. The outcome was the chip that was later put on the market as the Intel 4004. Steady progress continued, and led to further developments: In April 1972 came the Intel 8008 which comprised 3,300 transistors, and then in April 1974 came the 8080 which had 4,500 transistors. The 8080 was the basis of the Altar 8800 which some people regard as the ancestor of the modern personal computer. It was offered in the form of a kit in January 1975. Other semiconductor manufacturers then entered the field: Motorola introduced the 6800 and MOS Technology Inc. introduced the 6502.Microcomputers had great success in the personal computer market which grew up alongside the older industry, but was largely disconnected from it. Minicomputers were based on TTL and were faster than microcomputers. With instruction sets of their own design and with proprietary software, manufacturers of minicomputers felt secure in their well-established markets. It was not until the mid-1980s that they began to come to terms with the idea that one day they might find themselves basing some of their products on microprocessors taken from the catalogs of semiconductor manufacturers, over whose instruction sets they had no control. They were even less prepared for the idea that personal computers, in an enhanced form known as workstations, would eventually come to challenge the traditional minicomputer. This is what has happened‚Äîa minicomputer has become nothing more than a workstation in a larger box and provided with a wider range of peripheral and communication equipment.As time has passed, the number of CMOS transistors that can be put on a single chip has increased steadily and dramatically. While this has been primarily because improvements in process technology have enabled semiconductor manufacturers to make the transistors smaller, it has also been helped by the fact that chips have tended to become larger. It is a consequence of the laws of physics that scaling the transistors down in size makes them operate faster. As a result, processors have steadily increased in speed. It would not have been possible, however, to take full advantage of faster transistors if the increase in the number that could be put on a chip had not led to a reduction in the total number of chips required. This is because of the importance of signal propagation time and the need to reduce it as the transistors become faster. It takes much less time to send a signal from one part of the chip to another part than it does to send a signal from one chip to another.The progress that has been made during the last three or four years is well illustrated by comparing the MIPS R2000 processor developed in 1986 with two-micron technology, with the Intel i860 developed in 1989. The former is based on a RISC processor which takes up about half the available space. This would not have left enough space for more than a very small amount of cache memory. Instead the designer included the cache control circuits for off-chip instruction and data caches. The remaining space, amounting to about one-third of the whole was put to good use to accommodate a Memory Management Unit (MMU) with a Translation Look Aside Buffer (TLB) of generous proportions. At this stage in the evolution of processor design, the importance of RISC philosophy in making the processor as small as it was will be appreciated. A processor of the same power designed along pre-RISC lines would have taken up the entire chip, leaving no space for anything else.When the Intel i860 processor was developed three years later, it had become possible to accommodate on the chip, not only the units mentioned above, but also two caches‚Äîone for data and one for instructions‚Äîand a highly parallel floating point coprocessor. This was possible because the silicon area was greater by a factor of slightly more than 2, and the amount of space occupied by a transistor less by a factor of 2.5. This gave a five-fold effective increase in the space available. The space occupied by the basic RISC processor itself is only 10% of the whole as compared with 50% on the R2000. About 35% is used for the floating point coprocessor and 20% for the memory management and bus control. This left about 35% to be used for cache memory.There are about one million transistors on the i860‚Äîthat is 10 times as many as on the R2000, not 5 times as many as the above figures would imply. This is because much of the additional space is used for memory, and memory is very dense in transistors. When still more equivalent space on the silicon becomes available, designers who are primarily interested in high-speed operation will probably use the greater part of it for more memory, perhaps even providing two levels of cache on the chip. CMOS microprocessors have now pushed up to what used to be regarded as the top end of the minicomputer range and will no doubt go further as the transistor size is further reduced.Bipolar transistors have followed CMOS transistors in becoming smaller, although there has been a lag. This is mainly because of the intrinsically more complex nature of the bipolar process; but it is also partly because the great success of CMOS technology has led the semiconductor industry to concentrate its resources on it. Bipolar technology will always suffer from the handicap that it takes twice as many transistors to make a gate as it does in CMOS.The time to send a signal from one place to another depends on the amount of power available to charge the capacitance of the interconnecting wires. This capacitance is much greater for inter-chip wiring than for on-chip wiring. In the case of CMOS, which is very low-power technology, it is difficult to provide enough power to drive inter-chip wiring at a high speed. The premium placed on putting everything on the same chip is, therefore, very great.Much more power is available with bipolar circuits and the premium is not nearly so great. For this reason it has been possible to build multi-chip processors using gate arrays that take full advantage of the increasingly high speed of available bipolar technology. It is presently the case that all very fast computers on the market use multi-chip bipolar processors.Nevertheless, as switching speeds have become higher it has become necessary to develop interconnect systems that are faster than traditional printed circuit boards. It is becoming more and more difficult to do this as switching speeds continue to increase. In consequence, bipolar technology is approaching the point‚Äîreached earlier with CMOS‚Äîwhen further advance requires that all those units of a processor that need to communicate at high speed shall be on the same chip. Fortunately, we are in sight of achieving this. It will soon be possible to implement, in custom bipolar technology on a single chip, a processor similar to the R2000.Such a processor may be expected to show a spectacular increase of speed compared with multi-chip implementations based on similar technology, but using gate arrays. However, as it becomes possible to put even more transistors on a single chip, it may be that the balance of advantage will lie with CMOS. This is because it takes at least four times as many transistors to implement a memory cell in bipolar as it does in CMOS. Since any processor, especially a CMOS processor, gains greatly in performance by having a large amount of on-chip memory, this advantage could well tip the balance in favor of CMOS.The advantage that would result from being able to put CMOS transistors and bipolar transistors on the same chip has not gone unnoticed in the industry. Active development is proceeding in this area, under the generic name BiCMOS. BiCMOS is also of interest for analogue integrated circuits.If the BiCMOS process were optimized for bipolar transistors it would be possible to have a very high-performance bipolar processor with CMOS on-chip memory. If the bipolar transistors were of lower-performance levels they would still be of value for driving off-chip connections and also for driving long-distance connections on the chip itself.A pure bipolar chip, with a million transistors on it, will dissipate at least 50 watts, probably a good deal more. Removing the heat presents problems, but these are far from being insuperable. More severe problems are encountered in supplying the power to the chip and distributing it without a serious voltage drop or without incurring unwanted coupling. Design tools to help with these problems are lacking. A BiCMOS chip of similar size will dissipate much less power. On the other hand, BiCMOS will undoubtedly bring a spate of problems of its own, particularly as the noise characteristics of CMOS and bipolar circuits are very different.CMOS, bipolar, and BiCMOS technologies are all in a fluid state of evolution. It is possible to make projections about what may happen in the short term, but what will happen in the long term can only be a matter of guess work. Moreover, designing a computer is an exercise in system design and the overall performance depends on the statistical properties of programs as much as on the performance of the individual components. It would be a bold person who would attempt any firm predictions.And then, finally, there is the challenge of gallium arsenide. A colleague, with whom I recently corresponded, put it very well when he described gallium arsenide as the Wankel engine of the semiconductor industry!"
1988,ACM SIGOPS European workshop 1988: position paper.," When personal computers first appeared some people said that they marked the end of time-sharing. This did not happen. Personal computers were sold on a wide scale and through them new markets were opened up for the computer industry. However, serious users-those who had heavy requirements for processor cycles and the need to share programmes and data with colleagues-continued to use time-sharing systems. They found the new, low cost, departmental time-sharing systems ideal for their needs. These low cost systems owed their existence every bit as much to VLSI as personal computers did. With the advent of more powerful personal computers-which became known as work stations-users of the class I referred to above began to use them instead of time-sharing systems. However, few such users' regard their work stations as stand-alone entities. Services available via a local area network are essential to their needs. A file server was early perceived as being of first importance, but it soon became apparent that a work station user needed all the services traditionally available on a time-sharing system except processor cycles. Even that exception requires qualification, since the hunger for processor cycles is such that work station owners are often glad to avail themselves of cycles available elsewhere on the network, for example, in other users'workstations or in a time-sharing system. A user judges a computing environment by the response he gets to the commands he types at his work station. There was once a tendency to assume that the response would always be better if the work was done in the work station itself. There is now a growing appreciation that this is not necessarily the case, and that a judicious distribution of function between the work station and computers elsewhere on the local network may give improved performance. We thus have the concept of work station style computing in which it appears to the user that everything is being done in the equipment on his own desk whereas, in fact, much of it may be being done elsewhere. It is along the above lines that I approach the topics discussed in the handout for the proposed workshop. The availability, at the present time, of work stations in various price ranges and their acceptance by the user community, gives topical importance to the issues proposed for the workshop."
1984,Security Management and Protection - A Personal Approach.,"In all security systems, a distinction must be drawn between security management and protection. Security management can be described by the well-known lattice model and implemented by means of an access control algorithm in the file manager. The effect is to erect a security fence around each user's file directory. For protection the simpler capability model suffices. In addition to the user file directories there is a system-wide capability index. This is not protected by a security fence and procedures that can make direct use of it may only be used in that part of the operating systemñthe security kernelñwhich is certified by a security inspector. At the present time the efficient implementation of a capability-based protection system presents difficulties and further research is called for.
"
1983,"Size, Power, and Speed.",The author discusses the roles of power and size in determining the speed of a computer.
1982,Hardware Support for Memory Protection: Capability Implementations.,"This paper is intended to stimulate discussion on the present state of hardware supported capability systems. Interest in such systems grew up in the mid-1960's and since that time information has been published on several different versions. In the opinion of some observers, the software complexity of these systems outweighs the advantage gained. The paper surveys the situation, and endeavors to set out the general features that a hardware supported capability system should possess. An attempt is made to identify the causes of the complexity and to make recommendations for removing them. The arguments for and against the tagging of capabilities are discussed and attention is drawn to a system of semi-tagging previously proposed by the author."
1981,The Design of a Control Unit-Reflections on Reading Babbage's Notebooks.,"Abstract:
Babbage published very little on the detailed design of the control system for the Analytical Engine. In this paper the author puts forward a possible reason for this and discusses the relationship between Babbage's ideas as revealed in his unpublished work and modern concepts."
1980,The Impact of Wide-Band Local Area Communication Systems on Distributed Computing.,"Abstract:
Work in the design and implementation of local area systems holds the key to the future of large distributed computer installations."
1980,The Cambridge Model Distributed System.,n/a
1980,A New Hardware Capability Architecture.,"In existing capability machines, such as the CAP and the Plessey 250, the programmer recognises two types of segment, one in which he keeps capabilities and one in which he keeps data words and instructions. The hardware provides two kinds of access, C-type and D-type; generally speaking, C-type access is used for capability segments and D-type access for segments containing data and instructions but, exceptionally, a highly privileged routine within the operating system may have a D-type capability for a segment containing capabilities."
1977,Beyond Today's Computers.,n/a
1976,Software Engineering and Structured Programming.,"Abstract:
This paper discusses the requirements of programmers working in varying environments in relation to software engineering, structured programming, and program verification."
1976,Software Engineering and Structured Programming (Abstract).,"The paper discusses the requirements of programmers working in varying environments in relation to software engineering, structured programming, and program verification."
1974,Domains of Protection and the Management of Processes.,"The design of a system for process management on a given computer must take into account the facilities built into the hardware for memory protection. The problems arising are discussed from the point of view of the Cambridge CAP system now under development.
"
1973,The Dynamics of Paging.,"A general model of a paging system is given that can be used to describe a wide variety of particular systems referred to in the literature. This model is discussed from the point of view of control engineering, with particular reference to the avoidance of instability (thrashing). A distinction is drawn between different levels of control and between systems that are tuned and those that are under control. The relationship of the approach made in the paper and that via queueing theory is discussed.
"
1973,The Cambridge Multiple-Access System in Retrospect.,"This paper reports on some of the lessons that have been learned during 5 years' operation of the Cambridge Multiple?Access System. It begins by summarizing the design goals and goes on to describe the way in which on?line and off?line facilities have been integrated. There is a section on security of access and one on the maintenance and up?dating of the system. Scheduling of jobs is largely automatic and the operators receive their instructions from the system. Experience obtained in the operation of the system is discussed both from a software and a managerial point of view.
"
1972,On preserving the integrity of data bases.,"It is not usually possible to reconstruct from original documents a large data base that has been in existence for some time, but it must be possible to repair it after an error has occurred. This can only be done if a copy of all information in the data base is kept in some safe place. The paper discusses the use for this purpose of periodical dumps and of various forms of journal. The important role played by the data base manager is emphasised.
"
1972,Associative tabular data structures.,"Abstract
Attention is drawn to a method of implementing data structures in core memory by means of associative links instead of pointers. The properties of associative links are discussed and the way in which they may be exploited in a program for formal differentiation is illustrated. There is a section on microprogramming support for the associative search operations involved."
1972,The associative languageal -1.,"Abstract
A low-level language for manipulating associative tabular data structures is described and, as an example of its use, a program is given for formal differentiation with the removal of common subexpressions. Statistics are presented for this program and for two others of the amount of time spent in table searching and other operations, and an estimate is made of the reduction in running time that could be achieved by microprogramming."
1972,Historical perspectives: computer architecture.,"I shall attempt in this paper to record my own personal impressions of the way in which computer architecture and designers' objectives have changed during the period that has elapsed since the first stored program computers were being designed 25 years ago. I shall be concerned with the generally accepted ""state of the art"" at any time, rather than with the new ideas that were emerging in the more advanced centers. I shall not attempt to track innovations to their source nor to assign dates to them. For convenience of presentation, but for no other reason, I shall divide the period into three phases of very roughly equal duration. I shall end with some remarks about the way thinking has developed on the subject of memory hierarchies, this being one that can be traced through all three phases."
1972,The use of a writable control memory in a multiprogramming environment.,n/a
1971,Slave Memories and Segmentation.,"Abstract:
It is pointed out that in a computer provided with multiple base-limit registers a slave or buffer memory may be used to reduce time spent in address validation and relocation, as well as for its normal purpose of reducing memory access time."
1969,The Growth of Interest in Microprogramming: A Literature Survey.,"The literature is surveyed beginning with the first paper published in 1951. At that time microprogramming was proposed primarily as a means for designing the control unit of an otherwise conventional digital computer, although the possible use of a read/write control memory was noted. The survey reveals the way in which interest has successively developed in the following aspects of the subject: stored logic, the application of microprogramming to the design of a range of computers, emulation, microprogramming in support of software, and read/write control memories. The bibliography includes 55 papers."
1969,A model for core space allocation in a time-sharing system.,"In a time-sharing system that is intended to serve a number of console users simultaneously, there are two related, but distinct, functions to be performed. One is time slicing, which is the allocation of bursts of processor time to the various active programs according to a suitable algorithm. The other is core space allocation which arises because, in a modern multi-programmed system, there will be space in core for more than one active program at the same time. If, as will normally be the case, there are more active programs than can be accommodated in core, some of them must be held on a drum and brought into core periodically; this is swapping. Confusion has sometimes arisen between time slicing and swapping, since, in the early time-sharing systems, there was only one active object program resident in core at any time, all the others being on the drum. In these circumstances, swapping and time slicing go together; when a program is in core, it is receiving processor time, and as soon as it ceases to receive processor time it is removed from core. In a multi-programmed system, however, space allocation and time slicing can proceed independently. It is the responsibility of the space allocation algorithm to ensure that, as far as possible, there is always at least one program in core that is ready to run. The time-slicing algorithm is responsible for dividing up the available processor time between the various programs that are in core."
1968,The Design of Multiple-Access Computer Systems: Part 2.,"In a previous paper, one of the authors discussed some of the hardware and software problems facing the designer of a multiple-access computer system. The present paper carries the discussion further and surveys certain areas of system design that are at present far from being well understood. In particular the problem of communication between processes in a multi-computer configuration is discussed. The authors offer no final solutions, but endeavour to set in perspective some of the problems that must be solved before highly efficient multiple-access systems can be designed.
"
1968,The outer and inner syntax of a programming language.,"It is pointed out that the syntax of a higher level programming language such as ALGOL may be divided into two parts, to which the names outer and inner syntax are given. The outer syntax is concerned with the organisation of the flow of control, and is programmer-oriented, while the inner syntax is concerned with performing operations on data held in the memory of the computer, and is therefore data-oriented. It is shown how in the case of ALGOL it is possible to make a clean separation of the inner and outer syntax, and attention is drawn to certain practical advantages of regarding programming languages in this light.
"
1968,Computers Then and Now.,Reminiscences on the early developments leading to large scale electronic computers show that it took much longer than was expected for the first of the more ambitious and fully engineered computers to be completed and prove themselves in practical operation. Comments on the present computer field assess the needs for future development.
1967,The design of multiple-access computer systems.,"This paper contains a discussion of some of the problems encountered in the design of a large-scale multiple-access computer system. It is written largely from the point of view of the software programmer, but essential hardware requirements receive attention. Some sections of the paper, notably those on the security accorded to confidential information and the protection of the system against unauthorized users, will be of interest to a wider class of readers. There is a section on the new problems involved in the management of a multiple-access computing centre, and attention is drawn to the need for developing the rudiments of a science of computer centre management.
"
1965,Lists and Why They are Useful.,"Computers have long been in general use for solving numerical problems, and pioneering interest has now switched to their use for non-numerical work, that is, for manipulating symbols. Examples are compiling, studies in artificial intelligence, layout problems, etc. List-processing was a breakthrough in symbol manipulation since it provided a flexible way of organizing the computer memory. This paper explains in an expository manner what goes on in the computer memory when list-processing operations are performed, and takes as an example the formal differentiation of an algebraic expression written in Polish notation.
"
1965,Slave Memories and Dynamic Storage Allocation.,"Abstract:
The use is discussed of a fast core memory of, say, 32000 words as a slave to a slower core memory of, say, one million words in such a way that in practical cases the effective access time is nearer that of the fast memory than that of the slow memory."
1964,Constraint-type statements in programming languages.,A proposal is made for including in a programming language statements which imply relations between variables but which are not explicit assignment statements. The compiler sets up a Newtonian iteration making use for the purpose of a routine for formal differentiation.
1964,A programmer's utility filing system.,"A modern trend in computer organization is to provide facilities for a user to keep on magnetic tape or on a disc file all the information that he is likely to require in the immediate future. This includes current programs, old programs likely to be required again, and data of all kinds. PUFS is a system by which a user can create, edit, and up-date files of information which are stored on a magnetic medium in a form which is an exact image of the form in which they are written on paper.
"
1961,Some proposals for improving the efficiency of ALGOL 60.,n/a
1961,Data Transmission and the New Outlook for the Computer Field.,"This paper, which is based on a talk given to The British Computer Society in London on 14 November 1960, describes some of the developments now taking place in data transmission. These are likely to be of immense importance to users of data-processing equipment, especially in the business field.
"
1961,Self-Repairing Computers.,n/a
1958,The Second Decade of Computer Development.,n/a
