2017,"Uniform, integral and efficient proofs for the determinant identities.","Abstract:
We give a uniform and integral version of the short propositional proofs for the determinant identities demonstrated over GF(2) in Hrube≈°-Tzameret [9]. Specifically, we show that the multiplicativity of the determinant function over the integers is provable in the bounded arithmetic theory VNC 2 , which is a first-order theory corresponding to the complexity class NC 2 . This also establishes the existence of uniform polynomial-size and O(log 2 n)-depth Circuit-Frege (equivalently, Extended Frege) proofs over the integers, of the basic determinant identities (previous proofs hold only over GF(2)). In doing so, we give uniform NC 2 -algorithms for homogenizing algebraic circuits, balancing algebraic circuits (given as input an upper bound on the syntactic-degree of the circuit), and converting circuits with divisions into circuits with a single division gate-all (Œ£ 1 B -) definable in VNC 2 . This also implies an NC 2 -algorithm for evaluating algebraic circuits of any depth."
2016,Relativizing small complexity classes and their theories.,"Abstract
Existing definitions of the relativizations of NC 1, L and NL do not preserve the inclusions
NC
1
‚äÜL,NL‚äÜ
AC
1
. We start by giving the first definitions that preserve them. Here for L and NL we define their relativizations using Wilson‚Äôs stack oracle model, but limit the height of the stack to a constant (instead of log(n)). We show that the collapse of any two classes in
{
AC
0
(m),
TC
0
,
NC
1
,L,NL}
{
implies the collapse of their relativizations. Next we exhibit an oracle Œ± that makes AC k (Œ±) a proper hierarchy. This strengthens and clarifies the separations of the relativized theories in Takeuti (1995). The idea is that a circuit whose nested depth of oracle gates is bounded by k cannot compute correctly the (k + 1) compositions of every oracle function. Finally, we develop theories that characterize the relativizations of subclasses of P by modifying theories previously defined by the second two authors. A function is provably total in a theory iff it is in the corresponding relativized class, and hence, the oracle separations imply separations for the relativized theories."
2016,Exponential Lower Bounds for Monotone Span Programs.,"Abstract:
Monotone span programs are a linear-algebraic model of computation which were introduced by Karchmer and Wigderson in 1993 [1]. They are known to be equivalent to linear secret sharing schemes, and have various applications in complexity theory and cryptography. Lower bounds for monotone span programs have been difficult to obtain because they use non-monotone operations to compute monotone functions, in fact, the best known lower bounds are quasipolynomial for a function in (nonmonotone) P [2]. A fundamental open problem is to prove exponential lower bounds on monotone span program size for any explicit function. We resolve this open problem by giving exponential lower bounds on monotone span program size for a function in monotone P. This also implies the first exponential lower bounds for linear secret sharing schemes. Our result is obtained by proving exponential lower bounds using Razborov's rank method [3], a measure that is strong enough to prove lower bounds for many monotone models. As corollaries we obtain new proofs of exponential lower bounds for monotone formula size, monotone switching network size, and the first lower bounds for monotone comparator circuit size for a function in monotone P. We also obtain new polynomial degree lower bounds for Nullstellensatz refutations using an interpolation theorem of Pudlak and Sgall [4]. Finally, we obtain quasipolynomial lower bounds on the rank measure for the st-connectivity function, implying tight bounds for st-connectivity in all of the computational models mentioned above."
2016,Lower Bounds for Nondeterministic Semantic Read-Once Branching Programs.,We prove exponential lower bounds on the size of semantic read-once 3-ary nondeterministic branching programs. Prior to our result the best that was known was for D-ary branching programs with |D| >= 2^{13}.
2014,The Hardness of Being Private.,"Kushilevitz [1989] initiated the study of information-theoretic privacy within the context of communication complexity. Unfortunately, it has been shown that most interesting functions are not privately computable [Kushilevitz 1989, Brandt and Sandholm 2008]. The unattainability of perfect privacy for many functions motivated the study of approximate privacy. Feigenbaum et al. [2010a, 2010b] define notions of worst-case as well as average-case approximate privacy and present several interesting upper bounds as well as some open problems for further study. In this article, we obtain asymptotically tight bounds on the trade-offs between both the worst-case and average-case approximate privacy of protocols and their communication cost for Vickrey auctions.
Further, we relate the notion of average-case approximate privacy to other measures based on information cost of protocols. This enables us to prove exponential lower bounds on the subjective approximate privacy of protocols for computing the Intersection function, independent of its communication cost. This proves a conjecture of Feigenbaum et al. [2010a]."
2014,The complexity of the comparator circuit value problem.,"In 1990, Subramanian [1990] defined the complexity class CC as the set of problems log-space reducible to the comparator circuit value problem (CCV). He and Mayr showed that NL ‚äÜ CC ‚äÜ P, and proved that in addition to CCV several other problems are complete for CC, including the stable marriage problem, and finding the lexicographically first maximal matching in a bipartite graph. Although the class has not received much attention since then, we are interested in CC because we conjecture that it is incomparable with the parallel class NC which also satisfies NL ‚äÜ NC ‚äÜ P, and note that this conjecture implies that none of the CC-complete problems has an efficient polylog time parallel algorithm. We provide evidence for our conjecture by giving oracle settings in which relativized CC and relativized NC are incomparable.
We give several alternative definitions of CC, including (among others) the class of problems computed by uniform polynomial-size families of comparator circuits supplied with copies of the input and its negation, the class of problems AC0-reducible to Ccv, and the class of problems computed by uniform AC0 circuits with AXccv gates. We also give a machine model for CC, which corresponds to its characterization as log-space uniform polynomial-size families of comparator circuits. These various characterizations show that CC is a robust class. Our techniques also show that the corresponding function class FCC is closed under composition. The main technical tool we employ is universal comparator circuits.
Other results include a simpler proof of NL ‚äÜ CC, a more careful analysis showing the lexicographically first maximal matching problem and its variants are CC-complete under AC0 many-one reductions, and an explanation of the relation between the Gale--Shapley algorithm and Subramanian‚Äôs algorithm for stable marriage.
This article continues the previous work of Cook et al. [2011], which focused on Cook-Nguyen style uniform proof complexity, answering several open questions raised in that article."
2013,Theories for Subexponential-size Bounded-depth Frege Proofs.,"This paper is a contribution to our understanding of the relationship between uniform and nonuniform proof complexity. The latter studies the lengths of proofs in various propositional proof systems such as Frege and bounded-depth Frege systems, and the former studies the strength of the corresponding logical theories such as VNC1 and V0 in [Cook/Nguyen, 2010]. A superpolynomial lower bound on the length of proofs in a propositional proof system for a family of tautologies expressing a result like the pigeonhole principle implies that the result is not provable in the theory associated with the propositional proof system. We define a new class of bounded arithmetic theories n^epsilon-ioV^\infinity for epsilon < 1 and show that they correspond to complexity classes AltTime(O(1),O(n^epsilon)), uniform classes of subexponential-size bounded-depth circuits DepthSize(O(1),2^O(n^epsilon)). To accomplish this we introduce the novel idea of using types to control the amount of composition in our bounded arithmetic theories. This allows our theories to capture complexity classes that have weaker closure properties and are not closed under composition. We show that the proofs of Sigma^B_0-theorems in our theories translate to subexponential-size bounded-depth Frege proofs. We use these theories to formalize the complexity theory result that problems in uniform NC1 circuits can be computed by uniform subexponential bounded-depth circuits in [Allender/Koucky, 2010]. We prove that our theories contain a variation of the theory VNC1 for the complexity class NC1. We formalize Buss's proof in [Buss, 1993] that the (unbalanced) Boolean Formula Evaluation problem is in NC1 and use it to prove the soundness of Frege systems. As a corollary, we obtain an alternative proof of [Filmus et al, ICALP, 2011] that polynomial-size Frege proofs can be simulated by subexponential-size bounded-depth Frege proofs. Our results can be extended to theories corresponding to other nice complexity classes inside NTimeSpace(n^O(1), n^o(1)) such as NL. This is achieved by essentially formalizing the containment NTimeSpace(n^O(1), n^o(1)) \subseteq AltTime(O(1), O(n^epsilon)) for all epsilon > 0."
2013,Average Case Lower Bounds for Monotone Switching Networks.,"Abstract:
An approximate computation of a Boolean function by a circuit or switching network is a computation in which the function is computed correctly on the majority of the inputs (rather than on all inputs). Besides being interesting in their own right, lower bounds for approximate computation have proved useful in many sub areas of complexity theory, such as cryptography and derandomization. Lower bounds for approximate computation are also known as correlation bounds or average case hardness. In this paper, we obtain the first average case monotone depth lower bounds for a function in monotone P. We tolerate errors that are asymptotically the best possible for monotone circuits. Specifically, we prove average case exponential lower bounds on the size of monotone switching networks for the GEN function. As a corollary, we separate the monotone NC hierarchy in the case of errors -- a result which was previously only known for exact computations. Our proof extends and simplifies the Fourier analytic technique due to Potechin, and further developed by Chan and Potechin. As a corollary of our main lower bound, we prove that the communication complexity approach for monotone depth lower bounds does not naturally generalize to the average case setting."
2012,Formal Theories for Linear Algebra.,"We introduce two-sorted theories in the style of [CN10] for the complexity classes \oplusL and DET, whose complete problems include determinants over Z2 and Z, respectively. We then describe interpretations of Soltys' linear algebra theory LAp over arbitrary integral domains, into each of our new theories. The result shows equivalences of standard theorems of linear algebra over Z2 and Z can be proved in the corresponding theory, but leaves open the interesting question of whether the theorems themselves can be proved.
"
2012,The Complexity of Proving the Discrete Jordan Curve Theorem.,"The Jordan curve theorem (JCT) states that a simple closed curve divides the plane into exactly two connected regions. We formalize and prove the theorem in the context of grid graphs, under different input settings, in theories of bounded arithmetic that correspond to small complexity classes. The theory V0(2) (corresponding to AC0(2)) proves that any set of edges that form disjoint cycles divides the grid into at least two regions. The theory V0 (corresponding to AC0) proves that any sequence of edges that form a simple closed curve divides the grid into exactly two regions. As a consequence, the Hex tautologies and the st-connectivity tautologies have polynomial size AC0(2)-Frege-proofs, which improves results of Buss which only apply to the stronger proof system TC0-Frege."
2012,Pebbles and Branching Programs for Tree Evaluation.,"We introduce the tree evaluation problem, show that it is in LogDCFL (and hence in P), and study its branching program complexity in the hope of eventually proving a superlogarithmic space lower bound. The input to the problem is a rooted, balanced d-ary tree of height h, whose internal nodes are labeled with d-ary functions on [k]‚Äâ=‚Äâ{1,..., k}, and whose leaves are labeled with elements of [k]. Each node obtains a value in [k] equal to its d-ary function applied to the values of its d children. The output is the value of the root. We show that the standard black pebbling algorithm applied to the binary tree of height h yields a deterministic k-way branching program with O(kh) states solving this problem, and we prove that this upper bound is tight for h‚Äâ=‚Äâ2 and h‚Äâ=‚Äâ3. We introduce a simple semantic restriction called thrifty on k-way branching programs solving tree evaluation problems and show that the same state bound of Œò(kh) is tight for all h‚Äâ‚â•‚Äâ2 for deterministic thrifty programs. We introduce fractional pebbling for trees and show that this yields nondeterministic thrifty programs with Œò(kh/2+1) states solving the Boolean problem ‚Äúdetermine whether the root has value 1‚Äù, and prove that this bound is tight for h‚Äâ=‚Äâ2,3,4. We also prove that this same bound is tight for unrestricted nondeterministic k-way branching programs solving the Boolean problem for h‚Äâ=‚Äâ2,3."
2012,Complexity Theory for Operators in Analysis.,"We propose an extension of the framework for discussing the computational complexity of problems involving uncountably many objects, such as real numbers, sets and functions, that can be represented only through approximation. The key idea is to use a certain class of string functions as names representing these objects. These are more expressive than infinite sequences, which served as names in prior work that formulated complexity in more restricted settings. An advantage of using string functions is that we can define their size in a way inspired by higher-type complexity theory. This enables us to talk about computation on string functions whose time or space is bounded polynomially in the input size, giving rise to more general analogues of the classes P, NP, and PSPACE. We also define NP- and PSPACE-completeness under suitable many-one reductions.
Because our framework separates machine computation and semantics, it can be applied to problems on sets of interest in analysis once we specify a suitable representation (encoding). As prototype applications, we consider the complexity of functions (operators) on real numbers, real sets, and real functions. For example, the task of numerical algorithms for solving a certain class of differential equations is naturally viewed as an operator taking real functions to real functions. As there was no complexity theory for operators, previous results only stated how complex the solution can be. We now reformulate them and show that the operator itself is polynomial-space complete."
2012,The Hardness of Being Private.,"Abstract:
In 1989 Kushilevitz initiated the study of information-theoretic privacy within the context of communication complexity. Unfortunately, it has been shown that most interesting functions are not privately computable. The unattainability of perfect privacy for many functions motivated the study of approximate privacy. Feigenbaum et al. define notions of worst-case as well as average-case approximate privacy, and present several interesting upper bounds, and some open problems for further study. In this paper, we obtain asymptotically tight bounds on the tradeoffs between both the worst-case and average-case approximate privacy of protocols and their communication cost for Vickrey-auctions. Further, we relate the notion of average-case approximate privacy to other measures based on information cost of protocols. This enables us to prove exponential lower bounds on the subjective approximate privacy of protocols for computing the Intersection function, independent of its communication cost. This proves a conjecture of Feigenbaum et al."
2011,Formalizing Randomized Matching Algorithms.,"Using Je\v{r}·bek 's framework for probabilistic reasoning, we formalize the correctness of two fundamental RNC^2 algorithms for bipartite perfect matching within the theory VPV for polytime reasoning. The first algorithm is for testing if a bipartite graph has a perfect matching, and is based on the Schwartz-Zippel Lemma for polynomial identity testing applied to the Edmonds polynomial of the graph. The second algorithm, due to Mulmuley, Vazirani and Vazirani, is for finding a perfect matching, where the key ingredient of this algorithm is the Isolating Lemma.
"
2011,A Formal Theory for the Complexity Class Associated with the Stable Marriage Problem.,"Subramanian defined the complexity class CC as the set of problems log-space reducible to the comparator circuit value problem. He proved that several other problems are complete for CC, including the stable marriage problem, and finding the lexicographical first maximal matching in a bipartite graph. We suggest alternative definitions of CC based on different reducibilities and introduce a two-sorted theory VCC* based on one of them. We sharpen and simplify Subramanian's completeness proofs for the above two problems and formalize them in VCC*."
2011,Formalizing Randomized Matching Algorithms.,"Abstract:
Using Jer√°bek's framework for probabilistic reasoning, we formalize the correctness of two fundamental RNC 2 algorithms for bipartite perfect matching within the theory VPV for polytime reasoning. The first algorithm is for testing if a bipartite graph has a perfect matching, and is based on the Schwartz-Zippel Lemma for polynomial identity testing applied to the Edmonds polynomial of the graph. The second algorithm, due to Mulmuley, Vazirani and Vazirani, is for finding a perfect matching, where the key ingredient of this algorithm is the Isolating Lemma."
2010,Formal Theories for Linear Algebra.,"Abstract
We introduce two-sorted theories in the style of [CN10] for the complexity classes ‚äï‚ÄâL and DET, whose complete problems include determinants over ‚Ñ§2 and ‚Ñ§, respectively. We then describe interpretations of Soltys‚Äô linear algebra theory LAP over arbitrary integral domains, into each of our new theories. The result shows equivalences of standard theorems of linear algebra over ‚Ñ§2 and ‚Ñ§ can be proved in the corresponding theory, but leaves open the interesting question of whether the theorems themselves can be proved."
2010,Complexity theory for operators in analysis.,"We propose a new framework for discussing computational complexity of problems involving uncountably many objects, such as real numbers, sets and functions, that can be represented only through approximation. The key idea is to use a certain class of string functions, which we call regular functions, as names representing these objects. These are more expressive than infinite sequences, which served as names in prior work that formulated complexity in more restricted settings. An important advantage of using regular functions is that we can define their size in the way inspired by higher-type complexity theory. This enables us to talk about computation on regular functions whose time or space is bounded polynomially in the input size, giving rise to more general analogues of the classes P, NP, and PSPACE. We also define NP- and PSPACE-completeness under suitable many-one reductions.
Because our framework separates machine computation and semantics, it can be applied to problems on sets of interest in analysis once we specify a suitable representation (encoding). As prototype applications, we consider the complexity of functions (operators) on real numbers, real sets, and real functions. The latter two cannot be represented succinctly using existing approaches based on infinite sequences, so ours is the first treatment of them. As an interesting example, the task of numerical algorithms for solving the initial value problem of differential equations is naturally viewed as an operator taking real functions to real functions. As there was no complexity theory for operators, previous results could only state how complex the solution can be. We now reformulate them to show that the operator itself is polynomial-space complete."
2009,Fractional Pebbling and Thrifty Branching Programs.,"We study the branching program complexity of the {\em tree evaluation problem}, introduced in \cite{BrCoMcSaWe09} as a candidate for separating \nl\ from\logcfl. The input to the problem is a rooted, balanced $d$-ary tree of height$h$, whose internal nodes are labelled with $d$-ary functions on$[k]=\{1,\ldots,k\}$, and whose leaves are labelled with elements of $[k]$.Each node obtains a value in $[k]$ equal to its $d$-ary function applied to the values of its $d$ children. The output is the value of the root. Deterministic $k$-way branching programs as related to black pebbling algorithms have been studied in \cite{BrCoMcSaWe09}. Here we introduce the notion of {\em fractional pebbling} of graphs to study non-deterministicbranching program size. We prove that this yields non-deterministic branching programs with $\Theta(k^{h/2+1})$ states solving the Boolean problem ``determine whether the root has value 1'' for binary trees - this isasymptotically better than the branching program size corresponding toblack-white pebbling. We prove upper and lower bounds on the fractionalpebbling number of $d$-ary trees, as well as a general result relating thefractional pebbling number of a graph to the black-white pebbling number. We introduce a simple semantic restriction called {\em thrifty} on $k$-way branching programs solving tree evaluation problems and show that the branchingprogram size bound of $\Theta(k^h)$ is tight (up to a constant factor) for all $h\ge 2$ for deterministic thrifty programs. We show that thenon-deterministic branching programs that correspond to fractional pebbling are thrifty as well, and that the bound of $\Theta(k^{h/2+1})$ is tight for non-deterministic thrifty programs for $h=2,3,4$. We hypothesise that thrifty branching programs are optimal among $k$-way branching programs solving the tree evaluation problem - proving this for deterministic programs would separate \lspace\ from \logcfl\, and proving it for non-deterministic programs would separate \nl\ from \logcfl."
2009,Branching Programs for Tree Evaluation.,"Abstract
The problem
F
T
h
d
(k)
F
consists in computing the value in [k]‚Äâ=‚Äâ{1,...,k} taken by the root of a balanced d-ary tree of height h whose internal nodes are labelled with d-ary functions on [k] and whose leaves are labelled with elements of [k]. We propose
F
T
h
d
(k)
F
as a good candidate for witnessing
L‚ääLogDCFL
. We observe that the latter would follow from a proof that k-way branching programs solving
F
T
h
d
(k)
F
require
Œ©(
k
\scriptsize unbounded function(h)
)
Œ©
size. We introduce a ‚Äústate sequence‚Äù method that can match the size lower bounds on
F
T
h
d
(k)
F
obtained by the NecÃÜiporuk method and can yield slightly better (yet still subquadratic) bounds for some nonboolean functions. Both methods yield the tight bounds Œò(k 3) and Œò(k 5/2) for deterministic and nondeterministic branching programs solving
F
T
3
2
(k)
F
respectively. We propose as a challenge to break the quadratic barrier inherent in the NecÃÜiporuk method by adapting the state sequence method to handle
F
T
4
d
(k)
F
."
2007,Consequences of the provability of NP ‚äÜ P/poly.,"We prove the following results: (i) PV proves NP ? P/poly iff PV proves coNP ? NP/O(1). (ii) If PV proves NP ? P/poly then PV proves that the Polynomial Hierarchy collapses to the Boolean Hierarchy, (iii)  proves NP ? P/poly iff proves coNP ? NP/O(log n). (iv) If  proves NP ? P/poly then  proves that the Polynomial Hierarchy collapses to PNP[log n]. (v) If  proves NP ? P/poly then  proves that the Polynomial Hierarchy collapses to PNP.

Motivated by these results we introduce a new concept in proof complexity: proof systems with advice, and we make some initial observations about them."
2007,Relativizing Small Complexity Classes and Their Theories.,"Abstract
Existing definitions of the relativizations of NC 1, L and NL do not preserve the inclusions NC 1‚Äâ‚äÜ‚ÄâL, NL‚Äâ‚äÜ‚ÄâAC 1. We start by giving the first definitions that preserve them. Here for L and NL we define their relativizations using Wilson‚Äôs stack oracle model, but limit the height of the stack to a constant (instead of log(n)). We show that the collapse of any two classes in {AC 0(m), TC 0, NC 1, L, NL} implies the collapse of their relativizations. Next we develop theories that characterize the relativizations of subclasses of P by modifying theories previously defined by the second two authors. A function is provably total in a theory iff it is in the corresponding relativized class. Finally we exhibit an oracle Œ± that makes AC k (Œ±) a proper hierarchy. This strengthens and clarifies the separations of the relativized theories in [Takeuti, 1995]. The idea is that a circuit whose nested depth of oracle gates is bounded by k cannot compute correctly the (k‚Äâ+‚Äâ1) compositions of every oracle function."
2007,The Complexity of Proving the Discrete Jordan Curve Theorem.,"Abstract:
The Jordan Curve Theorem (JCT) states that a simple closed curve divides the plane into exactly two connected regions. We formalize and prove the theorem in the context of grid graphs, under different input settings, in theories of bounded arithmetic that correspond to small complexity classes. The theory V 0 (corresponding to AC 0 (2)) proves that any set of edges that form disjoint cycles divides the grid into at least two regions. The theory V 0 (corresponding to AC 0 ) proves that any sequence of edges that form a simple closed curve divides the grid into exactly two regions. As a consequence, the Hex tautologies and the st-Connectivity tautologies have polynomial size AC 0 (2)-Frege-proofs, which improves results of Buss which only apply to the stronger proof system TC 0 -Frege."
2006,Theories for TC0 and Other Small Complexity Classes.,"We present a general method for introducing finitely axiomatizable ""minimal"" two-sorted theories for various subclasses of P (problems solvable in polynomial time). The two sorts are natural numbers and finite sets of natural numbers. The latter are essentially the finite binary strings, which provide a natural domain for defining the functions and sets in small complexity classes. We concentrate on the complexity class TC^0, whose problems are defined by uniform polynomial-size families of bounded-depth Boolean circuits with majority gates. We present an elegant theory VTC^0 in which the provably-total functions are those associated with TC^0, and then prove that VTC^0 is ""isomorphic"" to a different-looking single-sorted theory introduced by Johannsen and Pollet. The most technical part of the isomorphism proof is defining binary number multiplication in terms a bit-counting function, and showing how to formalize the proofs of its algebraic properties.
"
2006,The strength of replacement in weak arithmetic.,"The replacement (or collection or choice) axiom scheme BB(Œì) asserts bounded quantifier exchange as follows: ‚àÄi < |a| ‚àÉx < aœÜ(i,x) ‚Üí ‚àÉw ‚àÄi < |a|œÜ(i,[w]i), for œÜ in the class Œì of formulas. The theory S12 proves the scheme BB(Œ£b1), and thus in S12 every Œ£b1 formula is equivalent to a strict Œ£b1 formula (in which all non-sharply-bounded quantifiers are in front). Here we prove (sometimes subject to an assumption) that certain theories weaker than S12 do not prove either BB(Œ£b1) or BB(Œ£b0). We show (unconditionally) that V 0 does not prove BB(Œ£b0), where V0 (essentially IŒ£1,b0) is the two-sorted theory associated with the complexity class AC0. We show that PV does not prove BB(Œ£b0), assuming that integer factoring is not possible in probabilistic polynomial time. Johannsen and Pollett introduced the theory C02 associated with the complexity class TC0, and later introduced an apparently weaker theory Œîb1 ‚àí CR for the same class. We use our methods to show that Œîb1 ‚àí CR is indeed weaker than C02, assuming that RSA is secure against probabilistic polynomial time attack.Our main tool is the KPT witnessing theorem."
2005,Quantified propositional calculus and a second-order theory for NC1.,"Let H be a proof system for quantified propositional calculus (QPC). We define the ? q j -witnessing problem for H to be: given a prenex ? q j -formula A, an H-proof of A, and a truth assignment to the free variables in A, find a witness for the outermost existential quantifiers in A. We point out that the ? q 1 -witnessing problems for the systems G * 1 and G1 are complete for polynomial time and PLS (polynomial local search), respectively.
We introduce and study the systems G * 0 and G0, in which cuts are restricted to quantifier-free formulas, and prove that the ? q j -witnessing problem for each is complete for NC1. Our proof involves proving a polynomial time version of Gentzenís midsequent theorem for G * 0 and proving that G0-proofs are TC0-recognizable. We also introduce QPC systems for TC0 and prove witnessing theorems for them.
We introduce a finitely axiomatizable second-order system VNC1 of bounded arithmetic which we prove isomorphic to Araiís first order theory AID + ? b 0 -CA for uniform NC1. We describe simple translations of VNC1 proofs of all bounded theorems to polynomial size families of G * 0 proofs. From this and the above theorem we get alternative proofs of the NC1 witnessing theorems for VNC1 and AID.
"
2004,The proof complexity of linear algebra.,"Abstract
We introduce three formal theories of increasing strength for linear algebra in order to study the complexity of the concepts needed to prove the basic theorems of the subject. We give what is apparently the first feasible proofs of the Cayley‚ÄìHamilton theorem and other properties of the determinant, and study the propositional proof complexity of matrix identities such as AB=I‚ÜíBA=I."
2004,The Strength of Replacement in Weak Arithmetic.,"Abstract:
The replacement (or collection or choice,) axiom scheme BB(/spl Gamma/) asserts bounded quantifier exchange as follows: /spl forall/I < |a| /spl exist/x < ao(i, x) /spl rarr/ /spl exist/w /spl forall/i < |a| o (i, [w]/sub i/) where o is in the class /spl Gamma/ of formulas. The theory S/sub 2//sup 1/ proves the scheme BB(/spl Sigma//sub 1//sup b/), and thus in S/sub 2//sup 1/ every /spl Sigma//sub 1//sup b/ formula is equivalent to a strict /spl Sigma//sub 1//sup b/ formula (in which all non-sharply-bounded quantifiers are in front). Here we prove (sometimes subject to an assumption) that certain theories weaker than S/sub 2//sup 1/ do not prove either BB(/spl Sigma//sub 1//sup b/) or BB(/spl Sigma//sub 0//sup b/). We show (unconditionally) that V/sup 0/ does not prove BB(/spl Sigma//sub 1//sup B/), where V/sup 0/ (essentially I/spl Sigma//sub 0//sup 1,b/) is the two-sorted theory associated with the complexity class AC/sup 0/. We show that PV does not prove BB(/spl Sigma//sub 0//sup b/), assuming that integer factoring is not possible in probabilistic polynomial time. Johannsen and Pollet introduced the theory C/sub 2//sup 0/ associated with the complexity class TC/sup 0/, and later introduced an apparently weaker theory /spl Delta//sub 1//sup b/ - CR for the same class. We use our methods to show that /spl Delta//sub 1//sup b/ - CR is indeed weaker than C/sub 2//sup 0/, assuming that RSA is secure against probabilistic polynomial time attack. Our main tool is the KPT witnessing theorem."
2004,VTC circ: A Second-Order Theory for TCcirc.,"Abstract:
We introduce a finitely axiomatizable second-order theory, which is VTC/sup 0/ associated with the class FO-uniform TC/sup 0/. It consists of the base theory V/sup 0/ for AC/sup 0/ reasoning together with the axiom NUMONES, which states the existence of a ""counting array"" Y for any string X: the ith row of Y contains only the number of 1 bits up to (excluding) bit i of X. We introduce the notion of ""strong /spl Delta//sub 1//sup B/-definability"" for relations in a theory, and use a recursive characterization of the TC/sup 0/ relations (rather than functions) to show that the TC/sup 0/ relations are strongly /spl Delta//sub 1//sup B/-definable. It follows that the TC/sup 0/ functions are /spl Sigma//sub 1//sup B/-definable in VTC/sup 0/. We prove a general witnessing theorem for second-order theories and conclude that the/spl Sigma//sub 1//sup B/ theorems of VTC/sup 0/ are witnessed by TC/sup 0/ functions. We prove that VTC/sup 0/ is RSUV isomorphic to the first order theory /spl Delta//sub 1//sup b/-CR of Johannsen and Pollett (the ""minimal theory for TC/sup 0/""), /spl Delta//sub 1//sup b/-CR includes the /spl Delta//sub 1//sup b/ comprehension rule, and J and P ask whether there is an upper bound to the nesting depth required for this rule. We answer ""yes"", because VTC/sup 0/ , and therefore /spl Delta//sub 1//sup b/-CR, are finitely axiomatizable. Finally, we show that /spl Sigma//sub 1//sup B/ theorems of VTC/sup 0/ translate to families of tautologies which have polynomial-size constant-depth TC/sup 0/-Frege proofs. We also show that PHP is a /spl Sigma//sub 0//sup B/ theorem of VTC/sup 0/. These together imply that the family of propositional tautologies associated with PHP has polynomial-size constant-depth TC/sup 0/-Frege proofs."
2004,A Second-Order Theory for NL.,"Abstract:
We introduce a second-order theory V-Krom of bounded arithmetic for nondeterministic log space. This system is based on Gradel's characterization of NL by second-order Krom formulae with only universal first-order quantifiers, which in turn is motivated by the result that the decision problem for 2-CNF satisfiability is complete for coNL (and hence for NL). This theory has the style of the authors' theory Vi-Horn [APAL 124 (2003)] for polynomial time. Both theories use Zambella's elegant second-order syntax, and are axiomatized by a set 2-BASIC of simple formulae, together with a comprehension scheme for either second-order Horn formulae (in the case of V/sub 1/-Horn), or second-order Krom (2CNF) formulae (in the case of V-Krom). Our main result for V-Krom is a formalization of the Immerman-Szelepcsenyi theorem that NL is closed under complementation. This formalization is necessary to show that the NL functions are /spl Sigma//sub 1//sup B/-definable in V-Krom. The only other theory for NL in the literature relies on the Immerman-Szelepcsenyi's result rather than proving it."
2003,A second-order system for polytime reasoning based on Gr√§del's theorem.,"Abstract
We introduce a second-order system V1-Horn of bounded arithmetic formalizing polynomial-time reasoning, based on Gr√§del's (Theoret. Comput. Sci. 101 (1992) 35) second-order Horn characterization of P. Our system has comprehension over P predicates (defined by Gr√§del's second-order Horn formulas), and only finitely many function symbols. Other systems of polynomial-time reasoning either allow induction on NP predicates (such as Buss's S21 or the second-order V11), and hence are more powerful than our system (assuming the polynomial hierarchy does not collapse), or use Cobham's theorem to introduce function symbols for all polynomial-time functions (such as Cook's PV and Zambella's P-def). We prove that our system is equivalent to QPV and Zambella's P-def. Using our techniques, we also show that V1-Horn is finitely axiomatizable, and, as a corollary, that the class of ‚àÄŒ£1b consequences of S21 is finitely axiomatizable as well, thus answering an open question."
2003,The importance of the P versus NP question.,n/a
2003,A Complete Axiomatization for Blocks World.,n/a
2002,The optimal location of replicas in a network using a READ-ONE-WRITE-ALL policy.,"Abstract.
We consider the problem of locating replicas in a network to minimize communications costs. Under the assumption that the read-one-write-all policy is used to ensure data consistency, an optimization problem is formulated in which the cost function estimates the total communications costs. The paper concentrates on the study of the optimal communications cost as a function of the ratio between the frequency of the read and write operations. The problem is reformulated as a zero-one linear programming problem, and its connection to the p-median problem is explained. The general problem is proved to be NP-complete. For path graphs a dynamic programming algorithm for the problem is presented."
2002,A Complete Axiomatization for Blocks World.,"Blocks World (BW) has been one of the most popular model domains in AI history. However, there has not been serious work on axiomatizing the state constraints of BW and giving justification for its soundness and completeness. In this paper, we model a state of BW by a finite collection of finite chains, and call the theory of all these structures BW theory. We present seven simple axioms and prove that their consequences are precisely BW theory, using Ehrenfeucht-Fr‰?ssÈ games. We give a simple decision procedure for the theory which can be implemented in exponential space, and prove that every decision procedure (even if nondeterministic) for the theory must take at least exponential time. We also give a characterization of all nonstandard models for the theory. Finally, we present an expansion of BW theory and show that it admits elimination of quantifiers. As a result, we are able to characterize all definable predicates in BW theory, and give simple examples of undefinable predicates."
2002,The Proof Complexity of Linear Algebra.,"Abstract:
We introduce three formal theories of increasing strength for linear algebra in order to study the complexity of the concepts needed to prove the basic theorems of the subject. We give what is apparently the first feasible proofs of the Cayley-Hamilton theorem and other properties of the determinant, and study the propositional proof complexity of matrix identities."
2001,A Second-Order System for Polytime Reasoning Using Graedel's Theorem.,"Abstract:
We introduce a second-order system V/sub 1/-Horn of bounded arithmetic formalizing polynomial-time reasoning, based on Gradel's (1992) second-order Horn characterization of P. Our system has comprehension over P predicates (defined by Gradel's second-order Horn formulas), and only finitely, many function symbols. Other systems of polynomial-time reasoning either allow induction on NP predicates (such as Buss's (1986) S/sub 2//sup 1/ or the second-order V/sub 1//sup 1/), and hence are more powerful than our system (assuming the polynomial hierarchy does not collapse), or use Cobham's theorem to introduce function symbols for all polynomial-time functions (such as Cook's PV and Zambella's P-def). We prove that our system is equivalent to QPV and Zambella's (1996) P-def. Using our techniques, we also show that V/sub 1/-Horn is finitely, axiomatizable, and, as a corollary, that the class of /spl forall//spl Sigma//sub 1//sup b/ consequences of S/sub 2//sup 1/ is finitely axiomatizable as well, thus answering an open question."
1999,An Exponential Lower Bound for the Size of Monotone Real Circuits.,"Abstract
We prove a lower bound, exponential in the eighth root of the input length, on the size of monotone arithmetic circuits that solve an NP problem related to clique detection. The result is more general than the famous lower bound of Razborov and Andreev, because the gates of the circuit are allowed to compute arbitrary monotone binary real-valued functions (including AND and OR). Our proof is relatively simple and direct and uses the method of counting bottlenecks. The generalization was proved independently by Pudl√°k using a different method, who also showed that the result can be used to obtain an exponential lower bound on the size of unrestricted cutting plane proofs in the propositional calculus."
1998,The Relative Complexity of NP Search Problems.,"Abstract
Papadimitriou introduced several classes of NP search problems based on combinatorial principles which guarantee the existence of solutions to the problems. Many interesting search problems not known to be solvable in polynomial time are contained in these classes, and a number of them are complete problems. We consider the question of the relative complexity of these search problem classes. We prove several separations which show that in a generic relativized world the search classes are distinct and there is a standard search problem in each of them that is not computationally equivalent to any decision problem. (Naturally, absolute separations would imply thatP‚â†NP.) Our separation proofs have interesting combinatorial content and go to the heart of the combinatorial principles on which the classes are based. We derive one result via new lower bounds on the degrees of poly- nomials asserted to exist by Hilbert's nullstellensatz over finite fields."
1997,A Tight Relationship Between Generic Oracles and Type-2 Complexity Theory.,"Abstract
We show that any two complexity classes satisfying some general conditions are distinct relative to a generic oracle iff the corresponding type-2 classes are distinct."
1996,A New Characterization of Type-2 Feasibility.,"K. Mehlhorn introduced a class of polynomial-time-computable operators in order to study poly-time reducibilities between functions. This class is defined using a generalization of A. Cobham's definition of feasibility for type-1 functions to type-2 functionals. Cobham's feasible functions are equivalent to the familiar poly-time functions. We generalize this equivalence to type-2 functionals. This requires a definition of the notion ìpoly time in the length of type-1 inputs.î The proof of this equivalence is not a simple generalization of the proof for type-1 functions; it depends on the fact that Mehlhornís class is closed under a strong form of simultaneous limited recursion on notation and requires an analysis of the structure of oracle queries in time-bounded computations.

"
1996,Finding hard instances of the satisfiability problem: A survey.,"Finding sets of hard instances of propositional satisfiability is of interest for understanding the complexity of SAT, and for experimentally evaluating SAT algorithms. In discussing this we consider the performance of the most popular SAT algorithms on random problems, the theory of average case complexity the threshold phenomenon, known lower bounds for certain classes of algorithms, and the problem of generating hard instances with solutions."
1995,The relative complexity of NP search problems.,"Papadimitriou introduced several classes of NP search problems based on combinatorial principles which guarantee the existence of solutions to the problems. Many interesting search problems not known to be solvable in polynomial time are contained in these classes, and a number of them are complete problems. We consider the question of the relative complexity of these search problem classes. We prove several separations which show that in a generic relativized world the search classes are distinct and there is a standard search problem in each of them that is not computationally equivalent to any decision problem. (Naturally, absolute separations would imply thatP?NP.) Our separation proofs have interesting combinatorial content and go to the heart of the combinatorial principles on which the classes are based. We derive one result via new lower bounds on the degrees of poly- nomials asserted to exist by Hilbert's nullstellensatz over finite fields."
1993,Functional Interpretations of Feasibly Constructive Arithmetic.,"Abstract
A notion of feasible function of finite type based on the typed lambda calculus is introduced which generalizes the familiar type 1 polynomial-time functions. An intuitionistic theory IPVœâ is presented for reasoning about these functions. Interpretations for IPVœâ are developed both in the style of Kreisel's modified realizability and G√∂del's Dialectica interpretation. Applications include alternative proofs for Buss's results concerning the classical first-order system S12 and its intuitionistic counterpart IS12 as well as proofs of some of Buss's conjectures concerning IS12, and a proof that IS12 cannot prove that extended Frege systems are not polynomially bounded."
1993,Parallel Pointer Machines.,"The parallel pointer machine is a synchronous collection of finite state transducers, each transducer receiving its inputs via pointers to the other transducers. Each transducer may change its input pointers dynamically by ìpointer jumpingî. These machines provide a simple example of a parallel model with a time-varying processor inter-connection structure, and are sufficiently powerful to simulate deterministic spaceS(n) within timeO(S(n)).
"
1992,A New Recursion-Theoretic Characterization of the Polytime Functions.,"Abstract
We give a recursion-theoretic characterization of FP which describes polynomial time computation independently of any externally imposed resource bounds. In particular, this syntactic characterization avoids the explicit size bounds on recursion (and the initial function 2|x|¬∑|y|) of Cobham."
1992,An Optimal Parallel Algorithm for Formula Evaluation.,"A new approach to Bussís ${\textbf{NC}}^1 $ algorithm [Proc. 19th ACM Symposium on Theory of Computing, Association for Computing Machinery, New York, 1987, pp. 123ñ131] for evaluation of Boolean formulas is presented. This problem is shown to be complete for ${\textbf{NC}}^1 $ over ${\textbf{AC}}^0 $ reductions. This approach is then used to solve the more general problem of evaluating arithmetic formulas by using arithmetic circuits.
"
1992,The optimal placement of replicas in a network using a read any - write all policy.,"This paper studies the problem of the optimal placement of replicas in a network so that communications costs are minimized when a strict consistency policy is in effect. A READ ANYWRITE ALL policy is introduced, and the appropriate cost function is developed to measure the relative effectiveness of each choice of replica placement. The general problem is proven to be NP complete. An efficient polynomial-time dynamic programming algorithm is introduced for linear graphs. A zero-one linear programming solution is provided for the general problem, using the cost function introduced here. A parametric approach is taken to the problem in the situation in which all activities of each node are identical. This leads to the introduction of piecewise linear solutions to particular problems and the existence of an underlying enveloping curve for the problem.The approach taken to the problem here is to examine simple models to develop some understanding of the issues. This has led to a number of interesting and challenging mathematical questions."
1992,A New Recursion-Theoretic Characterization of the Polytime Functions (Extended Abstract).,"We give a recursion-theoretic characterization of FP which describes polynomial time computation independently of any externally imposed resource bounds. In particular, this syntactic characterization avoids the explicit size bounds on recursion (and the initial function 2|x|.|y|) of Cobham."
1991,A New Characterization of Mehlhorn's Polynomial Time Functionals (Extended Abstract).,"Abstract:
A. Cobham (1964) presented a machine-independent characterization of computational feasibility, via inductive definition. R. Constable (1973) was apparently the first to consider the notion of feasibility for type 2 functionals. K. Mehlhorn's (1976) study of feasible reducibilities proceeds from Constable's work. Here, a class of polytime operators is defined, using a generalization of Cobham's definition. The authors provide an affirmative answer to the question of whether there is a natural machine based definition of Mehlhorn's class.< >"
1990,A Feasibly Constructive Lower Bound for Resolution Proofs.,n/a
1989,Complexity Theory of Parallel Time and Hardware.,"Abstract
The parallel resources time and hardware and the complexity classes defined by them are studied using the aggregate model. The equivalence of complexity classes defined by sequential space and uniform aggregate hardware is established. Aggregate time is related to (bounded fanin) circuit depth and, similarly, aggregate hardware is related to circuit width. Interelationships between aggregate time and hardware follow as corollaries. Aggregate time is related to the sequential resource reversal. Simultaneous relationships from aggregate hardware and time to sequential space and reversal are shown (and conversely), and these are used as evidence for an ‚Äúextended parallel computation thesis.‚Äù These simultaneous relationships provide new characterizations for the simultaneous parallel complexity class NC and for the complementary class SC. The evaluation of monotone planar circuits is shown to be in NC, in fact in LOGCFL."
1989,Two Applications of Inductive Counting for Complementation Problems.,"Following the recent independent proofs of Immerman [SIAM J. Comput., 17 (1988), pp. 935ñ938] and Szelepcsenyi [Bull. European Assoc. Theoret. Comput. Sci., 33 (1987), pp. 96ñ100] that nondeterministic space-bounded complexity classes are closed under complementation, two further applications of the inductive counting technique are developed. First, an errorless probabilistic algorithm for the undirected graph s-t connectivity problem that runs in $O(\log n)$ space and polynomial expected time is given. Then it is shown that the class LOGCFL is closed under complementation. The latter is a special case of a general result that shows closure under complementation of classes defined by semi-unbounded fan-in circuits (or, equivalently, nondeterministic auxiliary pushdown automata or tree-size bounded alternating Turing machines). As one consequence, it is shown that small numbers of ìrole switchesî in two-person pebbling can be eliminated."
1989,Erratum: Two Applications of Inductive Counting for Complementation Problems.,
1989,Characterizations of the Basic Feasible Functionals of Finite Type (Extended Abstract).,"Abstract:
The authors define a simple typed while-programming language that generalizes the sort of simple language used in computability texts to define the familiar numerical computable functions and corresponds roughly to the mu -recursion of R.O. Gandy (1967). This language does not fully capture the notion of higher type computability. The authors define run times for their programs and prove that the feasible functionals of S. Cook and A. Urquhart (1988) are precisely those functionals computable by typed while-programs with run times feasibly length-bounded. The authors introduce the notion of a bounded typed loop program and prove that a finite type functional is feasible if it is computable by a bounded typed loop program.< >"
1989,Functional Interpretations of Feasibly Constructive Arithmetic (Extended Abstract).,"This paper lies on the border between logic and complexity theory, and is intended to illuminate both fields. We introduce a new notion, that of ìfeasible functionalî, and use it to analyze logical theories which are intended to capture the concept of feasibly constructive reasoning. The logical theories studied are intended to capture the notion of ìfeasibly constructive proofî (see Cook [3]). A constructive proof of \(\forall x\exists yA(x,y)\) provides a computable function f and proves \(\forall xA\left( {x,f\left( x \right)} \right)\). If the proof is feasibly constructive, then f must be polynomial time computable. Our results build on the work of Buss ([1], [2]). In [2], Buss introduces the system of first order intuitionistic bounded arithmetic \(IS_2^1 \), and proved that for every theorem in \(IS_2^1 \) of the form \(\forall x\exists y\left( {x,y} \right)\) there is a poynomial time computable function f such that \(A\left( {n,f\left( n \right)} \right)\) holds for all n. He used methods of proof theory and a form of realizability, based on placing run-time bounds on higher types"
1988,Short Propositional Formulas Represent Nondeterministic Computations.,n/a
1988,A Simple Parallel Algorithm for Finding a Satisfying Truth Assignment to a 2-CNF Formula.,"Abstract
We show that the problem of finding a truth assignment which satisfies a 2-CNF formula can be solved in O(log n) time on a concurrent-read concurrent-write parallel random access machine (CRCW PRAM) with O(n4) processors, where n is the number of variables in the formula."
1988,Two applications of complementation via inductive counting.,"Abstract:
A recent proof that nondeterministic space-bounded complexity classes are closed under complementation is used to develop two further applications of the inductive counting technique. An errorless probabilistic algorithm is given for the undirected graph s-t connectivity problem that runs in O(log n) space and polynomial expected time, and it is shown that the class LOGCFL is closed under complementation. The latter is a special case of a general result that shows closure under complementation of classes defined by semiunbounded fan-in circuits (or, equivalently, nondeterministic auxiliary pushdown automata or tree-sized bounded alternating Turing machines). As one consequence, small numbers of role switches in two-person pebbling can be eliminated.< >"
1987,Problems Complete for Deterministic Logarithmic Space.,"Abstract
We exhibit several problems complete for deterministic logarithmic space under NC1 (i.e., log depth) reducibility. The list includes breadth-first search and depth-first search of an undirected tree, connectivity of undirected graphs known to be made up of one or more disjoint cycles, undirected graph acyclicity, and several problems related to representing and to operating with permutations of a finite set."
1987,The Parallel Complexity of Abelian Permutation Group Problems.,"We classify Abelian permutation group problems with respect to their parallel complexity. For such groups specified by generating permutations we show that testing membership, computing order and testing isomorphism are $NC^1 $-equivalent to (and therefore have essentially the same parallel complexity as) determining solvability of a system of linear equations modulo a product of small prime powers; we show that intersecting two such groups is $NC^1 $-equivalent to computing setwise stabilizers; we show that each of these problems is $NC^1 $-reducible to the problem of computing a generator-relator presentation. Then we prove that the aforementioned problems belong to $NC^3 $, thus identifying several natural set recognition problems in $NC$ which may lie outside $NC^2 $. Finally we prove that $NC^4 $ contains the problem of computing the cyclic decomposition of an Abelian permutation group. Background results include an $NC^1 $ solution to the problem of computing the product of n integers modulo a $\lceil {\log n} \rceil $-bit integer, and an $NC^1 $ reduction from the problem of computing a path between two nodes in a graph to that of determining accessibility of one node from another.
"
1986,Upper and Lower Time Bounds for Parallel Random Access Machines without Simultaneous Writes.,"One of the frequently used models for a synchronous parallel computer is that of a parallel random access machine, where each processor can read from and write into a common random access memory. Different processors may read the same memory location at the same time, but simultaneous writing is disallowed. We show that even if we allow nonuniform algorithms, an arbitrary number of processors, and arbitrary instruction sets, $\Omega (\log n)$ is a lower bound on the time required to compute various simple functions, including sorting n keys and finding the logical ìorî of n bits. We also prove a surprising time upper bound of $.72\log _2 n$ steps for these functions, which beats the obvious algorithms requiring $\log _2 n$ steps.If simultaneous writes are allowed, there are simple algorithms to compute these functions in a constant number of steps.

"
1986,Log Depth Circuits for Division and Related Problems.,"We present optimal depth Boolean circuits (depth $O(\log n)$) for integer division, powering, and multiple products. We also show that these three problems are of equivalent uniform depth and space complexity. In addition, we describe an algorithm for testing divisibility that is optimal for both depth and space.
"
1985,A Taxonomy of Problems with Fast Parallel Algorithms.,"The class NC consists of problems solvable very fast (in time polynomial in log n) in parallel with a feasible (polynomial) number of processors. Many natural problems in NC are known; in this paper an attempt is made to identify important subclasses of NC and give interesting examples in each subclass. The notion of NC1-reducibility is introduced and used throughout (problem R is NC1-reducible to problem S if R can be solved with uniform log-depth circuits using oracles for S). Problems complete with respect to this reducibility are given for many of the subclasses of NC. A general technique, the ‚Äúparallel greedy algorithm,‚Äù is identified and used to show that finding a minimum spanning forest of a graph is reducible to the graph accessibility problem and hence is in NC2 (solvable by uniform Boolean circuits of depth O(log2 n) and polynomial size). The class LOGCFL is given a new characterization in terms of circuit families. The class DET of problems reducible to integer determinants is defined and many examples given. A new problem complete for deterministic polynomial time is given, namely, finding the lexicographically first maximal clique in a graph. This paper is a revised version of S. A. Cook, (1983, in ‚ÄúProceedings 1983 Intl. Found. Comut. Sci. Conf.,‚Äù Lecture Notes in Computer Science Vol. 158, pp. 78‚Äì93, Springer-Verlag, Berlin/New York)."
1985,A Depth-Universal Circuit.,"This paper describes a family of depth-universal circuits. For any n, c, d there is a universal circuit $U(n,c,d)$ that can simulate any circuit $\alpha $ having n inputs, of size c and depth d, and U has depth $O(d)$ and size $O(c^3 d/\log c)$. The construction is used to give an alternative proof of a theorem of Ruzzo showing the invariance under different uniformity conditions of complexity classes defined by uniform circuit families.



"
1984,Log Depth Circuits for Division and Related Problems.,"Abstract:
We present optimal depth Boolean circuits (depth O(log n)) for integer division, powering, and multiple products. We also show that these three problems are of equivalent uniform depth and space complexity. In addition, we describe an algorithm for testing divisibility that is optimal for both depth and space."
1983,An Overview of Computational Complexity.,An historical overview of computational complexity is presented. Emphasis is on the fundamental issues of defining the intrinsic computational complexity of a problem and proving upper and lower bounds on the complexity of problems. Probabilistic and parallel computation are discussed.
1983,The Recognition of Deterministic CFL's in Small Time and Space.,"Let S(n) be a nice space bound such that log2 n ‚©Ω S(n) ‚©Ω n. Then every DCFL is recognized by a multitape Turing machine simultaneously in time O(n2/S(n)) and space O(S(n)), and this time bound is optimal. If the machine is allowed a random access input, then the time bound can be improved so that the time-space product is O(n1 + …õ)."
1983,Parallel Computation for Well-Endowed Rings and Space-Bounded Probabilistic Machines.,"It is shown that a probabilistic Turing acceptor or transducer running within space bound S can be simulated by a time S2 parallel machine and therefore by a space S2 deterministic machine. (Previous simulations ran in space S6.) In order to achieve these simulations, known algorithms are extended for the computation of determinants in small arithmetic parallel time to computations having small Boolean parallel time, and this development is applied to computing the completion of stochastic matrices. The method introduces a generalization of the ring of integers, called well-endowed rings. Such rings possess a very efficient parallel implementation of the basic (+,‚àí,√ó) ring operations."
1983,The Classifikation of Problems which have Fast Parallel Algorithms.,n/a
1983,The Parallel Complexity of the Abelian Permutation Group Membership Problem.,"Abstract:
We show that the permutation group membership problem can be solved in depth (logn)3 on a Monte Carlo Boolean circuit of polynomial size in the restricted case in which the group is abelian. We also show that this restricted problem is NC1-hard for NSPACE(logn)."
1982,A Time-Space Tradeoff for Sorting on a General Sequential Model of Computation.,"In a general sequential model of computation, no restrictions are placed on the way in which the computation may proceed, except that parallel operations are not allowed. We show that in such an unrestricted environment ${\text{TIME}} \cdot {\text{SPACE}} = \Omega (N^2 /\log N)$ in order to sort N integers, each in the range $[1,N^2 ]$.
"
1982,Bounds on the Time for Parallel RAM's to Compute Simple Functions.,"We prove that a parallel RAM with no write conflicts allowed requires Œ©(log n) steps to compute the Boolean or of n bits stored in the first n global memory cells. We first argue that this result is subtler than it appears, and in fact the ‚Äúobvious‚Äù lower bound of log2n steps can be beaten."
1981,Corrigendum: Soundness and Completeness of an Axiom System for Program Verification.,n/a
1980,Space Lower Bounds for Maze Threadability on Restricted Machines.,"A restricted model of a Turing machine called a JAG (Jumping Automaton for Graphs) is introduced for solving the maze threadability problem (determining whether there is a path joining two distinguished nodes in an input graph). A JAG accesses its input graph by moving pebbles from a limited supply along the edges of the graph under a finite state control, and detecting when two pebbles coincide. It can also cause one pebble to jump to another. We prove that for every N there is a JAG which can determine threadability of an arbitrary N node input graph in storage $O((\log N)^2 )$, where the storage of a JAG with Ppebbles and N states is defined to be $P\log N + \log Q$. Further, we prove that any JAG which determines threadability requires storage $\Omega ((\log N)^2 /\log \log N)$. Finally, we prove that even when the inputs are restricted to undirected graphs (with no bound on the number of nodes), no single JAG can determine threadability.
"
1980,Hardware Complexity and Parallel Computation (Preliminary Version).,n/a
1980,A Time-Space Tradeoff for Sorting on a General Sequential Model of Computation.,"In a general sequential model of computation, no restrictions are placed on the way in which the computation may proceed, except parallel operations are not allowed. We show that in such an unrestricted environment TIME‚Ä¢SPACE&equil;Œ©(N2/log N) in order to sort N elements, each in the range [1,N2]."
1979,The Relative Efficiency of Propositional Proof Systems.,"We are interested in studying the length of the shortest proof of a propositional tautology in various proof systems as a function of the length of the tautology. The smallest upper bound known for this function is exponential, no matter what the proof system. A question we would like to answer (but have not been able to) is whether this function has a polynomial bound for some proof system. (This question is motivated below.) Our results here are relative results.
In ßß2 and 3 we indicate that all standard Hilbert type systems (or Frege systems, as we call them) and natural deduction systems are equivalent, up to application of a polynomial, as far as minimum proof length goes. In ß4 we introduce extended Frege systems, which allow introduction of abbreviations for formulas. Since these abbreviations can be iterated, they eliminate the need for a possible exponential growth in formula length in a proof, as is illustrated by an example (the pigeonhole principle). In fact, Theorem 4.6 (which is a variation of a theorem of Statman) states that with a penalty of at most a linear increase in the number of lines of a proof in an extended Frege system, no line in the proof need be more than a constant times the length of the formula proved.
"
1979,Deterministic CFL's Are Accepted Simultaneously in Polynomial Time and Log Squared Space.,"We propose to prove the theorem in the title. Let PLOSS be the class of sets recognizable on a deterministic Turing machine simultaneously in polynomial time and log squared space. Using the notation of Bruss and Meyer [1], PLOSS &equil; &ugr;k TISP(nk,k log2n)."
1978,Soundness and Completeness of an Axiom System for Program Verification.,"A simple ALGOL-like language is defined which includes conditional, while, and procedure call statements as well as blocks. A formal interpretive semantics and a Hoare style axiom system are given for the language. The axiom system is proved to be sound, and in a certain sense complete, relative to the interpretive semantics. The main new results are the completeness theorem, and a careful treatment of the procedure call rules for procedures with global variables in their declarations.



"
1976,Storage Requirements for Deterministic Polynomial Time Recognizable Languages.,"An intriguing question is whether (log n)2 space is enough to recognize the class of languages recognizable in deterministic polynomial time. This question has earlier been narrowed down to the storage required to recognize a particular language called SP. SP is clearly in and it has been shown that if SP has tape complexity (log n)k, then every member of has tape complexity (log n)k. This paper presents further evidence in support of the conjecture that SP cannot be recognized using storage (log n)k for any k. We have no techniques at present for proving such a statement for Turing machines in general; we prove the result for a suitably restricted device."
1976,On the Number of Additions to Compute Specific Polynomials.,"The number of addition-subtraction operations required to compute univariate pol nomials is investigated. The existence of rational coefficient polynomials of degree n requiring $ \sim (\sqrt n ) \pm $ operations is established using an argument based on algebraic independence. A more analytic argument is used to relate $ \pm $ complexity to the number of distinct real zeros possessed by a given real coefficient polynomial.
"
1975,An Assertion Language for Data Structures.,"In this paper we wish to consider the problem of proving assertions about programs that construct and alter arbitrarily complex data structures. In recent years several papers have been written on the subject of proving assertions about such programs; however, the class of data structures considered has generally been a proper sub-class of the class of all data structures, such as the classes of linear lists or trees. [Burstall 1972] discusses the problem of what he calls Distinct Non-repeating Lists and Distinct Non-repeating Trees. [Kowaltowski 1973] extends Burstall's approach. His approach is likewise basically tree-oriented but is applicable to more general data structures. [Laventhal 1974] restricts his attention to 'simple singly-linked lists', noting the problem of providing 'a complete framework for correctness proofs' if one attempts to handle very general data structures. [Morris 1972] discusses the question of designing a programming language for general data structures in order to facilitate verification of programs written in such a language. [Standish 1973] provides a set of axioms for the class of data structures in which, for instance, two data structures are equal iff they are component-wise equal."
1975,Feasibly Constructive Proofs and the Propositional Calculus (Preliminary Version).,"The motivation for this work comes from two general sources. The first source is the basic open question in complexity theory of whether P equals NP (see [1] and [2]). Our approach is to try to show they are not equal, by trying to show that the set of tautologies is not in NP (of course its complement is in NP). This is equivalent to showing that no proof system (in the general sense defined in [3]) for the tautologies is ‚Äúsuper‚Äù in the sense that there is a short proof for every tautology. Extended resolution is an example of a powerful proof system for tautologies that can simulate most standard proof systems (see [3]). The Main Theorem (5.5) in this paper describes the power of extended resolution in a way that may provide a handle for showing it is not super. The second motivation comes from constructive mathematics. A constructive proof of, say, a statement @@@@√óA must provide an effective means of finding a proof of A for each value of x, but nothing is said about how long this proof is as a function of x. If the function is exponential or super exponential, then for short values of x the length of the proof of the instance of A may exceed the number of electrons in the universe. In section 2, I introduce the system PV for number theory, and it is this system which I suggest properly formalizes the notion of a feasibly constructive proof."
1975,Proving Assertions about Programs that Manipulate Data Structures.,"In this paper we wish to consider the problem of proving assertions about programs that construct and alter data structures. Our method will be to define a suitable assertion language L for data structures, to define a simple programming language L' for constructing and altering data structures, to give axioms and rules of inference (in the style of [Hoare 1969]) which specify the effect of program segments on data structures (described by formulas in L) and finally to prove that these axioms are correct (relative to a formal definition of the semantics of L') and, in a reasonable sense, complete. Thus our intention is to provide a complete theoretical framework for describing arbitrary data structures and proving assertions about programs that manipulate them."
1974,An Observation on Time-Storage Trade Off.,"There are two main results proved here. The first states that a certain set SP of strings (those coding ‚Äúsolvable path systems‚Äù) has tape complexity (log n)2 iff every set in
Download : Download full-size image
(i.e., of deterministic polynomial time complexity) has tape complexity (log n)2. The second result gives evidence that SP does not have tape complexity (log n)k for any k."
1974,Storage Requirements for Deterministic Polynomial Time Recognizable Languages.,"A striking example of practical tradeoffs between storage space and execution time is provided by the IBM 1401 Fortran compiler. On another level, there is an interesting relation between the time and storage required to recognize context free languages. The recognition algorithm in [Y] requires time no more than 0(n3), but requires at least linear storage, whereas the algorithm in [LI requires recognition space no more than 0((log n)2) and requires more than polynomial time. An intriguing question is whether (log n)2 space is enough to recognize all languages recognizable in deterministic polynomial time. The above question has been narrowed down in [C] to the storage required to recognize a particular language called SP. This paper presents further evidence in support of the conjecture that SP cannot be recognized using storage (log n)k for any k. In section 2 we consider a game on directed acyclic graphs (dags) and show that at least 0(n1/4) markers are needed to play the game on some n node dags. The 0(n1/4) bound is used in section 3 to show that a fairly general machine to recognize SP also requires 0(n1/4) storage."
1974,On the Lengths of Proofs in the Propositional Calculus (Preliminary Version).,"One of the most important open questions in the field of computational complexity is the question of whether there is a polynomial time decision procedure for the classical propositional calculus. The purpose of the present paper is to study a question related to the complexity of decision procedures for the propositional calculus; namely, the complexity of proof systems for the propositional calculus. The fundamental issue here is whether there exists any proof system, and a polynomial p(n) such that every valid formula has a proof of length not exceeding p(n), where n is the length of the formula. Theorem 1 below helps establish the importance of this question. For the purposes of this theorem, we give the following definitions."
1974,On the Number of Additions to Compute Specific Polynomials (Preliminary Version).,"It is well known from the work of Motzkin [55], Belaga [58] and Pan [66], that ‚Äúmost‚Äù nth degree polynomials p &egr; R[x] require about n/2 √ó, √∑ ops and n ¬± ops and that these bounds can always be achieved within the framework of preconditioned evaluation (1). More precisely, if p can be computed using less than [equation] √ó, √∑ or less than n ¬± ops, then the coefficients of p are algebraically dependent. The situation when counting ¬± ops with the potential of unlimited * ops, is not as clear. While the arguments based on algebraic dependence provide us with our best lower bounds thus far, a different approach of independent interest is taken in section IV. Namely, we are able to show that the number of ¬± ops required to compute any p &egr; R[x] is bounded below by a function of the number of distinct real zeros of p. The potential (e.g., for producing non linear lower bounds) and limitations of this approach will be discussed."
1973,A Hierarchy for Nondeterministic Time Complexity.,"We prove the following theorem in this paper: For any real numbers r1, r2, 1‚â§r1<r2, there is a set A of strings which has nondeterministic time complexity nr2, but not nondeterministic time complexity nr1. The computing devices are nondeterminsitic multitape Turing machines."
1973,Time Bounded Random Access Machines.,"The RAM, an abstract model for a random access computer, is introduced. A unique feature of the model is that the execution time of an instruction is defined in terms of l(n), a function of the size of the numbers manipulated by the instruction. This model has a fixed program, but it is shown that the computing speeds of this model and a stored-program model can differ by no more than a constant factor. It is proved that a T(n) time-bounded Turing machine can be simulated by an O(T(n)¬∑l(T(n))) timebounded RAM, and that a T(n) time-bounded RAM can be simulated by a Turing machine whose execution time is bounded by (T(n))3 if l(n) is constant, or (T(n))2 if l(n) is logarithmic.
The main result states that if T2(n) is a function such that there is a RAM that computes T2(n) in time O(T2(n)), and if T1(n) is any function such that
, then there is a set S that can be recognized by some RAM in time O(T2(n)), but no RAM recognizes S in time O(T1(n)). This is a sharper diagonal result than has been obtained for Turing machines.
The proofs of most of the above results are constructive and are aided by the introduction of an ALGOL-like programming language for RMA's."
1973,An Observation on Time-Storage Trade Off.,"Recently there have been several attempts to prove that every set of strings in @@@@ (i.e., recognizable in deterministic polynomial time) can be recognized in deterministic storage (log n)2. The methods used in the attempts were based on that of [1], in which it is shown that every context free language can be accepted in storage (log n)2 Our thesis in the present paper is that these attempts must fail. We define a specific set SP of strings which is clearly in @@@@, but in a certain well-defined sense cannot be recognized in storage (log n)2 using the techniques in [1]. We conjecture that no Turing machine recognizes SP within storage (log n)2, and show that if this conjecture is false, then in fact every member of @@@@ can be recognized within storage (log n)2."
1972,Time-Bounded Random Access Machines.,"In this paper we introduce a formal model for random access computers and argue that the model is a good one to use in the theory of computational complexity. Results are proved which compare run times for recognizing sets using this model (which has a fixed program) with a stored program model and with Turing machines. The main result, theorem 3, shows the existence of a time complexity hierarchy which is finer than that of any standard abstract computer model. An Algol-like programming language is introduced which facilitates proofs of the theorems."
1972,A Hierarchy for Nondeterministic Time Complexity.,"The purpose of this paper is to prove the following result: Theorem 1 For any real numbers r1, r2, 1 ‚â§ r1 < r2, there is a set A of strings which has nondeterministic time complexity nr2 but not nondeterministic time complexity nr1 The computing devices are non-deterministic multitape Turing machines."
1971,Characterizations of Pushdown Machines in Terms of Time-Bounded Computers.,"A class of machines called auxiliary pushdown machines is introduced. Several types of pushdown automata, including stack automata, are characterized in terms of these machines. The computing power of each class of machines in question is characterized in"
1971,Linear Time Simulation of Deterministic Two-Way Pushdown Automata.,n/a
1971,The Complexity of Theorem-Proving Procedures.,"It is shown that any recognition problem solved by a polynomial time-bounded nondeterministic Turing machine can be ‚Äúreduced‚Äù to the problem of determining whether a given propositional formula is a tautology. Here ‚Äúreduced‚Äù means, roughly speaking, that the first problem can be solved deterministically in polynomial time provided an oracle is available for solving the second. From this notion of reducible, polynomial degrees of difficulty are defined, and it is shown that the problem of determining tautologyhood has the same polynomial degree as the problem of determining whether the first of two given graphs is isomorphic to a subgraph of the second. Other examples are discussed. A method of measuring the complexity of proof procedures for the predicate calculus is introduced and discussed."
1970,Path Systems and Language Recognition.,"Our main result, theorem 2, gives a bound on the storage required for a Turing machine to simulate certain time-bounded pushdown machines. The theorem is a generalization of the result appearing in [3] stating that any context-free language can be recognized by a deterministic Turing machine within storage (log n)2. We introduce a combinatorial object, called a path system, develop its theory briefly, and use the theory to prove both the result on pushdown machines and the result on context free languages, as well as a third result. The third result is the Theorem of Savitch [5] stating that a non-deterministic L(n) - storage bounded Turing machine can be simulated by a deterministic (L(n))2 - storage bounded Turing machine."
1969,Variations on Pushdown Machines (Detailed Abstract).,"A class of machines called auxiliary pushdown machines is introduced. Several types of pushdown automata, including stack automata, are characterized in terms of these machines. The computing power of each class of machines in question is characterized in terms of time bounded Turing machines, and corollaries are derived which answer some open questions in the field."
1966,The Solvability of the Derivability Problem for One-Normal Systems.,"A one-normal system is a Post production system on a finite alphabet {s1, s2, ¬∑ ¬∑ ¬∑, s&sgr;} with productions siP ‚Üí PEij, where i ranges over a subset of {1, 2, ¬∑ ¬∑ ¬∑, &sgr;} and, for fixed i, j takes on the values 1, 2, ¬∑ ¬∑ ¬∑, ni. The following derivability problem is shown to be solvable for each such system: Given two words P and Q, decide whether Q can be derived from P by successive applications of the production rules. The result was proved by Hao Wang for the monogenic case (i.e., when each ni = 1)."
