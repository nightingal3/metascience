2019,What is Causal Inference?,n/a
2019,Sensitivity Analysis of Linear Structural Causal Models.,"Causal inference requires assumptions about the data generating process, many of which are unverifiable from the data. Given that some causal assumptions might be uncertain or disputed, formal methods are needed to quantify how sensitive research conclusions are to violations of those assumptions. Although an extensive literature exists on the topic, most results are limited to specific model structures, while a general-purpose algorithmic framework for sensitivity analysis is still lacking. In this paper, we develop a formal, systematic approach to sensitivity analysis for arbitrary linear Structural Causal Models (SCMs). We start by formalizing sensitivity analysis as a constrained identification problem. We then develop an efficient, graph-based identification algorithm that exploits non-zero constraints on both directed and bidirected edges. This allows researchers to systematically derive sensitivity curves for a target causal quantity with an arbitrary set of path coefficients and error covariances as sensitivity parameters. These results can be used to display the degree to which violations of causal assumptions affect the target quantity of interest, and to judge, on scientific grounds, whether problematic degrees of violations are plausible."
2019,Unit Selection Based on Counterfactual Logic.,"The unit selection problem aims to identify a set of individuals who are most likely to exhibit a desired mode of behavior, which is defined in counterfactual terms. A typical example is that of selecting individuals who would respond one way if encouraged and a different way if not encouraged. Unlike previous works on this problem, which rely on ad-hoc heuristics, we approach this problem formally, using counterfactual logic, to properly capture the nature of the desired behavior. This formalism enables us to derive an informative selection criterion which integrates experimental and observational data. We demonstrate the superiority of this criterion over A/B-test-based approaches."
2018,Estimation with Incomplete Data: The Linear Case.,"Traditional methods for handling incomplete data, including Multiple Imputation and Maximum Likelihood, require that the data be Missing At Random (MAR). In most cases, however, missingness in a variable depends on the underlying value of that variable. In this work, we devise model-based methods to consistently estimate mean, variance and covariance given data that are Missing Not At Random (MNAR). While previous work on MNAR data require variables to be discrete, we extend the analysis to continuous variables drawn from Gaussian distributions. We demonstrate the merits of our techniques by comparing it empirically to state of the art software packages."
2018,Consistent Estimation given Missing Data.,"This paper presents a unified approach for recovering causal and probabilistic queries using graphical models given missing (or incomplete) data. To this end, we develop a general algorithm that can recover conditional probability distributions and conditional causal effects in semi-Markovian models."
2018,Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution.,"Current machine learning systems operate, almost exclusively, in a statistical, or model-free mode, which entails severe theoretical limits on their power and performance. Such systems cannot reason about interventions and retrospection and, therefore, cannot serve as the basis for strong AI. To achieve human level intelligence, learning machines need the guidance of a model of reality, similar to the ones used in causal inference tasks. To demonstrate the essential role of such models, I will present a summary of seven tasks which are beyond reach of current machine learning systems and which have been accomplished using the tools of causal modeling."
2017,Counterfactual Data-Fusion for Online Reinforcement Learners.,"The Multi-Armed Bandit problem with Unobserved Confounders (MABUC) considers decision-making settings where unmeasured variables can influence both the agent‚Äôs decisions and received rewards (Bareinboim et al., 2015). Recent findings showed that unobserved confounders (UCs) pose a unique challenge to algorithms based on standard randomization (i.e., experimental data); if UCs are naively averaged out, these algorithms behave sub-optimally, possibly incurring infinite regret. In this paper, we show how counterfactual-based decision-making circumvents these problems and leads to a coherent fusion of observational and experimental data. We then demonstrate this new strategy in an enhanced Thompson Sampling bandit player, and support our findings‚Äô efficacy with extensive simulations."
2016,Causal inference and the data-fusion problem.,"We review concepts, principles, and tools that unify current approaches to causal analysis and attend to new challenges presented by big data. In particular, we address the problem of data fusionópiecing together multiple datasets collected under heterogeneous conditions (i.e., different populations, regimes, and sampling methods) to obtain valid answers to queries of interest. The availability of multiple heterogeneous datasets presents new opportunities to big data analysts, because the knowledge that can be acquired from combined data would not be possible from any individual source alone. However, the biases that emerge in heterogeneous environments require new analytical tools. Some of these biases, including confounding, sampling selection, and cross-population biases, have been addressed in isolation, largely in restricted parametric models. We here present a general, nonparametric framework for handling these biases and, ultimately, a theoretical solution to the problem of data fusion in causal inference tasks.
"
2016,Preface to the ACM TIST Special Issue on Causal Discovery and Inference.,n/a
2016,Incorporating Knowledge into Structural Equation Models Using Auxiliary Variables.,"In this paper, we extend graph-based identification methods by allowing background knowledge in the form of non-zero parameter values. Such information could be obtained, for example, from a previously conducted randomized experiment, from substantive understanding of the domain, or even an identification technique. To incorporate such information systematically, we propose the addition of auxiliary variables to the model, which are constructed so that certain paths will be conveniently cancelled. This cancellation allows the auxiliary variables to help conventional methods of identification (e.g., single-door criterion, instrumental variables, half-trek criterion), as well as model testing (e.g., d-separation, over-identification). Moreover, by iteratively alternating steps of identification and adding auxiliary variables, we can improve the power of existing identification methods via a bootstrapping approach that does not require external knowledge. We operationalize this method for simple instrumental sets (a generalization of instrumental variables) and show that the resulting method is able to identify at least as many models as the most general identification method for linear systems known to date. We further discuss the application of auxiliary variables to the tasks of model testing and z-identification.
"
2015,Missing Data from a Causal Perspective.,"Abstract
This paper applies graph based causal inference procedures for recovering information from missing data. We establish conditions that permit and prohibit recoverability. In the event of theoretical impediments to recoverability, we develop graph based procedures using auxiliary variables and external data to overcome such impediments. We demonstrate the perils of model-blind recovery procedures both in determining whether or not a query is recoverable and in choosing an estimation procedure when recoverability holds."
2015,Bandits with Unobserved Confounders: A Causal Approach.,"The Multi-Armed Bandit problem constitutes an archetypal setting for sequential decision-making, permeating multiple domains including engineering, business, and medicine. One of the hallmarks of a bandit setting is the agent's capacity to explore its environment through active intervention, which contrasts with the ability to collect passive data by estimating associational relationships between actions and payouts. The existence of unobserved confounders, namely unmeasured variables affecting both the action and the outcome variables, implies that these two data-collection modes will in general not coincide. In this paper, we show that formalizing this distinction has conceptual and algorithmic implications to the bandit setting. The current generation of bandit algorithms implicitly try to maximize rewards based on estimation of the experimental distribution, which we show is not always the best strategy to pursue. Indeed, to achieve low regret in certain realistic classes of bandit problems (namely, in the face of unobserved confounders), both experimental and observational quantities are required by the rational agent. After this realization, we propose an optimization metric (employing both experimental and observational distributions) that bandit agents should pursue, and illustrate its benefits over traditional algorithms."
2015,Efficient Algorithms for Bayesian Network Parameter Learning from Incomplete Data.,"We propose a family of efficient algorithms for learning the parameters of a Bayesian network from incomplete data. Our approach is based on recent theoretical analyses of missing data problems, which utilize a graphical representation, called the missingness graph. In the case of MCAR and MAR data, this graph need not be explicit, and yet we can still obtain closed-form, asymptotically consistent parameter estimates, without the need for inference. When this missingness graph is explicated (based on background knowledge), even partially, we can obtain even more accurate estimates with less data. Empirically, we illustrate how we can learn the parameters of large networks from large datasets, which are beyond the scope of algorithms like EM (which require inference). "
2015,Missing Data as a Causal and Probabilistic Problem.,"Causal inference is often phrased as a missing data problem - for every unit, only the response to observed treatment assignment is known, the response to other treatment assignments is not. In this paper, we extend the converse approach of [7] of representing missing data problems to causal models where only interventions on missingness indicators are allowed. We further use this representation to leverage techniques developed for the problem of identification of causal effects to give a general criterion for cases where a joint distribution containing missing variables can be recovered from data actually observed, given assumptions on missingness mechanisms. This criterion is significantly more general than the commonly used ""missing at random"" (MAR) criterion, and generalizes past work which also exploits a graphical representation of missingness. In fact, the relationship of our criterion to MAR is not unlike the relationship between the ID algorithm for identification of causal effects [22, 18], and conditional ignorability [13]."
2014,Generalizing causal knowledge: theory and algorithms.,"This article is a short summary of the full dissertation thesis that was defended in 2014 at the University of California, Los Angeles. "
2014,Recovering from Selection Bias in Causal and Statistical Inference.,"Selection bias is caused by preferential exclusion of units from the samples and represents a major obstacle to valid causal and statistical inferences; it cannot be removed by randomized experiments and can rarely be detected in either experimental or observational studies. In this paper, we provide complete graphical and algorithmic conditions for recovering conditional probabilities from selection biased data. We also provide graphical conditions for recoverability when unbiased data is available over a subset of the variables. Finally, we provide a graphical condition that generalizes the backdoor criterion and serves to recover causal effects when the data is collected under preferential selection."
2014,Testable Implications of Linear Structural Equation Models.,"In causal inference, all methods of model learning rely on testable implications, namely, properties of the joint distribution that are dictated by the model structure. These constraints, if not satisfied in the data, allow us to reject or modify the model. Most common methods of testing a linear structural equation model (SEM) rely on the likelihood ratio or chi-square test which simultaneously tests all of the restrictions implied by the model. Local constraints, on the other hand, offer increased power (Bollen and Pearl, 2013; McDonald, 2002) and, in the case of failure, provide the modeler with insight for revising the model specification. One strategy of uncovering local constraints in linear SEMs is to search for overidentified path coefficients. While these overidentifying constraints are well known, no method has been given for systematically discovering them. In this paper, we extend the half-trek criterion of (Foygel et al., 2012) to identify a larger set of structural coefficients and use it to systematically discover overidentifying constraints. Still open is the question of whether our algorithm is complete."
2014,Random Bayesian networks with bounded indegree.,"Bayesian networks (BN) are an extensively used graphical model for representing a probability distribution in artificial intelligence, data mining, and machine learning. In this paper, we propose a simple model for large random BNs with bounded indegree, that is, large directed acyclic graphs (DAG) where the edges appear at random and each node has at most a given number of parents. Using this model, we can study useful asymptotic properties of large BNs and BN algorithms with basic combinatorics tools. We estimate the expected size of a BN, the expected size increase of moralization, the expected size of the Markov blanket, and the maximum size of a minimal d-separator. We also provide an upper bound on the average time complexity of an algorithm for finding a minimal d-separator. In addition, the estimates are evaluated against BNs learned from real world data."
2014,On the Testability of Models with Missing Data.,"Graphical models that depict the process by which data are lost are helpful in recovering information from missing data. We address the question of whether any such model can be submitted to a statistical test given that the data available are corrupted by missingness. We present sufficient conditions for testability in missing data applications and note the impediments for testability when data are contaminated by missing entries. Our results strengthen the available tests for MCAR and MAR and further provide tests in the category of MNAR. Furthermore, we provide sufficient conditions to detect the existence of dependence between a variable and its missingness mechanism. We use our results to show that model sensitivity persists in almost all models typically categorized as MNAR."
2014,Transportability from Multiple Environments with Limited Experiments: Completeness Results.,"This paper addresses the problem of mz
-transportability, that is, transferring causal knowledge collected in several heterogeneous domains to a target domain in which only passive observations and limited experimental data can be collected. The paper first establishes a necessary and sufficient condition for deciding the feasibility of mz
-transportability, i.e., whether causal effects in the target domain are estimable from the information available. It further proves that a previously established algorithm for computing transport formula is in fact complete, that is, failure of the algorithm implies non-existence of a transport formula. Finally, the paper shows that the do-calculus is complete for the mz
-transportability class."
2014,Graphical Models for Recovering Probabilistic and Causal Queries from Missing Data.,"We address the problem of deciding whether a causal or probabilistic query is estimable from data corrupted by missing entries, given a model of missingness process. We extend the results of Mohan et al, 2013 by presenting more general conditions for recovering probabilistic queries of the form P(y|x) and P(y,x) as well as causal queries of the form P(y|do(x)). We show that causal queries may be recoverable even when the factors in their identifying estimands are not recoverable. Specifically, we derive graphical conditions for recovering causal effects of the form P(y|do(x)) when Y and its missingness mechanism are not d-separable. Finally, we apply our results to problems of attrition and characterize the recovery of causal effects from data corrupted by attrition."
2013,Structural Counterfactuals: A Brief Introduction.,"Recent advances in causal reasoning have given rise to a computational model that emulates the process by which humans generate, evaluate, and distinguish counterfactual sentences. Contrasted with the ìpossible worldsî account of counterfactuals, this ìstructuralî model enjoys the advantages of representational economy, algorithmic simplicity, and conceptual clarity. This introduction traces the emergence of the structural model and gives a panoramic view of several applications where counterfactual reasoning has benefited problem areas in the empirical sciences.
"
2013,Causal Transportability with Limited Experiments.,"We address the problem of transferring causal knowledge learned in one environment to another, potentially different environment, when only limited experiments may be conducted at the source. This generalizes the treatment of transportability introduced in [Pearl and Bareinboim, 2011; Bareinboim and Pearl, 2012b], which deals with transferring causal information when any experiment can be conducted at the source. Given that it is not always feasible to conduct certain controlled experiments, we consider the decision problem whether experiments on a selected subset Z of variables together with qualitative assumptions encoded in a diagram may render causal effects in the target environment computable from the available data. This problem, which we call z-transportability, reduces to ordinary transportability when Z is all-inclusive, and, like the latter, can be given syntactic characterization using the do-calculus [Pearl, 1995; 2000]. This paper establishes a necessary and sufficient condition for causal effects in the target domain to be estimable from both the non-experimental information available and the limited experimental information transferred from the source. We further provides a complete algorithm for computing the transport formula, that is, a way of fusing experimental and observational information to synthesize an unbiased estimate of the desired causal relation."
2013,Meta-Transportability of Causal Effects: A Formal Approach.,"This paper considers the problem of transferring experimental findings learned from multiple heterogeneous domains to a different environment, in which only passive observations can be collected. Pearl and Bareinboim (2011) established a complete characterization for such transfer between two domains, a source and a target, and this paper generalizes their results to multiple heterogeneous domains. It establishes a necessary and sufficient condition for deciding when effects in the target domain are estimable from both statistical and causal information transferred from the experiments in the source domains. The paper further provides a complete algorithm for computing the transport formula, that is, a way of fusing observational and experimental information to synthesize an unbiased estimate of the desired effects."
2013,A simple criterion for controlling selection bias.,"Controlling selection bias, a statistical error caused by preferential sampling of data, is a fundamental problem in machine learning and statistical inference. This paper presents a simple criterion for controlling selection bias in the odds ratio, a widely used measure for association between variables, that connects the nature of selection bias with the graph modeling the selection mechanism. If the graph contains certain paths, we show that the odds ratio cannot be expressed using data with selection bias. Otherwise, we show that a d-separability test can determine whether the odds ratio can be recovered, and when the answer is affirmative, output an unbiased estimand of the odds ratio. The criterion can be test in linear time and enhances the power of the estimand."
2013,Transportability from Multiple Environments with Limited Experiments.,"This paper considers the problem of transferring experimental findings learned from multiple heterogeneous domains to a target environment, in which only limited experiments can be performed. We reduce questions of transportability from multiple domains and with limited scope to symbolic derivations in the do-calculus, thus extending the treatment of transportability from full experiments introduced in Pearl and Bareinboim (2011). We further provide different graphical and algorithmic conditions for computing the transport formula for this setting, that is, a way of fusing the observational and experimental information scattered throughout different domains to synthesize a consistent estimate of the desired effects."
2013,Graphical Models for Inference with Missing Data.,"We address the problem of deciding whether there exists a consistent estimator of a given relation Q, when data are missing not at random. We employ a formal representation called `Missingness Graphs' to explicitly portray the causal mechanisms responsible for missingness and to encode dependencies between these mechanisms and the variables being measured. Using this representation, we define the notion of \textit{recoverability} which ensures that, for a given missingness-graph G
and a given query Q
an algorithm exists such that in the limit of large samples, it produces an estimate of Q
\textit{as if} no data were missing. We further present conditions that the graph should satisfy in order for recoverability to hold and devise algorithms to detect the presence of these conditions."
2012,Transportability of Causal Effects: Completeness Results.,"The study of transportability aims to identify conditions under which causal information learned from experiments can be reused in a different environment where only passive observations can be collected. The theory introduced in [Pearl and Bareinboim, 2011] (henceforth [PB, 2011]) defines formal conditions for such transfer but falls short of providing an effective procedure for deciding, given assumptions about differences between the source and target domains, whether transportability is feasible. This paper provides such procedure. It establishes a necessary and sufficient condition for deciding when causal effects in the target domain are estimable from both the statistical information available and the causal information transferred from the experiments. The paper further provides a complete algorithm for computing the transport formula, that is, a way of fusing experimental and observational information to synthesize an estimate of the desired causal relation.
"
2012,Human and Machine Intelligence.,"In his 1950 Mind paper, Alan Turing reframed the question of whether machines could think as an operational or behavioral question: Could a computer be built that was indistinguishable from people in playing the ""imitation game,"" now known as ""the Turing Test""? He conjectured that by the end of the 20th century ""one [would] be able to speak of machines thinking without expecting to be contradicted"" and that computers would succeed in the Turing Test.

Turing's first conjecture proved right. Although his second has not yet been realized, research in Artifi cial Intelligence (AI) has generated a variety of algorithms and techniques regularly deployed in systems enabling them to behave in ways that are broadly considered to be intelligent. The performances of Watson, Siri, and driverless cars are but a few examples in the public eye. This session's panelists will highlight some of the major accomplishments of research in AI and its infl uential role in the development of computer science and computer systems more broadly, considering not only progress in individual subfi elds, but also designs for integrating these into well-functioning systems. They will also consider the ways in which AI theories and methods have infl uenced research on human cognition in behavioral sciences and neuroscience as well as scientifi c research more generally, and they will discuss major challenges and opportunities for the decades ahead."
2012,The Do-Calculus Revisited.,"The do-calculus was developed in 1995 to facilitate the identification of causal effects in non-parametric models. The completeness proofs of [Huang and Valtorta, 2006] and [Shpitser and Pearl, 2006] and the graphical criteria of [Tian and Shpitser, 2010] have laid this identification problem to rest. Recent explorations unveil the usefulness of the do-calculus in three additional areas: mediation analysis [Pearl, 2012], transportability [Pearl and Bareinboim, 2011] and metasynthesis. Meta-synthesis (freshly coined) is the task of fusing empirical results from several diverse studies, conducted on heterogeneous populations and under different conditions, so as to synthesize an estimate of a causal relation in some target environment, potentially different from those under study. The talk surveys these results with emphasis on the challenges posed by meta-synthesis. For background material, see http://bayes.cs.ucla.edu/csl_papers.html"
2012,Causal Inference by Surrogate Experiments: z-Identifiability.,"We address the problem of estimating the effect of intervening on a set of variables X from experiments on a different set, Z, that is more accessible to manipulation. This problem, which we call z-identifiability, reduces to ordinary identifiability when Z = empty and, like the latter, can be given syntactic characterization using the do-calculus [Pearl, 1995; 2000]. We provide a graphical necessary and sufficient condition for z-identifiability for arbitrary sets X,Z, and Y (the outcomes). We further develop a complete algorithm for computing the causal effect of X on Y using information provided by experiments on Z. Finally, we use our results to prove completeness of do-calculus relative to z-identifiability, a result that does not follow from completeness relative to ordinary identifiability."
2012,Controlling Selection Bias in Causal Inference.,"Selection bias, caused by preferential exclusion of samples from the data, is a major obstacle to valid causal and statistical inferences; it cannot be removed by randomized experiments and can hardly be detected in either experimental or observational studies. This paper highlights several graphical and algebraic methods capable of mitigating and sometimes eliminating this bias. These nonparametric methods generalize previously reported results, and identify the type of knowledge that is needed for reasoning in the presence of selection bias. Specifically, we derive a general condition together with a procedure for deciding recoverability of the odds ratio (OR) from s-biased data. We show that recoverability is feasible if and only if our condition holds. We further offer a new method of controlling selection bias using instrumental variables that permits the recovery of other effect measures besides OR."
2011,The algorithmization of counterfactuals.,"Abstract
Recent advances in causal reasoning have given rise to a computation model that emulates the process by which humans generate, evaluate and distinguish counterfactual sentences. Though compatible with the ‚Äúpossible worlds‚Äù account, this model enjoys the advantages of representational economy, algorithmic simplicity and conceptual clarity. Using this model, the paper demonstrates the processing of counterfactual sentences on a classical example due to Ernest Adam. It then gives a panoramic view of several applications where counterfactual reasoning has benefited problem areas in the empirical sciences."
2011,Controlling Selection Bias in Causal Inference.,"Selection bias, caused by preferential exclusion of units (or samples) from the data, is a major obstacle to valid causal inferences, for it cannot be removed or even detected by randomized experiments. This paper highlights several graphical and algebraic methods capable of mitigating and sometimes eliminating this bias. These nonparametric methods generalize and improve previously reported results, and identify the type of knowledge that need to be available for reasoning in the presence of selection bias"
2011,Transportability of Causal and Statistical Relations: A Formal Approach.,"We address the problem of transferring information learned from experiments to a different environment, in which only passive observations can be collected. We introduce a formal representation called ""selection diagrams'' for expressing knowledge about differences and commonalities between environments and, using this representation, we derive procedures for deciding whether effects in the target environment can be inferred from experiments conducted elsewhere. When the answer is affirmative, the procedures identify the set of experiments and observations that need be conducted to license the transport. We further discuss how transportability analysis can guide the transfer of knowledge in non-experimental learning to minimize re-measurement cost and improve prediction power."
2011,The algorithmization of counterfactuals.,"One of the most striking phenomenon in the study of conditionals is the ease and uniformity with which people generate, evaluate and interpret counterfactual utterance. To witness, the majority of people would accept the statement: ``If Oswald didn't kill Kennedy, someone else did,'' but few, if any, would accept its subjunctive version: ``If Oswald hadn't killed Kennedy, someone else would have.'' I will present a computational model that explains how humans reach such consensus or, more concretely, what mental representation permits such consensus to emerge from the little knowledge we have about Oswald, Kennedy and 1960's Texas, and what algorithms would need to be postulated to account for the swiftness, comfort and confidence with which such judgments are issued. The model presented is compatible with the ""possible world"" account of Lewis (1973), yet it enjoys the advantages of representational economy, algorithmic simplicity and conceptual clarity. Armed with these advantages, I will then present a panoramic view of several applications where counterfactual reasoning has benefited problem areas in the empirical sciences, including policy evaluation, causal-pathways mapping, credit and blame analysis, and personal decision making.
"
2011,Local Characterizations of Causal Bayesian Networks.,"Abstract
The standard definition of causal Bayesian networks (CBNs) invokes a global condition according to which the distribution resulting from any intervention can be decomposed into a truncated product dictated by its respective mutilated subgraph. We analyze alternative formulations which emphasizes local aspects of the causal process and can serve therefore as more meaningful criteria for coherence testing and network construction. We first examine a definition based on ‚Äúmodularity‚Äù and prove its equivalence to the global definition. We then introduce two new definitions, the first interprets the missing edges in the graph, and the second interprets ‚Äúzero direct effect‚Äù (i.e., ceteris paribus). We show that these formulations are equivalent but carry different semantic content."
2011,Transportability of Causal and Statistical Relations: A Formal Approach.,"Abstract:
We address the problem of transferring information learned from experiments to a different environment, in which only passive observations can be collected. We introduce a formal representation called ""selection diagrams'' for expressing knowledge about differences and commonalities between environments and, using this representation, we derive procedures for deciding whether effects in the target environment can be inferred from experiments conducted elsewhere. When the answer is affirmative, the procedures identify the set of experiments and observations that need be conducted to license the transport. We further discuss how transportability analysis can guide the transfer of knowledge in non-experimental learning to minimize re-measurement cost and improve prediction power."
2011,The mathematics of causal inference.,"I will review concepts, principles, and mathematical tools that were found useful in applications involving causal and counterfactual relationships. This semantical framework, enriched with a few ideas from logic and graph theory, gives rise to a complete, coherent, and friendly calculus of causation that unifies the graphical and counterfactual approaches to causation and resolves many long-standing problems in several of the sciences. These include questions of causal effect estimation, policy analysis, and the integration of data from diverse studies. Of special interest to KDD researchers would be the following topics:

The Mediation Formula, and what it tells us about direct and indirect effects.
What mathematics can tell us about ""external validity"" or ""generalizing from experiments""
What can graph theory tell us about recovering from sample-selection bias."
2010,Belief propagation: technical perspective.,n/a
2010,On a Class of Bias-Amplifying Variables that Endanger Effect Estimates.,"This note deals with a class of variables that, if conditioned on, tends to amplify confound- ing bias in the analysis of causal effects. This class, independently discovered by Bhat- tacharya and Vogt (2007) and Wooldridge (2009), includes instrumental variables and variables that have greater influence on treat- ment selection than on the outcome. We offer a simple derivation and an intuitive explana- tion of this phenomenon and then extend the analysis to non linear models. We show that: 1. the bias-amplifying potential of instru- mental variables extends over to non- linear models, though not as sweepingly as in linear models; 2. in non-linear models, conditioning on in- strumental variables may introduce new bias where none existed before; 3. in both linear and non-linear models, in- strumental variables have no effect on selection-induced bias."
2010,On Measurement Bias in Causal Inference.,"This paper addresses the problem of measurement errors in causal inference and highlights several algebraic and graphical methods for eliminating systematic bias induced by such errors. In particulars, the paper discusses the control of partially observable confounders in parametric and non parametric models and the computational problem of obtaining bias-free effect estimates in such models."
2010,Confounding Equivalence in Causal Inference.,"The paper provides a simple test for deciding, from a given causal diagram, whether two sets of variables have the same bias-reducing potential under adjustment. The test re- quires that one of the following two condi- tions holds: either (1) both sets are admis- sible (i.e., satisfy the back-door criterion) or (2) the Markov boundaries surrounding the manipulated variable(s) are identical in both sets. Applications to covariate selection and model testing are discussed."
2010,Causal Inference.,"This paper reviews a theory of causal inference based on the Structural Causal Model (SCM) described in Pearl (2000a). The theory unifies the graphical, potential-outcome (Neyman-Rubin), decision analytical, and structural equation approaches to causation, and provides both a mathematical foundation and a friendly calculus for the analysis of causes and counterfactuals. In particular, the paper establishes a methodology for inferring (from a combination of data and assumptions) the answers to three types of causal queries: (1) queries about the effect of potential interventions, (2) queries about counterfactuals, and (3) queries about the direct (or indirect) effect of one event on another."
2009,Effects of Treatment on the Treated: Identification and Generalization.,"Many applications of causal analysis call for assessing, retrospectively, the effect of withholding an action that has in fact been implemented. This counterfactual quantity, sometimes called ""effect of treatment on the treated,"" (ETT) have been used to to evaluate educational programs, critic public policies, and justify individual decision making. In this paper we explore the conditions under which ETT can be estimated from (i.e., identified in) experimental and/or observational studies. We show that, when the action invokes a singleton variable, the conditions for ETT identification have simple characterizations in terms of causal diagrams. We further give a graphical characterization of the conditions under which the effects of multiple treatments on the treated can be identified, as well as ways in which the ETT estimand can be constructed from both interventional and observational distributions."
2008,Complete Identification Methods for the Causal Hierarchy.,"We consider a hierarchy of queries about causal relationships in graphical models, where each level in the hierarchy requires more detailed information than the one below. The hierarchy consists of three levels: associative relationships, derived from a joint distribution over the observable variables; cause-effect relationships, derived from distributions resulting from external interventions; and counterfactuals, derived from distributions that span multiple ""parallel worlds"" and resulting from simultaneous, possibly conflicting observations and interventions. We completely characterize cases where a given causal query can be computed from information lower in the hierarchy, and provide algorithms that accomplish this computation. Specifically, we show when effects of interventions can be computed from observational studies, and when probabilities of counterfactuals can be computed from experimental studies. We also provide a graphical characterization of those queries which cannot be computed (by any method) from queries at a lower layer of the hierarchy.
"
2008,Dormant Independence.,"The construction of causal graphs from non-experimental data rests on a set of constraints that the graph structure imposes on all probability distributions compatible with the graph. These constraints are of two types: conditional independencies and algebraic constraints, first noted by Verma. While conditional independencies are well studied and frequently used in causal induction algorithms, Verma constraints are still poorly understood, and rarely applied. In this paper we examine a special subset of Verma constraints which are easy to understand, easy to identify and easy to apply; they arise from ``dormant independencies,'' namely, conditional independencies that hold in interventional distributions. We give a complete algorithm for determining if a dormant independence between two sets of variables is entailed by the causal graph, such that this independence is identifiable, in other words if it resides in an interventional distribution that can be predicted without resorting to interventions. We further show the usefulness of dormant independencies in model testing and induction by giving an algorithm that uses constraints entailed by dormant independencies to prune extraneous edges from a given causal graph.
"
2007,Causality and Counterfactuals in the Situation Calculus.,"Structural causal models offer a popular framework for exploring causal concepts. However, due to their limited expressiveness, structural models have difficulties coping with such concepts as actual (event-to-event) causation. In this article, we propose a new type of causal model, based on embedding structural considerations in the language of situation calculus. By using situation calculus as a basic language, we leverage its power to express complex, dynamically changing situations and, by relying on structural considerations, we can formulate an effective theory of counterfactuals within the situation-calculus.
"
2007,What Counterfactuals Can Be Tested.,"Counterfactual statements, e.g., ""my headache would be gone had I taken an aspirin"" are central to scientific discourse, and are formally interpreted as statements derived from ""alternative worlds"". However, since they invoke hypothetical states of affairs, often incompatible with what is actually known or observed, testing counterfactuals is fraught with conceptual and practical difficulties. In this paper, we provide a complete characterization of ""testable counterfactuals,"" namely, counterfactual statements whose probabilities can be inferred from physical experiments. We provide complete procedures for discerning whether a given counterfactual is testable and, if so, expressing its probability in terms of experimental data."
2006,Identification of Joint Interventional Distributions in Recursive Semi-Markovian Causal Models.,"The subject of this paper is the elucidation of effects of actions from causal assumptions represented as a directed graph, and statistical knowledge given as a probability distribution. In particular, we are interested in predicting conditional distributions resulting from performing an action on a set of variables and, subsequently, taking measurements of another set. We provide a necessary and sufficient graphical condition for the cases where such distributions can be uniquely computed from the available information, as well as an algorithm which performs this computation whenever the condition holds. Furthermore, we use our results to prove completeness of do-calculus [Pearl, 1995] for the same identification problem."
2006,A Characterization of Interventional Distributions in Semi-Markovian Causal Models.,"We offer a complete characterization of the set of distributions that could be induced by local interventions on variables governed by a causal Bayesian network of unknown structure, in which some of the variables remain unmeasured. We show that such distributions are constrained by a simply formulated set of inequalities, from which bounds can be derived on causal effects that are not directly measured in randomized experiments.
"
2006,Graphical Condition for Identification in recursive SEM.,"The paper concerns the problem of predicting the effect of actions or interventions on a system from a combination of (i) statistical data on a set of observed variables, and (ii) qualitative causal knowledge encoded in the form of a directed acyclic graph (DAG). The DAG represents a set of linear equations called Structural Equations Model (SEM), whose coefficients are parameters representing direct causal effects. Reliable quantitative conclusions can only be obtained from the model if the causal effects are uniquely determined by the data. That is, if there exists a unique parametrization for the model that makes it compatible with the data. If this is the case, the model is called identified. The main result of the paper is a general sufficient condition for identification of recursive SEM models."
2006,Identification of Conditional Interventional Distributions.,"The subject of this paper is the elucidation of effects of actions from causal assumptions represented as a directed graph, and statistical knowledge given as a probability distribution. In particular, we are interested in predicting conditional distributions resulting from performing an action on a set of variables and, subsequently, taking measurements of another set. We provide a necessary and sufficient graphical condition for the cases where such distributions can be uniquely computed from the available information, as well as an algorithm which performs this computation whenever the condition holds. Furthermore, we use our results to prove completeness of do-calculus [Pearl, 1995] for the same identification problem."
2005,Influence Diagrams - Historical and Personal Perspectives.,"The usefulness of graphical models in reasoning and decision making stems from facilitating four main computational features: (1) modular representation of probabilities, (2) systematic construction methods, (3) explicit encoding of independencies, and (4) efficient inference procedures. This note explains why the original introduction of influence diagrams, lacking formal underpinning of these features, has had only mild influence on automated reasoning research, and how Bayesian belief networks, which were formulated and defined directly by these features, became the focus of graphical modeling research.
"
2005,Identifiability of Path-Specific Effects.,"Counterfactual quantities representing path-specific effects arise in cases where we are interested in computing the effect of one variable on another only along certain causal paths in the graph (in other words by excluding a set of edges from consideration). A recent paper [Pearl, 2001] details a method by which such an exclusion can be specified formally by fixing the value of the parent node of each excluded edge. In this paper we derive simple, graphical conditions for experimental identifiability of path-specific effects, namely, conditions under which path-specific effects can be estimated consistently from data obtained from controlled experiments. "
2004,Robustness of Causal Claims.,"A causal claim is any assertion that invokes causal relationships between variables, for example that a drug has a certain effect on preventing a disease. Causal claims are established through a combination of data and a set of causal assumptions called a causal model. A claim is robust when it is insensitive to violations of some of the causal assumptions embodied in the model. This paper gives a formal definition of this notion of robustness and establishes a graphical condition for quantifying the degree of robustness of a given causal claim. Algorithms for computing the degree of robustness are also presented."
2002,Reasoning with Cause and Effect.,"Abstract
This article is an edited transcript of a lecture given at IJCAI-99, Stockholm, Sweden, on 4 August 1999. The article summarizes concepts, principles, and tools that were found useful in applications involving causal modeling. The principles are based on structural-model semantics in which functional (or counterfactual) relationships representing autonomous physical processes are the fundamental building blocks. The article presents the conceptual basis of this semantics, illustrates its application in simple problems, and discusses its ramifications to computational and cognitive problems concerning causation."
2002,A Graphical Criterion for the Identification of Causal Effects in Linear Models.,"This paper concerns the assessment of direct causal effects from a combination of:(i) non-experimental data, and (ii) qualitative domain knowledge. Domain knowledge is encoded in the form of a directed acyclic graph (DAG), in which all interactions are assumed linear, and some variables are presumed to be unobserved. The paper establishes a sufficient criterion for the identifiability of all causal effects in such models as well as a procedure for estimating the causal effects from the observed covariance matrix."
2002,A General Identification Condition for Causal Effects.,"This paper concerns the assessment of the effects of actions or policy interventions from a combination of: (i) nonexperimental data, and (ii) substantive assumptions. The assumptions are encoded in the form of a directed acyclic graph, also called ""causal graph,"" in which some variables are presumed to be unobserved. The paper establishes a necessary and sufficient criterion for the identifiability of the causal effects of a singleton variable on all other variables in the model, and a powerful sufficient criterion for the effects of a singleton variable on any set of variables."
2002,A New Characterization of the Experimental Implications of Causal Bayesian Networks.,"We offer a complete characterization of the set of distributions that could be induced by local interventions on variables governed by a causal Bayesian network. We show that such distributions must adhere to three norms of coherence, and we demonstrate the use of these norms as inferential tools in tasks of learning and identification. Testable coherence norms are subsequently derived for networks containing unmeasured variables."
2002,Qualitative MDPs and POMDPs: An Order-Of-Magnitude Approximation.,"We develop a qualitative theory of Markov Decision Processes (MDPs) and Partially Observable MDPs that can be used to model sequential decision making tasks when only qualitative information is available. Our approach is based upon an order-of-magnitude approximation of both probabilities and utilities, similar to epsilon-semantics. The result is a qualitative theory that has close ties with the standard maximum-expected-utility theory and is amenable to general planning techniques."
2002,Generalized Instrumental Variables.,"This paper concerns the assessment of direct causal effects from a combination of: (i) non-experimental data, and (ii) qualitative domain knowledge. Domain knowledge is encoded in the form of a directed acyclic graph (DAG), in which all interactions are assumed linear, and some variables are presumed to be unobserved. We provide a generalization of the well-known method of Instrumental Variables, which allows its application to models with few conditional independeces."
2002,On the Testable Implications of Causal Models with Hidden Variables.,"The validity OF a causal model can be tested ONLY IF the model imposes constraints ON the probability distribution that governs the generated data. IN the presence OF unmeasured variables, causal models may impose two types OF constraints : conditional independencies, AS READ through the d - separation criterion, AND functional constraints, FOR which no general criterion IS available.This paper offers a systematic way OF identifying functional constraints AND, thus, facilitates the task OF testing causal models AS well AS inferring such models FROM data."
2001,On Two Pseudo-Paradoxes in Bayesian Analysis.,n/a
2001,Causes and Explanations: A Structural-Model Approach - Part II: Explanations.,"We propose new definitions of (causal) explanation, using structural equations to model counterfactuals. The definition is based on the notion of actual cause, as defined and motivated in a companion paper. Essentially, an explanation is a fact that is not known for certain but, if found to be true, would constitute an actual cause of the fact to be explained, regardless of the agent's initial uncertainty. We show that the definition handles well a number of problematic examples from the literature."
2001,Causes and Explanations: A Structural-Model Approach: Part 1: Causes.,"We propose a new definition of actual causes, using structural equations to model counterfactuals.We show that the definitions yield a plausible and elegant account ofcausation that handles well examples which have caused problems forother definitions and resolves major difficulties in the traditionalaccount. In a companion paper, we show how the definition of causality can beused to give an elegant definition of (causal) explanation."
2001,Direct and Indirect Effects.,"The direct effect of one eventon another can be defined and measured byholding constant all intermediate variables between the two.Indirect effects present conceptual andpractical difficulties (in nonlinear models), because they cannot be isolated by holding certain variablesconstant. This paper shows a way of defining any path-specific effectthat does not invoke blocking the remainingpaths.This permits the assessment of a more naturaltype of direct and indirect effects, one thatis applicable in both linear and nonlinear models. The paper establishesconditions under which such assessments can be estimated consistentlyfrom experimental and nonexperimental data,and thus extends path-analytic techniques tononlinear and nonparametric models."
2001,Causal Discovery from Changes.,"We propose a new method of discovering causal structures, based on the detection of local, spontaneous changes in the underlying data-generating model. We analyze the classes of structures that are equivalent relative to a stream of distributions produced by local changes, and devise algorithms that output graphical representations of these equivalence classes. We present experimental results, using simulated data, and examine the errors associated with detection of changes and recovery of structures."
2000,Probabilities of causation: Bounds and identification.,"Abstract
This paper deals with the problem of estimating the probability of causation, that is, the probability that one event was the real cause of another, in a given scenario. Starting from structural‚Äêsemantical definitions of the probabilities of necessary or sufficient causation (or both), we show how to bound these quantities from data obtained in experimental and observational studies, under general assumptions concerning the data‚Äêgenerating process. In particular, we strengthen the results of Pearl [39] by presenting sharp bounds based on combined experimental and nonexperimental data under no process assumptions, as well as under the mild assumptions of exogeneity (no confounding) and monotonicity (no prevention). These results delineate more precisely the basic assumptions that must be made before statistical measures such as the excess‚Äêrisk‚Äêratio could be used for assessing attributional quantities such as the probability of causation."
2000,Probabilities of Causation: Bounds and Identification.,"This paper deals with the problem of estimating the probability that one event was a cause of another in a given scenario. Using structural-semantical definitions of the probabilities of necessary or sufficient causation (or both), we show how to optimally bound these quantities from data obtained in experimental and observational studies, making minimal assumptions concerning the data-generating process. In particular, we strengthen the results of Pearl (1999) by weakening the data-generation assumptions and deriving theoretically sharp bounds on the probabilities of causation. These results delineate precisely how empirical data can be used both in settling questions of attribution and in solving attribution-related problems of decision making."
1999,Probabilities Of Causation: Three Counterfactual Interpretations And Their Identification.,"According to common judicial standard, judgment in favor ofplaintiff should be made if and only if it is ìmore probable than notî thatthe defendant's action was the cause for the plaintiff's damage (or death). This paper provides formal semantics, based on structural models ofcounterfactuals, for the probability that event x was a necessary orsufficient cause (or both) of another event y. The paper then explicates conditions under which the probability of necessary (or sufficient)causation can be learned from statistical data, and shows how data fromboth experimental and nonexperimental studies can be combined to yieldinformation that neither study alone can provide. Finally, we show thatnecessity and sufficiency are two independent aspects of causation, andthat both should be invoked in the construction of causal explanations for specific scenarios.
"
1999,Testing regression models with fewer regressors.,n/a
1999,Reasoning with Cause and Effect.,"This paper summarizes concepts, principles, and tools that were found useful in applications involving causal modelling. The principles are based on structural-model semantics, in which functional (or counterfactual) relationships, representing autonomous physical processes are the fundamental building blocks. The paper presents the formal basis of this semantics, illustrates its application in simple problems and discusses its ramifactions to computational and cognitive problems concerning causation."
1997,On the Logic of Iterated Belief Revision.,"Abstract
We show in this paper that the AGM postulates are too weak to ensure the rational preservation of conditional beliefs during belief revision, thus permitting improper responses to sequences of observations. We remedy this weakness by proposing four additional postulates, which are sound relative to a qualitative version of probabilistic conditioning. Contrary to the AGM framework, the proposed postulates characterize belief revision as a process which may depend on elements of an epistemic state that are not necessarily captured by a belief set. We also show that a simple modification to the AGM framework can allow belief revision to be a function of epistemic states. We establish a model-based representation theorem which characterizes the proposed postulates and constrains, in turn, the way in which entrenchment orderings may be transformed under iterated belief revision."
1997,The Relevance of Relevance (Editorial).,n/a
1997,Axioms of Causal Relevance.,"Abstract
This paper develops axioms and formal semantics for statements of the form ‚ÄúX is causally irrelevant to Y in context Z‚Äù, which we interpret to mean ‚ÄúChanging X will not affect Y once Z is held constant‚Äù. The axiomization of causal irrelevance is contrasted with the axiomization of informational irrelevance, as in ‚ÄúFinding X will not alter our belief in Y, once we know Z‚Äù. Two versions of causal irrelevance are analyzed: probabilistic and deterministic. We show that, unless stability is assumed, the probabilistic definition yields a very loose structure that is governed by just two trivial axioms. Under the stability assumption, probabilistic causal irrelevance is isomorphic to path interception in cyclic graphs. Under the deterministic definition, causal irrelevance complies with all of the axioms of path interception in cyclic graphs except transitivity. We compare our formalism to that of Lewis (1973) and offer a graphical method of proving theorems about causal relevance."
1996,"Qualitative Probabilities for Default Reasoning, Belief Revision, and Causal Modeling.","Abstract
This paper presents a formalism that combines useful properties of both logic and probabilities. Like logic, the formalism admits qualitative sentences and provides symbolic machinery for deriving deductively closed beliefs and, like probability, it permits us to express if-then rules with different levels of firmness and to retract beliefs in response to changing observations. Rules are interpreted as order-of-magnitude approximations of conditional probabilities which impose constraints over the rankings of worlds. Inferences are supported by a unique priority ordering on rules which is syntactically derived from the knowledge base. This ordering accounts for rule interactions, respects specificity considerations and facilitates the construction of coherent states of beliefs. Practical algorithms are developed and analyzed for testing consistency, computing rule ordering, and answering queries. Imprecise observations are incorporated using qualitative versions of Jeffrey's rule and Bayesian updating, with the result that coherent belief revision is embodied naturally and tractably. Finally, causal rules are interpreted as imposing Markovian conditions that further constrain world rankings to reflect the modularity of causal organizations. These constraints are shown to facilitate reasoning about causal projections, explanations, actions and change."
1996,Uncovering Trees in Constraint Networks.,"Abstract
This paper examines the possibility of removing redundant information from a given knowledge base and restructuring it in the form of a tree to enable efficient problem-solving routines. We offer a novel approach that guarantees removal of all redandancies that hide a tree structure. We develop a polynomial-time algorithm that, given an arbitrary binary constraint network, either extracts (by edge removal) a precise tree representation from the path-consistent version of the network or acknowledges that no such tree can be extracted. In the latter case, a tree is generated that may serve as an approximation to the original network."
1996,Decision Making Under Uncertainty.,n/a
1996,Logarithmic-Time Updates and Queries in Probabilistic Networks.,"Traditional databases commonly support efficient query and update procedures that operate in time which is sublinear in the size of the database. Our goal in this paper is to take a first step toward dynamic reasoning in probabilistic databases with comparable efficiency. We propose a dynamic data structure that supports efficient algorithms for updating and querying singly connected Bayesian networks. In the conventional algorithm, new evidence is absorbed in O(1) time and queries are processed in time O(N), where N is the size of the network. We propose an algorithm which, after a preprocessing phase, allows us to answer queries in time O(log N) at the expense of O(log N) time per evidence absorption. The usefulness of sub-linear processing time manifests itself in applications requiring (near) real-time response over large probabilistic databases. We briefly discuss a potential application of dynamic probabilistic reasoning in computational biology."
1996,A new characterization of graphs based on interception relations.,"While graphs are normally defined in terms of the 2?place relation of adjacency, we take the 3?place relation of interception as the basic primitive of their definition. The paper views graphs as an economical scheme for encoding interception relations, and establishes an axiomatic characterization of relations that lend themselves to representation in terms of graph interception, thus providing a new characterization of graphs. © 1996 John Wiley & Sons, Inc.
"
1996,A Clinician's Tool for Analyzing Non-Compliance.,"We describe a computer program to assist a clinician with assessing the efficacy of treatments in experimental studies for which treatment assignment is random but subject compliance is imperfect. The major difficulty in such studies is that treatment efficacy is not ""identifiable"", that is, it cannot be estimated from the data, even when the number of subjects is infinite, unless additional knowledge is provided. Our system combines Bayesian learning with Gibbs sampling using two inputs: (1) the investigator' s prior probabilities of the relative sizes of subpopulations and (2) the observed data from the experiment. The system outputs a histogram depicting the posterior distribution of the average treatment effect, that is, the probability that the average outcome (e.g., survival) would attain a given level, had the treatment been taken uniformly by the entire population. This paper describes the theoretical basis for the proposed approach and presents experimental results on both simulated and real data, showing agreement with the theoretical asymptotic bounds."
1996,"Causation, Action and Counterfactuals.","The central aim of many empirical studies in the physical, behavioral, social, and biological sciences is the elucidation of cause-effect relationships among variables. It is through cause-effect relationships that we obtain a sense of a ""deep understanding"" of a given phenomenon, and it is through such relationships that we obtain a sense of being ""in control,"" namely, that we are able to shape the course of events by deliberate actions or policies. It is for these two reasons, understanding and control, that causal thinking is so pervasive, popping up in everything from everyday activities to high-level decision-making: For example, every car owner wonders why an engine won't start; a cigarette smoker would like to know, given his/her specific characteristics, to what degree his/her health would be affected by refraining from further smoking; a policy maker would like to know to what degree anti-smoking advertising would reduce costs of health care; and so on. Although a plethora of data has been collected on cars and on smoking and health, the appropriate methodology for extracting answers to such questions from the data has been fiercely debated, partly because some fundamental questions of causality have not been given fully satisfactory answers."
1996,Identifying Independencies in Causal Graphs with Feedback.,We show that the d -separation criterion constitutes a valid test for conditional independence relationships that are induced by feedback systems involving discrete variables.
1995,Causal inference from indirect experiments.,"Abstract
An indirect experiment is a study in which randomized control is replaced by randomized encouragement, that is, subjects are encouraged, rather than forced, to receive a given treatment program. The purpose of this paper is to bring to the attention of experimental researchers simple mathematical results that enable us to assess, from indirect experiments, the strength with which causal influences operate among variables of interest. The results reveal that despite the laxity of the encouraging instrument, data from indirect experimentation can yield significant and sometimes accurate information on the impact of a program on the population as a whole, as well as on the particular individuals who participated in the program."
1995,A Causal Calculus for Statistical Research.,"Abstract
A calculus is proposed that admits two conditioning operators: ordinary Bayes conditioning, P (y|X = x), and causal conditioning, P (y|set(X = x)), that is, conditioning P (y) on holding X constant (at x) by external intervention. This distinction, which will be supported by three rules of inference, will permit us to derive probability expressions for the combined effect of observations and interventions. The resulting calculus yields simple solutions to a number of interesting problems in causal inference and should allow rank-and-file researchers to tackle practical problems that are generally considered too hard, or impossible. Examples are:
1.
Deciding whether the information available in a given observational study is sufficient for obtaining consistent estimates of causal effects.
 2.
Deriving algebraic expressions for causal effect estimands.
 3.
Selecting measurements that would render randomized experiments unnecessary.
 4.
Selecting a set of indirect (randomized) experiments to replace direct experiments that are either infeasible or too expensive.
 5.
Predicting (or bounding) the efficacy of treatments from randomized trials with imperfect compliance.
  Starting with nonparametric specification of structural equations, the paper establishes the semantics necessary for a theory of interventions, presents the three rules of inference, and proposes an operational definition of structural equations."
1995,Specificity and Inheritance in Default Reasoning.,"When specificity considerations are incorporated in default reasoning systems, it is hard to ensure that exceptional subclasses inherit all legitimate features of their parent classes To reconcile these two requirements specificity and inheritance, this paper proposes the addition of a new rule called coherence rule, to the desiderata for default inference The coherence rule captures the intuition that formulae which are more compatible with the defaults in the database are more believable. We offer a formal definition of this extended desiderata and analyze the behavior of its associated closure relation which we call coference closure. We provide a concrete embodiment of a system satisfying the extended desiderata by taking the coherence closure of system Z A procedure for computing the (unique) most compact, be lief ranking in the coherence closure of system Z is also described."
1995,Counterfactuals and Policy Analysis in Structural Models.,"Evaluation of counterfactual queries (e.g., ""If A were true, would C have been true?"") is important to fault diagnosis, planning, determination of liability, and policy analysis. We present a method of revaluating counterfactuals when the underlying causal model is represented by structural models - a nonlinear generalization of the simultaneous equations models commonly used in econometrics and social sciences. This new method provides a coherent means for evaluating policies involving the control of variables which, prior to enacting the policy were influenced by other variables in the system."
1995,Logarithmic-Time Updates and Queries in Probabilistic Networks.,"In this paper we propose a dynamic data structure that supports efficient algorithms for updating and querying singly connected Bayesian networks (causal trees and polytrees). In the conventional algorithms, new evidence in absorbed in time O(1) and queries are processed in time O(N), where N is the size of the network. We propose a practical algorithm which, after a preprocessing phase, allows us to answer queries in time O(log N) at the expense of O(logn N) time per evidence absorption. The usefulness of sub-linear processing time manifests itself in applications requiring (near) real-time response over large probabilistic databases."
1995,Testing Identifiability of Causal Effects.,"This paper concerns the probabilistic evaluation of the effects of actions in the presence of unmeasured variables. We show that the identification of causal effect between a singleton variable X and a set of variables Y can be accomplished systematically, in time polynomial in the number of variables in the graph. When the causal effect is identifiable, a closed-form expression can be obtained for the probability that the action will achieve a specified goal, or a set of goals."
1995,On the Testability of Causal Models With Latent and Instrumental Variables.,"This paper concerns the probabilistic evaluation of the effects of actions in the presence of unmeasured variables. We show that the identification of causal effect between a singleton variable X and a set of variables Y can be accomplished systematically, in time polynomial in the number of variables in the graph. When the causal effect is identifiable, a closed-form expression can be obtained for the probability that the action will achieve a specified goal, or a set of goals."
1995,Probabilistic evaluation of sequential plans from causal models with hidden variables.,"The paper concerns the probabilistic evaluation of plans in the presence of unmeasured variables, each plan consisting of several concurrent or sequential actions. We establish a graphical criterion for recognizing when the effects of a given plan can be predicted from passive observations on measured variables only. When the criterion is satisfied, a closed-form expression is provided for the probability that the plan will achieve a specified goal."
1994,Probabilistic Evaluation of Counterfactual Queries.,"Evaluation of counterfactual queries (e.g., ""If A were true, would C have been true?"") is important to fault diagnosis, planning, and determination of liability. We present a formalism that uses probabilistic causal networks to evaluate one‚Äôs belief that the counterfactual consequent, C, would have been true if the antecedent, A, were true. The antecedent of the query is interpreted as an external action that forces the proposition A to be true, which is consistent with Lewis‚Äô Miraculous Analysis. This formalism offers a concrete embodiment of the ""closest world"" approach which (I) properly reflects common understanding of causal influences, (2) deals with the uncertainties inherent in the world, and (3) is amenable to machine representation."
1994,Symbolic Causal Networks.,"For a logical database to faithfully represent our beliefs about the world, one should not only insist on its logical consistency but also on its causal consistency. Intuitively, a database is causally inconsistent if it supports belief changes that contradict with our perceptions of causal influences - for example, coming to conclude that it must have rained only because the sprinkler was observed to be on. In this paper, we (1) suggest the notion of a causal structure to represent our perceptions of causal influences; (2) provide a formal definition of when a database is causally consistent with a given causal structure; (3) introduce symbolic causal networks as a tool for constructing databases that are guaranteed to be causally consistent; and (4) d iscuss various applications of causal consistency and symbolic causal networks, including nonmonotonic reasoning, Dempster-Shafer reasoning, truth maintenance, and reasoning about actions."
1994,Qualitative Decision Theory.,"We describe a framework for specifying conditional desires and evaluating preference queries ""would you prefer s1 over s2 given f"" under uncertainty. We refine the semantics presented in (Tan and Pearl 1994) to allow conditional desires to be overridden by more specific desires in the database. Within this framework, we also enable consideration of surprising worlds having extreme desirability values and the determination of degrees of preference."
1994,"Causation, Action and Counterfactuals.","The central aim of many empirical studies in the physical, behavioral, social, and biological sciences is the elucidation of cause-effect relationships among variables. It is through cause-effect relationships that we obtain a sense of a ""deep understanding"" of a given phenomenon, and it is through such relationships that we obtain a sense of being ""in control,"" namely, that we are able to shape the course of events by deliberate actions or policies. It is for these two reasons, understanding and control, that causal thinking is so pervasive, popping up in everything from everyday activities to high-level decision-making: For example, every car owner wonders why an engine won't start; a cigarette smoker would like to know, given his/her specific characteristics, to what degree his/her health would be affected by refraining from further smoking; a policy maker would like to know to what degree anti-smoking advertising would reduce costs of health care; and so on. Although a plethora of data has been collected on cars and on smoking and health, the appropriate methodology for extracting answers to such questions from the data has been fiercely debated, partly because some fundamental questions of causality have not been given fully satisfactory answers. "
1994,Specification and Evaluation of Preferences Under Uncertainty.,"This paper describes a framework for specifying preferences in terms of conditional desires of the form ì? is desirable if ?î, to be interpreted as ì? is preferred to ? other things being equal in any ? worldî. We demonstrate how such preference sentences may be interpreted as constraints on admissible preference rankings of worlds and how they, together with normality defaults, allow a reasoning agent to evaluate queries of the form ìwould you prefer ? 1 over ? 2 given ?î where ? 1 and ? 2 are action sequences. We also prove that by extending the syntax to allow for importance-rating of preference sentences, we obtain a language that is powerful enough to represent all possible preferences among worlds."
1994,On the Logic of iterated Belief Revision.,"We show in this paper that the AGM postulates are too weak to ensure the rational preservation of conditional beliefs during belief revision, thus permitting improper responses to sequences of observations. We remedy this weakness by proposing four additional postulates, which are sound relative to a qualitative version of probabilistic conditioning. Contrary to the AGM framework, the proposed postulates characterize belief revision as a process which may depend on elements of an epistemic state that are not necessarily captured by a belief set. We also show that a simple modification to the AGM framework can allow belief revision to be a function of epistemic states. We establish a model-based representation theorem which characterizes the proposed postulates and constrains, in turn, the way in which entrenchment orderings may be transformed under iterated belief revision.
"
1994,"Counterfactual Probabilities: Computational Methods, Bounds and Applications.","Evaluation of counterfactual queries (e.g., ""If A were true, would C have been true?"") is important to fault diagnosis, planning, and determination of liability. In this paper we present methods for computing the probabilities of such queries using the formulation proposed in [Balke and Pearl, 1994], where the antecedent of the query is interpreted as an external action that forces the proposition A to be true. When a prior probability is available on the causal mechanisms governing the domain, counterfactual probabilities can be evaluated precisely. However, when causal knowledge is specified as conditional probabilities on the observables, only bounds can computed. This paper develops techniques for evaluating these bounds, and demonstrates their use in two applications: (1) the determination of treatment efficacy from studies in which subjects may choose their own treatment, and (2) the determination of liability in product-safety litigation."
1994,On Testing Whether an Embedded Bayesian Network Represents a Probability Model.,"Testing the validity of probabilistic models containing unmeasured (hidden) variables is shown to be a hard task. We show that the task of testing whether models are structurally incompatible with the data at hand, requires an exponential number of independence evaluations, each of the form: ""X is conditionally independent of Y, given Z."" In contrast, a linear number of such evaluations is required to test a standard Bayesian network (one per vertex). On the positive side, we show that if a network with hidden variables G has a tree skeleton, checking whether G represents a given probability model P requires the polynomial number of such independence evaluations. Moreover, we provide an algorithm that efficiently constructs a tree-structured Bayesian network (with hidden variables) that represents P if such a network exists, and further recognizes when such a network does not exist."
1994,A Probabilistic Calculus of Actions.,"We present a symbolic machinery that admits both probabilistic and causal information about a given domain and produces probabilistic statements about the effect of actions and the impact of observations. The calculus admits two types of conditioning operators: ordinary Bayes conditioning, P(y|X = x), which represents the observation X = x, and causal conditioning, P(y|do(X = x)), read the probability of Y = y conditioned on holding X constant (at x ) by deliberate action. Given a mixture of such observational and causal sentences, together with the topology of the causal graph, the calculus derives new conditional probabilities of both types, thus enabling one to quantify the effects of actions (and policies) from partially specified knowledge bases, such as Bayesian networks in which some conditional probabilities may not be available."
1993,Belief Networks Revisited.,n/a
1993,A Maximum Entropy Approach to Nonmonotonic Reasoning.,"Abstract:
An approach to nonmonotonic reasoning that combines the principle of infinitesimal probabilities with that of maximum entropy, thus extending the inferential power of the probabilistic interpretation of defaults, is proposed. A precise formalization of the consequences entailed by a conditional knowledge base is provided, the computational machinery necessary for drawing these consequences is developed, and the behavior of the maximum entropy approach is compared to related work in default reasoning. The resulting formalism offers a compromise between two extremes: the cautious approach based on the conditional interpretations of defaults and the bold approach based on minimizing abnormalities.< >"
1993,From Conditional Oughts to Qualitative Decision Theory.,"The primary theme of this investigation is a decision theoretic account of conditional ought statements (e.g., ""You ought to do A, if C"") that rectifies glaring deficiencies in classical deontic logic. The resulting account forms a sound basis for qualitative decision theory, thus providing a framework for qualitative planning under uncertainty. In particular, we show that adding causal relationships (in the form of a single graph) as part of an epistemic state is sufficient to facilitate the analysis of action sequences, their consequences, their interaction with observations, their expected utilities and, hence, the synthesis of plans and strategies under uncertainty."
1993,Deciding Morality of Graphs is NP-complete.,In order to find a causal explanation for data presented in the form of covariance and concentration matrices it is necessary to decide if the graph formed by such associations is a projection of a directed acyclic graph (dag). We show that the general problem of deciding whether such a dag exists is NP-complete.
1992,On the Consistency of Defeasible Databases.,"Abstract
We propose a norm of consistency for a mixed set of defeasible and strict sentences which, guided by a probabilistic interpretation of these sentences, establishes a clear distinction between exceptions, ambiguities and outright contradictions. A notion of entailment is then defined which represents a minimal core of beliefs that must follow from the database if one is committed to avoid inconsistencies.
The paper establishes necessary and sufficient conditions for consistency, and provides a simple decision procedure for testing the consistency of a database or whether a given sentence is entailed by the database. It is also shown that if all sentences are of Horn type, consistency and entailment can be tested in polynomial time. Finally, we discuss procedures for reasoning with inconsistent databases and identifying sentences directly responsible for the inconsistency."
1992,Conditional Entailment: Bridging two Approaches to Default Reasoning.,"Abstract
In recent years, two conceptually different interpretations of default expressions have been advanced: extensional interpretations, in which defaults are regarded as prescriptions for extending one's set of beliefs, and conditional interpretations, in which defaults are regarded as beliefs whose validity is bound to a particular context. The two interpretations possess virtues and limitations that are practically orthogonal to each other. The conditional interpretations successfully resolve arguments of different ‚Äúspecificity‚Äù (e.g., ‚Äúpenguins don't fly in spite of being birds‚Äù) but fail to capture arguments of ‚Äúirrelevance‚Äù (e.g., concluding ‚Äúred birds fly‚Äù from ‚Äúbirds fly‚Äù). The opposite is true for the extensional interpretations.
This paper develops a new account of defaults, called conditional entailment, which combines the benefits of the two interpretations. Like prioritized circumscriptions, conditional entailment resolves arguments by enforcing priorities among defaults. However, instead of having to be specified by the user, these priorities are extracted automatically from the knowledge base. Similarly, conditional entailment possesses a sound and complete proof theory, based on interacting arguments and amenable to implementation in conventional ATMSs."
1992,Structure Identification in Relational Data.,"Abstract
This paper presents several investigations into the prospects for identifying meaningful structures in empirical data, namely, structures permitting effective organization of the data to meet requirements of future queries. We propose a general framework whereby the notion of identifiability is given a precise formal definition similar to that of learnability. Using this framework, we then explore if a tractable procedure exists for deciding whether a given relation is decomposable into a constraint network or a CNF theory with desirable topology and, if the answer is positive, identifying the desired decomposition. Finally, we address the problem of expressing a given relation as a Horn theory and, if this is impossible, finding the best k-Horn approximation to the given relation. We show that both problems can be solved in time polynomial in the length of the data."
1992,"Rejoinder to comments on ""reasoning with belief functions: An analysis of compatibility"".","Abstract
An earlier position paper has examined the applicability of belief-functions methodology in three reasoning tasks: (1) representation of incomplete knowledge, (2) belief-updating, and (3) evidence pooling. My conclusions were that the use of belief functions encounters basic difficulties along all three tasks, and that extensive experimental and theoretical studies should be undertaken before belief functions could be applied safely. This article responds to the discussion, in this issue, of my conclusions and the degree to which they affect the applicability of belief functions in automated reasoning tasks."
1992,Empirical Semantics for Defeasible Databases.,n/a
1992,"Rank-based Systems: A Simple Approach to Belief Revision, Belief Update, and Reasoning about Evidence and Actions.",n/a
1992,Reasoning with Qualitative Probabilities Can Be Tractable.,"We recently described a formalism for reasoning with if-then rules that re expressed with different levels of firmness [18]. The formalism interprets these rules as extreme conditional probability statements, specifying orders of magnitude of disbelief, which impose constraints over possible rankings of worlds. It was shown that, once we compute a priority function Z+ on the rules, the degree to which a given query is confirmed or denied can be computed in O(log n`) propositional satisfiability tests, where n is the number of rules in the knowledge base. In this paper, we show that computing Z+ requires O(n2 X log n) satisfiability tests, not an exponential number as was conjectured in [18], which reduces to polynomial complexity in the case of Horn expressions. We also show how reasoning with imprecise observations can be incorporated in our formalism and how the popular notions of belief revision and epistemic entrenchment are embodied naturally and tractably."
1992,An Algorithm for Deciding if a Set of Observed Independencies Has a Causal Explanation.,"In a previous paper [Pearl and Verma, 1991] we presented an algorithm for extracting causal influences from independence information, where a causal influence was defined as the existence of a directed arc in all minimal causal models consistent with the data. In this paper we address the question of deciding whether there exists a causal model that explains ALL the observed dependencies and independencies. Formally, given a list M of conditional independence statements, it is required to decide whether there exists a directed acyclic graph (dag) D that is perfectly consistent with M, namely, every statement in M, and no other, is reflected via dseparation in D. We present and analyze an effective algorithm that tests for the existence of such a day, and produces one, if it exists."
1991,Temporal Constraint Networks.,"Abstract
This paper extends network-based methods of constraint satisfaction to include continuous variables, thus providing a framework for processing temporal constraints. In this framework, called temporal constraint satisfaction problem (TCSP), variables represent time points and temporal information is represented by a set of unary and binary constraints, each specifying a set of permitted intervals. The unique feature of this framework lies in permitting the processing of metric information, namely, assessments of time differences between events. We present algorithms for performing the following reasoning tasks: finding all feasible times that a given event can occur, finding all possible relationships between two given events, and generating one or more scenarios consistent with the information provided.
We distinguish between simple temporal problems (STPs) and general temporal problems, the former admitting at most one interval constraint on any pair of time points. We show that the STP, which subsumes the major part of Vilain and Kautz's point algebra, can be solved in polynomial time. For general TCSPs, we present a decomposition scheme that performs the three reasoning tasks considered, and introduce a variety of techniques for improving its efficiency. We also study the applicability of path consistency algorithms as preprocessing of temporal problems, demonstrate their termination and bound their complexities."
1991,Axioms and Algorithms for Inferences Involving Probabilistic Independence.,"Abstract
This paper offers an axiomatic characterization of the probabilistic relation ‚ÄúX is independent of Y (written (X, Y))‚Äù, where X and Y are two disjoint sets of variables. Four axioms for (X, Y) are presented and shown to be complete. Based on these axioms, a polynomial membership algorithm is developed to decide whether any given independence statement (X, Y) logically follows from a set Œ£ of such statements, i.e., whether (X, Y) holds in every probability distribution that satisfies Œ£. The complexity of the algorithm is O(|Œ£| ¬∑ k2 + |Œ£| ¬∑ n), where |Œ£| is the number of given statements, n is the number of variables in Œ£ ‚à™ {(X, Y)}, and k is the number of variables in (X, Y)."
1991,System-Z+: A Formalism for Reasoning with Variable-Strength Defaults.,"We develop a formalism for reasoning with defaults that are expressed with different levels of firmness. Necessary and sufficient conditions for consistency are established, and a unique ranking of the rules is found, called Z+, which renders models as normal as possible subject to the consistency conditions. We provide the necessary machinery for testing consistency, computing the Z+ ranking and drawing the set of plausible conclusions it entails."
1991,Directed Constraint Networks: A Relational Framework for Causal Modeling.,"Normally, constraint networks are undirected, since constraints merely tell us which sets of values are compatible, and compatibility is a symmetrical relationship. In contrast, causal models use directed links, conveying cause-effect asymmetries. In this paper we give a relational semantics to this directionality, thus explaining why prediction is easy while diagnosis and planning are hard. We use this semantics to show that certain relations possess intrinsic directionalities, similar to those characterizing causal influences. We also use this semantics to decide when and how an unstructured set of symmetrical constraints can be configured so as to form a directed causal theory."
1991,A Theory of Inferred Causation.,"This chapter discusses the theory of inferred causation. The study of causation is central to the understanding of human reasoning. Inferences involving changing environments require causal theories that make formal distinctions between beliefs based on passive observations and those reflecting intervening actions. In applications such as diagnosis, qualitative physics, and plan recognition, a central task is that of finding a satisfactory explanation to a given set of observations, and the meaning of explanation is intimately related to the notion of causation. In some systems, causal ordering is defined as the ordering at which subsets of variables can be solved independently of others; in other systems, it follows the way a disturbance is propagated from one variable to others. An empirical semantics for causation is important for several reasons. The notion of causation is often associated with those of necessity and functional dependence; causal expressions often tolerate exceptions, primarily because of missing variables and coarse description. Temporal precedence is normally assumed essential for defining causation, and it is one of the most important clues that people use to distinguish causal from other types of associations."
1990,Logical and algorithmic properties of independence and their application to Bayesian networks.,"Abstract
This paper establishes a partial axiomatic characterization of the predicateI(X, Z, Y), to read ‚ÄúX is conditionally independent ofY, givenZ‚Äù. The main aim of such a characterization is to facilitate a solution of theimplication problem namely, deciding whether an arbitrary independence statementI(X, Z, Y) logically follows from a given setŒ£ of such statements. In this paper, we provide acomplete axiomatization and efficient algorithms for deciding implications in the case whereŒ£ is limited to one of four types of independencies:marginal independencies,fixed context independencies, arecursive set of independencies or afunctional set of independencies. The recursive and functional sets of independencies are the basic building blocks used in the construction ofBayesian networks. For these models, we show that the implication algorithm can be used to efficiently identify which propositions are relevant to a task at hand at any given state of knowledge. We also show that conditional independence is anArmstrong relation [10], i.e., checkingconsistency of a mixed set of independencies and dependencies can be reduced to a sequence of implication problems. This property also implies a strong correspondence between conditional independence and graphical representations: for every undirected graphG there exists a probability distributionP that exhibits all the dependencies and independencies embodied inG."
1990,Reasoning with belief functions: An analysis of compatibility.,"Abstract
This paper examines the applicability of belief funstions methodology in three reasoning tasks: (1) representation of incomplete knowledge, (2) belief updating, and (3) evidence pooling. We find that belief functions have difficulties representing incomplete knowledge, primarily knowledge expressed in conditional sentences. In this context, we also show that the prevailing practices of encoding if-then rules as belief function expressions are inadequate, as they lead to counterintuitive conclusions under chaining, contraposition, and reasoning by cases. Next, we examine the role of belief functions in updating states of belief and find that, if partial knowledge is encoded and updated by belief function methods, the updating process violates basic patterns of plausibility and the resulting beliefs cannot serve as a basis for rational decisions. Finally, assessing their role in evidence pooling, we find that belief functions offer a rich language for describing the evidence gathered, highly compatible with the way people summarize observations. However, the methods available for integrating evidence into a coherent state of belief capable of supporting plausible decisions cannot make use of this richness and are challenged by simpler methods based on likelihood functions."
1990,Identifying independence in bayesian networks.,"An important feature of Bayesian networks is that they facilitate explicit encoding of information about independencies in the domain, information that is indispensable for efficient inferencing. This article characterizes all independence assertions that logically follow from the topology of a network and develops a linear time algorithm that identifies these assertions. The algorithm's correctness is based on the soundness of a graphical criterion, called d?separation, and its optimality stems from the completeness of d?separation. An enhanced version of d?separation, called D?separation, is defined, extending the algorithm to networks that encode functional dependencies. Finally, the algorithm is shown to work for a broad class of nonprobabilistic independencies.
"
1990,Tree Decomposition with Applications to Constraint Processing.,"This paper concerns the task of removing redundant information from a given knowledge base, and restructuring it in the form of a tree, so as to admit efficient problem solving routines. We offer a novel approach which guarantees the removal of all redundancies that hide a tree structure. We develop a polynomial time algorithm that, given an arbitrary constraint network, generates a precise tree representation whenever such a tree can be extracted from the input network, otherwise, the fact that no tree representation exists is acknowledged, and the tree generated may serve as a good approximation to the original network."
1990,A Maximum Entropy Approach to Nonmonotonic Reasoning.,"This paper describes a probabilistic approach to nonmonotonic reasoning which combines the principle of infinitesimal probabilities with that of maximum entropy, and which sanctions inferences similar to those produced by the principle of minimizing abnormalities. The paper provides a precise formalization of the consequences entailed by a defeasible knowledge base, develops the computational machinery necessary for deriving these consequences, and compares the behavior of the maximum entropy approach to those of e-semantics ([Pearl 89a]) and rational closure ([Lehmann 89])."
1990,Learning Causal Trees from Dependence Information.,"In constructing probabilistic networks from human judgments, we use causal relationships to convey useful patterns of dependencies. The converse task, that of inferring causal relationships from patterns of dependencies, is far less understood. This paper establishes conditions under which the directionality of some interactions can be determined from non-temporal probabilistic information - an essential prerequisite for attributing a causal interpretation to these interactions. An efficient algorithm is developed that, given data generated by an undisclosed causal polytree, recovers the structure of the underlying polytree, as well as the directionality of all its identifiable links."
1990,System Z: A Natural Ordering of Defaults with Tractable Applications to Nonmonotonic Reasoning.,"Recent progress towards unifying the probabilistic and preferential models semantics for non-monotonic reasoning has led to a remarkable observation: Any consistent system of default rules imposes an unambiguous and natural ordering on these rules which, to emphasize its simple and basic character, we term ""Z-ordering."" This ordering can be used with various levels of refinement, to prioritize conflicting arguments, to rank the degree of abnormality of states of the world, and to define plausible consequence relationships. This paper defines the Z-ordering, briefly mentions its semantical origins, and illustrates two simple entailment relationships induced by the ordering. Two extensions are then described, maximum-entropy and conditional entailment, which trade in computational simplicity for semantic refinements."
1990,Equivalence and synthesis of causal models.,"Scientists often use directed acyclic graphs (days) to model the qualitative structure of causal theories, allowing the parameters to be estimated from observational data. Two causal models are equivalent if there is no experiment which could distinguish one from the other. A canonical representation for causal models is presented which yields an efficient graphical criterion for deciding equivalence, and provides a theoretical basis for extracting causal structures from empirical data. This representation is then extended to the more general case of an embedded causal model, that is, a dag in which only a subset of the variables are observable. The canonical representation presented here yields an efficient algorithm for determining when two embedded causal models reflect the same dependency information. This algorithm leads to a model theoretic definition of causation in terms of statistical dependencies."
1989,Tree Clustering for Constraint Networks.,"Abstract
The paper offers a systematic way of regrouping constraints into hierarchical structures capable of supporting search without backtracking. The method involves the formation and preprocessing of an acyclic database that permits a large variety of queries and local perturbations to be processed swiftly, either by sequential backtrack-free procedures, or by distributed constraint propagation processes."
1989,Conditional independence and its representations.,This paper summarizes recent investigations into the nature of informational dependencies and their representations. Axiomatic and graphical representations are presented which are both sound and complete for specialized types of independence statements.
1989,Learning Structure from Data: A Survey.,"This paper summarizes several investigations into the prospects of identifying meaningful structures in empirical data. Starting with an early work on identifying probabilistic trees, we extend the method to polytrees (directed trees with arbitrary edge orientation) and show that, under certain conditions, the skeleton of the polytree as well as the orientation of some of the arrows, are identifiable. We next address the problem of identifying probabilistic trees in which some of the nodes are unobservable. It is shown that such trees can be effectively identified in cases where all variables are either bi-valued or normal, and where all correlation coefficients are known precisely. Finally, it is shown that an effective procedure exists for determining whether a given categorical relation is decomposable into a tree of binary relations and, if the answer is positive, identifying the topology of such a tree. Guided by these results, we then propose a general framework whereby the notion of identifiability is given a precise formal definition, similar to that of learnability."
1989,Temporal Constraint Networks.,"This paper extends network-based methods of constraint satisfaction to include continuous variables, thus providing a framework for processing temporal constraints. In this framework, called temporal constraint satisfaction problem (TCSP), variables represent time points and temporal information is represented by a set of unary and binary constraints, each specifying a set of permitted intervals. The unique feature of this framework lies in permitting the processing of metric information, namely, assessments of time differences between events. We present algorithms for performing the following reasoning tasks: finding all feasible times that a given event can occur, finding all possible relationships between two given events, and generating one or more scenarios consistent with the information provided. We distinguish between simple temporal problems (STPs) and general temporal problems, the former admitting at most one interval constraint on any pair of time points. We show that the STP, which subsumes the major part of Vilain and Kautz's point algebra, can be solved in polynomial time. For general TCSPs, we present a decomposition scheme that performs the three reasoning tasks considered, and introduce a variety of techniques for improving its efficiency. We also study the applicability of path consistency algorithms as preprocessing of temporal problems, demonstrate their termination and bound their complexities."
1989,Probabilistic Semantics for Nonmonotonic Reasoning: A Survey.,n/a
1989,Deciding Consistency of Databases Containing Defeasible and Strict Information.,"We propose a norm of consistency for a mixed set of defeasible and strict sentences, based on a probabilistic semantics. This norm establishes a clear distinction between knowledge bases depicting exceptions and those containing outright contradictions. We then define a notion of entailment based also on probabilistic considerations and provide a characterization of the relation between consistency and entailment. We derive necessary and sufficient conditions for consistency, and provide a simple decision procedure for testing consistency and deciding whether a sentence is entailed by a database. Finally, it is shown that if al1 sentences are Horn clauses, consistency and entailment can be tested in polynomial time."
1989,d-Separation: From Theorems to Algorithms.,An efficient algorithm is developed that identifies all independencies implied by the topology of a Bayesian network. Its correctness and maximality stems from the soundness and completeness of d-separation with respect to probability theory. The algorithm runs in time O ( l E l ) where E is the number of edges in the network.
1988,Embracing Causality in Default Reasoning.,"Abstract
The purpose of this note is to draw attention to certain aspects of causal reasoning which are pervasive in ordinary discourse yet, based on the author's scan of the literature, have not received due treatment by logical formalisms of common-sense reasoning. In a nutshell, it appears that almost every default rule falls into one of two categories: expectation-evoking or explanation-evoking. The former describes association among events in the outside world (e.g., fire is typically accompanied by smoke); the latter describes how we reason about the world (e.g., smoke normally suggests fire). This distinction is consistently recognized by people and serves as a tool for controlling the invocation of new default rules. This note questions the ability of formal systems to reflect common-sense inferences without acknowledging such distinction and outlines a way in which the flow of causation can be summoned within the formal framework of default logic."
1988,On logic and probability.,n/a
1988,On probability intervals.,"Abstract
The apparent failure of individual probabilistic expressions to distinguish between uncertainty and ignorance, and between certainty and confidence, have swayed researchers to seek alternative formalisms, where confidence measures are provided explicit notation. This paper summarizes how a causal networks formulation of probabilities facilitates the representation of confidence measures as an integral part of a knowledge system that does not require the use of higher order probabilities. We also examine whether Dempster-Shafer intervals represent confidence about probabilities."
1988,"Do we need higher-order probabilities, and, if so, what do they mean?","The apparent failure of individual probabilistic expressions to distinguish uncertainty about truths from uncertainty about probabilistic assessments have prompted researchers to seek formalisms where the two types of uncertainties are given notational distinction. This paper demonstrates that the desired distinction is already a built-in feature of classical probabilistic models, thus, specialized notations are unnecessary. "
1988,The recovery of causal poly-trees from statistical data.,"Poly-trees are singly connected causal networks in which variables may arise from multiple causes. This paper develops a method of recovering ply-trees from empirically measured probability distributions of pairs of variables. The method guarantees that, if the measured distributions are generated by a causal process structured as a ply-tree then the topological structure of such tree can be recovered precisely and, in addition, the causal directionality of the branches can be determined up to the maximum extent possible. The method also pinpoints the minimum (if any) external semantics required to determine the causal relationships among the variables considered."
1988,Tree-Clustering Schemes for Constraint-Processing.,"The paper offers a systematic way of regrouping constraints into hierarchical structures capable of supporting information retrieval without backtracking. The method involves the formation and preprocessing of an acyclic database that permits a large variety of queries and local perturbations to be processed swiftly, either by sequential backtrack-free procedures, or by distributed constraint-propagation processes."
1988,On the logic of causal models.,"This paper explores the role of Directed Acyclic Graphs (DAGs) as a representation of conditional independence relationships. We show that DAGs offer polynomially sound and complete inference mechanisms for inferring conditional independence relationships from a given causal set of such relationships. As a consequence, d-separation, a graphical criterion for identifying independencies in a DAG, is shown to uncover more valid independencies then any other criterion. In addition, we employ the Armstrong property of conditional independence to show that the dependence relationships displayed by a DAG are inherently consistent, i.e. for every DAG D there exists some probability distribution P that embodies all the conditional independencies displayed in D and none other."
1988,Causal networks: semantics and expressiveness.,"Dependency knowledge of the form ""x is independent of y once z is known"" invariably obeys the four graphoid axioms, examples include probabilistic and database dependencies. Often, such knowledge can be represented efficiently with graphical structures such as undirected graphs and directed acyclic graphs (DAGs). In this paper we show that the graphical criterion called d-separation is a sound rule for reading independencies from any DAG based on a causal input list drawn from a graphoid. The rule may be extended to cover DAGs that represent functional dependencies as well as conditional dependencies."
1987,Evidential Reasoning Using Stochastic Simulation of Causal Models.,"Abstract
Stochastic simulation is a method of computing probabilities by recording the fraction of time that events occur in a random series of scenarios generated from some causal model. This paper presents an efficient, concurrent method of conducting the simulation which guarantees that all generated scenarios will be consistent with the observed data. It is shown that the simulation can be performed by purely local computations, involving products of parameters given with the initial specification of the model. Thus, the method proposed renders stochastic simulation a powerful technique of coherent inferencing, especially suited for tasks involving complex, nondecomposable models where ‚Äúballpark‚Äù estimates of probabilities will suffice."
1987,Distributed Revision of Composite Beliefs.,"Abstract
This paper extends the applications of belief network models to include the revision of belief ‚Äúcommitments,‚Äù i.e., the categorical acceptance of a subset of hypotheses which, together, constitute the most satisfactory explanation of the evidence at hand. A coherent model of nonmonotonic reasoning is introduced, and distributed algorithms for belief revision are presented. We show that, in singly connected networks, the most satisfactory explanation can be found in linear time by a message-passing algorithm similar to the one used in belief updating. In multiply connected networks, the problem may be exponentially hard but, if the network is sparse, topological considerations can be used to render the interpretation task tractable. In general, finding the most probable combination of hypotheses is no more complex than computing the degree of belief for any individual hypothesis. Applications to circuit and medical diagnosis are illustrated."
1987,Network-Based Heuristics for Constraint-Satisfaction Problems.,"Abstract
Many AI tasks can be formulated as constraint-satisfaction problems (CSP), i.e., the assignment of values to variables subject to a set of constraints. While some CSPs are hard, those that are easy can often be mapped into sparse networks of constraints which, in the extreme case, are trees. This paper identifies classes of problems that lend themselves to easy solutions, and develops algorithms that solve these problems optimally. The paper then presents a method of generating heuristic advice to guide the order of value assignments based on both the sparseness found in the constraint network and the simplicity of tree-structured CSPs. The advice is generated by simplifying the pending subproblems into trees, counting the number of consistent solutions in each simplified subproblem, and comparing these counts to decide among the choices pending in the original problem."
1987,Convince: A Conversational Inference Consolidation Engine.,"Abstract:
An operational domain-independent decision-aiding system for situation assessment tasks is described. The system elicits the user's perception of a given situation through a stylized English dialogue and focuses the user's attention on the issue of highest relevancy. Elicited problems are structured as networks where nodes represent variables and directed links represent causal relationships. The system uses a Bayesian inference procedure which combines causal and diagnostic reasoning using a bidirectional propagation of evidence in the form of belief parameters. Upon completion of the dialogue the system provides a formal structure representing relevant propositions, their interrelations, and their updated belief distributions."
1987,Embracing Causality in Formal Reasoning.,"The purpose of this note is to draw attention to certain aspects of causal reasoning which are pervasive in ordinary discourse yet, based on the author‚Äôs scan of the literature, have not received due treatment by logical formalisms of common-sense reasoning. In a nutshell, it appears that almost every default rule falls into one of two categories: expectation-evoking or explanation-evoking. The former describes association among events in the outside world (e.g., Fire is typically accompanied by smoke.); the latter describes how we reason about the world (e.g., Smoke normally suggests fire.). This distinction is consistently recognized by people and serves as a tool for controlling the invocation of new default rules. This note questions the ability of formal systems to reflect common-sense inferences without acknowledging such distinction and outlines a way in which the flow of causation can be summoned within the formal framework of default logic."
1987,The Logic of Representing Dependencies by Directed Graphs.,"Data-dependencies of the type ""x can tell us more about y given that we already know z"" can be represented in various formalisms: Probabilistic Dependencies, Embedded-Multi-Valued Dependencies, Undirected Graphs and Directed-Acyclic Graphs (DAGs). This paper provides an axiomatic basis, called a semi-graphoid which captures the structure common to all four types of dependencies and explores the expressive power of DAGs in representing various types of data dependencies. It is shown that DAGs can represent a richer set of dependencies than undirected graphs, that DAGs completely represent the closure of their specification bases, and that they offer an effective computational device for testing membership in that closure as well as inferring new dependencies from given inputs. These properties might explain the prevailing use of DAGs in causal reasoning and semantic nets."
1987,An Improved Constraint-Propagation Algorithm for Diagnosis.,"Diagnosing a system requires the identification of a set of components whose abnormal behavior could explain the faulty system behavior. Previously, model-based diagnosis schemes have proceeded through a cycle of assumptions - predictions observations assumptions-adjustment, where the basic assumptions entail the proper functioning of those components whose failure is not established. Here we propose a scheme in which every component's status is treated as a variable; therefore, predictions covering all possible behavior of the system can be generated. Remarkably, the algorithm exhibits a drastic reduction in complexity for a large family of system-models. Additionally, the intermediate computations provide useful guidance for selecting new tests.

The proposed scheme may be considered as either an enhancement of the scheme proposed in [de Kleer, 1986] or an adaptation of the probabilistic propagation scheme proposed in [Pearl, 1986] for the diagnosis of deterministic systems."
1987,The Recovery of Causal Poly-Trees from Statistical Data.,"Poly-trees are singly connected causal networks in which variables may arise from multiple causes. This paper develops a method of recovering poly-trees from empirically measured probability distributions of pairs of variables. The method guarantees that, if the measured distributions are generated by a causal process structured as a poly-tree then the topological structure of such tree cam be recovered precisely and, in addition, the causal directionality of the branches can be determined up to the maximum extent possible. The method also pinpoints the minimum (if any) external semantics required to determine the causal relationships among the variables considered."
1987,Structuring Causal Tree Models with Continuous Variables.,"This paper considers the problem of invoking auxiliary, unobservable variables to facilitate the structuring of causal tree models for a given set of continuous variables. Paralleling the treatment of bi-valued variables in [Pearl 1986], we show that if a collection of coupled variables are governed by a joint normal distribution and a tree-structured representation exists, then both the topology and all internal relationships of the tree can be uncovered by observing pairwise dependencies among the observed variables (i.e., the leaves of the tree). Furthermore, the conditions for normally distributed variables are less restrictive than those governing bi-valued variables. The result extends the applications of causal tree models which were found useful in evidential reasoning tasks."
1986,On Evidential Reasoning in a Hierarchy of Hypotheses.,n/a
1986,"Fusion, Propagation, and Structuring in Belief Networks.","Abstract
Belief networks are directed acyclic graphs in which the nodes represent propositions (or variables), the arcs signify direct dependencies between the linked propositions, and the strengths of these dependencies are quantified by conditional probabilities. A network of this sort can be used to represent the generic knowledge of a domain expert, and it turns into a computational architecture if the links are used not merely for storing factual knowledge but also for directing and activating the data flow in the computations which manipulate this knowledge.
The first part of the paper deals with the task of fusing and propagating the impacts of new information through the networks in such a way that, when equilibrium is reached, each proposition will be assigned a measure of belief consistent with the axioms of probability theory. It is shown that if the network is singly connected (e.g. tree-structured), then probabilities can be updated by local propagation in an isomorphic network of parallel and autonomous processors and that the impact of new information can be imparted to all propositions in time proportional to the longest path in the network.
The second part of the paper deals with the problem of finding a tree-structured representation for a collection of probabilistically coupled propositions using auxiliary (dummy) variables, colloquially called ‚Äúhidden causes.‚Äù It is shown that if such a tree-structured representation exists, then it is possible to uniquely uncover the topology of the tree by observing pairwise dependencies among the available propositions (i.e., the leaves of the tree). The entire tree structure, including the strengths of all internal relationships, can be reconstructed in time proportional to n log n, where n is the number of leaves."
1986,Structuring causal trees.,"Abstract
Models of complex phenomena often consist of hypothetical entities called ‚Äúhidden causes,‚Äù which cannot be observed directly and yet play a major role in understanding those phenomena. This paper examines the computational roles of these constructs, and addresses the question of whether they can be discovered from empirical observations. Causal models are treated as trees of binary random variables where the leaves are accessible to direct observation, and the internal nodes‚Äîrepresenting hidden causes‚Äîaccount for interleaf dependencies. In probabilistic terms, every two leaves are conditionally independent given the value of some internal node between them. We show that if the mechanism which drives the visible variables is indeed tree structured, then it is possible to uncover the topology of the tree uniquely by observing pairwise dependencies among the leaves. The entire tree structure, including the strengths of all internal relationships, can be reconstructed in time proportional to n log n, where n is the number of leaves."
1986,On the Logic of Probabilistic Dependencies.,"This paper uncovers the axiomatic basis for the probabilistic relation ""x is independent of y, given z"" and offers it as a formal definition of informational dependency. Given an initial set of such independence relationships, the axioms established permits us to infer new independencies by non-numeric, logical manipulations. Additionally, the paper legitimizes the use of inference networks to represent probabilistic dependencies by establishing a clear correspondence between the two relational structures. Given an arbitrary probabilistic model, P, we demonstrate a construction of a unique edge-minimum graph G such that each time we observe a vertex x separated from y by a subset S of vertices, we can be guaranteed that variables x and y are independent in P, given the values of the variables in S."
1986,Comprehension-Driven Generation of Meta-Technical Utterances in Math Tutoring.,"A technical discussion often contains conversational expressions like ""however,"" ""as I have stated before,"" ""next,"" etc. These expressions, denoted Meta-technical Utterances (MTUs) carry important information which the listener uses to speed up the comprehension process. In this research we model the meaning of MTUs in terms of their anticipated effect on the listener comprehension, and use these predictions to select MTUs and weave them into a computer generated discourse. This paradigm was implemented in a system called FIGMENT, which generates commentaries on the solution of algebraic equations."
1986,Graphoids: Graph-Based Logic for Reasoning about Relevance Relations or When would x tell you more about y if you already know z?,n/a
1986,Probabilistic reasoning using graphs.,n/a
1986,Distributedrevision of belief commitment in composite explanations.,"This paper extends the applications of belief-networks to include the revision of belief commitments, i.e., the categorical acceptance of a subset of hypotheses which, together, constitute the most satisfactory explanation of the evidence at hand. A coherent model of non-monotonic reasoning is established and distributed algorithms for belief revision are presented. We show that, in singly connected networks, the most satisfactory explanation can be found in linear time by a message-passing algorithm similar to the one used in belief updating. In multiply-connected networks, the problem may be exponentially hard but, if the network is sparse, topological considerations can be used to render the interpretation task tractable. In general, finding the most probable combination of hypotheses is no more complex than computing the degree of belief for any individual hypothesis. Applications to medical diagnosis are illustrated."
1985,Generalized Best-First Search Strategies and the Optimality of A*.,"This paper reports several properties of heuristic best-first search strategies whose scoring functions É depend on all the information available from each candidate path, not merely on the current cost g and the estimated completion cost h. It is shown that several known properties of A* retain their form (with the minmax of f playing the role of the optimal cost), which helps establish general tests of admissibility and general conditions for node expansion for these strategies. On the basis of this framework the computational optimality of A*, in the sense of never expanding a node that can be skipped by some other algorithm having access to the same heuristic information that A* uses, is examined. A hierarchy of four optimality types is defined and three classes of algorithms and four domains of problem instances are considered. Computational performances relative to these algorithms and domains are appraised. For each class-domain combination, we then identify the strongest type of optimality that exists and the algorithm for achieving it. The main results of this paper relate to the class of algorithms that, like A*, return optimal solutions (i.e., admissible) when all cost estimates are optimistic (i.e., h ? h*). On this class, A* is shown to be not optimal and it is also shown that no optimal algorithm exists, but if the performance tests are confirmed to cases in which the estimates are also consistent, then A* is indeed optimal. Additionally, A* is also shown to be optimal over a subset of the latter class containing all best-first algorithms that are guided by path-dependent evaluation functions.
"
1985,How to Do with Probabilities What People Say You Can't.,n/a
1985,Learning Hidden Causes from Empirical Data.,"Models of complex phenomena often consist of hypothetical entities called ""hidden causes"", which cannot be observed directly and yet play a major role in understanding, communicating, and predicting the dynamics of those phenomena. This paper examines the cognitive and computational roles of these constructs, and addresses the question of whether they can be discovered from empirical observations.

Causal models are treated as trees of binary random variables where the leaves are accessible to direct observation, and the internal nodes-representing hidden causes-account for inter-leaf dependencies. In probabilistic terms, every two leaves are conditionally independent given the value of some internal node between them.

We show that if the mechanism which drives the visible variables is indeed tree-structured, then it is possible to uncover the topology of the tree uniquely by observing pair-wise dependencies among the leaves. The entire tree structure, including the strengths of all internal relationships, can be reconstructed in time proportional to nlogn, where n is the number of leaves."
1985,The Anatomy of Easy Problems: A Constraint-Satisfaction Formulation.,"This work aims towards the automatic generation of advice to guide the solution of difficult constraint-satisfaction problems (CSPs). The advice is generated by consulting relaxed, easy models which are backtrack-free.

We identify a subset of CSPs whose syntactic and semantic properties make them easy to solve. The syntactic properties involve the structure of the constraint graph, while the semantic properties guarantee some local consistencies among the constraints. In particular, problems supported by tree-like constraint graphs, and some width-2 graphs, can be easily solved and are therefore chosen as the target model for the relaxation scheme. Optimal algorithms for solving easy problems are presented and analyzed. Finally, an efficient method is introduced for extracting advice from easy problems and using it to speedup the solution of hard problems."
1985,A Constraint-Propagation Approach to Probabilistic Reasoning.,"The paper demonstrates that strict adherence to probability theory does not preclude the use of concurrent, self-activated constraint-propagation mechanisms for managing uncertainty. Maintaining local records of sources-of-belief allows both predictive and diagnostic inferences to be activated simultanously and propagate harmoniously towards a stable equillibrium. "
1984,Some Recent Results in Heuristic Search Theory.,"Abstract:
The paper summarizes recent analytical investigations of the mathematical properties of heuristics and their influence on the performance of common search techniques. The results are reported without proofs together with discussions of motivations and interpretations. The highlights include the following: the optimality of A*; relations between the precision of the heuristic estimates and the average complexity of the search; comparisons of the average complexities of A* and BACKTRACKING; procedures for comparing and combining nonadmissible heuristic functions; the influence of the weight w (in f = (l - w) g + wh) on the complexity of A*; the pruning power of alphabeta, SSS*, and SCOUT; the effect of successor ordering on search complexity, and the effect of search depth of the quality of decisions in game-playing."
1983,Knowledge Versus Search: A Quantitative Analysis Using A*.,"Abstract
This paper analyzes the average number of nodes expanded by
as a function of the accuracy of its heuristic estimates, by treating the errors h‚àó - h as random variables whose distribution may vary over the nodes in the graph. The search model consists of an m-ary tree with unit branch costs and a unique goal state situated at a distance N from the root.
The main result states that if the typical error grows like œÜ(h‚àó) then the mean complexity of
grows approximately like G(N)exp[cœÜ(N)], where c is a positive constant and G(N) is O(N2). Thus, a necessary and sufficient condition for maintaining polynomial search complexity is that
be guided by heuristics with logarithmic precision, e.g. œÜ(N) = (log N)k.
is shown to make much greater use of its heuristic knowledge than a backtracking procedure would under similar conditions."
1983,On the Nature of Pathology in Game Searching.,"Abstract
Game-playing programs usually process the estimates attached to game positions through repeated minimax operations, as if these were true terminal payoffs. This process introduces a spurious noise which degrades the quality of the decisions and, in the extreme case, may cause a pathological phenomenon: the deeper we search the worse we play.
Using a probabilistic game model, this paper examines the nature of this distortion, quantifies its magnitude, determines the conditions when its damage is curtailed and explains why search-depth pathology (the extreme manifestation of minimax distortion) is rarely observed in common games."
1983,Searching for an Optimal Path in a Tree with Random Costs.,"Abstract
We consider the problem of finding an optimal path leading from the root of a tree to any of its leaves. The tree is known to be uniform, binary, and of height N, and each branch independently may have a cost of 1 or 0 with probability p and 1‚àíp, respectively.
We show that for p<1/2 the uniform cost algorithm can find a cheapest path in linear expected time. By contrast, when p>1/2, every algorithm which guarantees finding an exact cheapest path, or even a path within a fixed cost ratio of the cheapest, must run in exponential average time. If, however, we are willing to accept a near optimal solution almost always, then a pruning algorithm exists which finds such a solution in linear expected time. The algorithm employs a depth-first strategy which stops at regular intervals to appraise its progress and, if the progress does not meet a criterion based on domain-specific knowledge, the current node is irrevocably pruned."
1983,A Minimax Algorithm Better Than Alpha-Beta? Yes and No.,"Abstract
This paper contains a probabilistic analysis of the performance of the game-searching SSS* algorithm which has been shown to be superior to Œ±-Œ≤. Necessary and sufficient conditions for node expansion are established, and an expression for the average number of nodes expanded is derived. The branching factor of SSS* is shown to coincide with that of Œ±-Œ≤, thus rendering the two algorithms asymptotically equivalent. Numerical comparison of the expected complexities of the two algorithms is finally carried out over a wide spectrum of search depths and branching degrees. The latter shows that the savings in the number of positions evaluated by SSS* relative to that of Œ±-Œ≤ is rather limited and is not enough to offset the increase in other computational resources."
1983,On the Discovery and Generation of Certain Heuristics.,"Abstract
This paper explores the paradigm that heuristics are discovered by consulting simplified models of the problem domain. After describing the features of typical heuristics on some popular problems, we demonstrate that these heuristics can be obtained by the process of deleting constraints from the original problem and solving the relaxed problem which ensues. We then outline a scheme for generating such heuristics mechanically, which involves systematic refinement and deletion of constraints from the original problem specification until a semidecomposable model is identified. The solution to the latter constitutes a heuristic for the former."
1983,The Optimality of A* Revisited.,"This paper examines the optimality of A*, in the sense of expanding the least number of distinct nodes, over three classes of algorithms which return solutions of comparable costs to that found by A*. We first show that A* is optimal over those algorithms guaranteed to And a solution at least as good as A*‚Äôs for every heuristic assignment h. Second, we consider a wider class of algorithms which, like A*, are guaranteed to find an optimal solution (i.e., admissible) if all cost estimates are optimistic (i.e., h(h*). On this class we show that A* is not optimal and that no optimal algorithm exists unless h is also consistent, in which case A* is optimal. Finally we show that A* is optimal over the subclass of best-first algorithms which are admissible whenever h(h*)."
1983,A Computational Model for Causal and Diagnostic Reasoning in Inference Systems.,"This paper introduces a representation of evidential relationships which permits updating of belief in two simultaneous modes: causal (i. e. top-down) and diagnostic (i.e. bottom-up). It extends the hierarchical tree representation by allowing multiple causes to a given manifestation. We develop an updating scheme that obeys the axioms of probability, is computationally efficient, and is compatible with experts reasoning. The belief parameters of each variable are defined and updated by those of its neighbors in such a way that the impact of each new evidence propagates and settles through the network in a single pass."
1982,The Solution for the Branching Factor of the Alpha-Beta Pruning Algorithm and its Optimality.,n/a
1982,GODDESS: A Goal-Directed Decision Structuring System.,"Abstract:
This paper describes an operational version of a computerized, domain-independent, decision support system which is based on a novel, goal-directed structure for representing decision problems. The structure allows the user to state relations among aspects, effects, conditions, and goals, in addition to actions and states which are the basic components of the traditional decision tree approach. The program interacts with the user in a stylized English-like dialogue, starting with the stated objectives and proceeding to unravel the more detailed means by which these objectives can be realized. At any point in time, the program focuses the user's attention on the issues which are most crucial to the problem at hand. The structure used is more compatible with the way people encode knowledge about problems and actions, and therefore promises to offer the following advantages: 1) judgments and beliefs issued by the user constitute a more valid representation of the user's experience; and 2) the user may be guided toward the discovery of action alternatives he otherwise would not have identified."
1982,Studies in Semi-Admissible Heuristics.,"Abstract:
The paper introduces three extensions of the A* search algorithm which improve the search efficiency by relaxing the admissibility condition. 1) A* œµ employs an admissible heuristic function but invokes quicker termination conditions while still guaranteeing that the cost of the solution found will not exceed the optimal cost by a factor greater than 1 + œµ. 2) R* Œ¥ may employ heuristic functions which occasionally violate the admissibility condition, but guarantees that at termination the risk of missing the opportunity for further cost reduction is at most Œ¥. 3) RŒ¥*,*œµ is a speedup version of RŒ¥*, combining the termination condition of A*œµ with the risk-admissibility condition of RŒ¥*. The Traveling Salesman problem was used as a test vehicle to examine the performances of the algorithms A*œµ and RŒ¥*. The advantages of A*œµ are shown to be significant in difficult problems, i.e., problems requiring a large number of expansions due to the presence of many subtours of roughly equal costs. The use of RŒ¥* is shown to produce a 4:1 reduction in search time with only a minor increase in final solution cost."
1982,Reverend Bayes on Inference Engines: A Distributed Hierarchical Approach.,"This paper presents generalizations of Bayes likelihood-ratio updating rule which facilitate an asynchronous propagation of the impacts of new beliefs and/or new evidence in hierarchically organized inference structures with multi-hypotheses variables. The computational scheme proposed specifies a set of belief parameters, communication messages and updating rules which guarantee that the diffusion of updated beliefs is accomplished in a single pass and complies with the tenets of Bayes calculus."
1982,The Utility of Precision in Search Heuristics.,n/a
1981,A Space-Efficient On-Line Method of Computing Quantile Estimates.,"Abstract
The task of computing an estimate for the quantile (Œ∂q) for an unknown distribution F (i.e., F(Œ∂q) = q) is usually performed by the ‚Äúsample quantile‚Äù method, which computes the ‚åäNq‚åã + 1 smallest element from the set of N observations, and thus requires that all N samples be retained in memory. This paper introduces a recursive method of estimating Œ∂q based on the fact that if the terminal nodes of a uniform d-ary tree are assigned random values, independently drawn from a distribution F, then the minimax alue of the root node converges to a specified quantile of F for very tall trees. The new estimate is shown to be almost as precise as that produced by the sample quantile method and, like it, is guaranteed to converge to Œ∂q when the sample is large for any arbitrary distribution F. However, in contrast to the sample quantile computation the proposed method requires the retention in storage of at most log2N representative data points, where N is the number of samples observed in the past. Moreover, the estimate can be updated quickly using an average of 4, and a maximum of 2 log2N, comparisons with each new observation."
1981,The Solution for the Branching Factor of the Alpha-Beta Pruning Algorithm.,"Abstract
This paper analyzes Nn,d, the average number of terminal nodes examined by the Œ±-Œ≤ pruning algorithm in a uniform game-tree of degree n and depth d for which the terminal values are drawn at random from a continuous distribution. It is shown that Nn,d attains the branching factor ‚ÑùŒ±‚àíŒ≤(n)=Œæn/l-Œæn where Œæn is the positive root of xn+x-l=0. The quantity Œæn/1-Œæn has previously been identified as a lower bound for all directional algorithms. Thus, the equality ‚ÑùŒ±‚àíŒ≤(n)=Œæn/1-Œæn renders Œ±-Œ≤ asymptotically optimal over the class of directional, game-searching algorithms."
1981,Heuristic Search Theory: Survey of Recent Results.,"This paper summarizes recent analytical investigations of the mathematical properties of heuristics and their influence on the performance of common search techniques. The results are reported without proofs, together with discussions of motivations and interpretations. Highlights include the following: relations between the precision of the heuristic estimates and the average complexity of the search, comparisons of the average complexity of the search, comparisons of the average complexities of A* and BACKTRACKING, procedures for comparing and combining non-admissible heuristic functions, the influence of the weight on the complexity of A*, determination of the branching factors of alpha-beta and SSS*, and the effects of successor ordering on the complexity of alpha-beta and of search depth on the quality of decisions."
1980,Asymptotic Properties of Minimax Trees and Game-Searching Procedures.,"Abstract
The model most frequently used for evaluating the behavior of game-searching methods consists of a uniform tree of height h and a branching degree d, where the terminal positions are assigned random, independent and identically distributed values. This paper highlights some curious properties of such trees when h is very large and examines their implications on the complexity of various game-searching methods.
If the terminal positions are assigned a WIN-LOSS status with the probabilities P0 and 1 ‚àí P0, respectively, then the root node is almost a sure MIN or a sure LOSS, depending on whether P0 is higher or lower than some fixed-point probability
. When the terminal positions are assigned continuous real values, the minimax value of the root node converges rapidly to a unique predetermined value
, which is the
of the terminal distribution.
Exploiting these properties we show that a game with WIN-LOSS terminals can be solved by examining, on the average,
terminal positions if positions if
and
positions if
, the former performance being optimal for all search algorithms. We further show that a game with continuous terminal values can be evaluated by examining an average of
positions, and that this is a lower bound for all directional algorithms. Games with discrete terminal values can, in almost all cases, be evaluated by examining an average of
terminal positions. This performance is optimal and is also achieved by the ALPHA-BETA procedure."
1980,Probabilistic Analysis of the Complexity of A*.,"Abstract
This paper analyzes the number of nodes expanded by
as a function of the accuracy of its heuristic estimates by treating the errors
- h as random variables whose distributions may vary over the nodes in the graph. Our model consists of an m-ary tree with unit branch costs and a unique goal state situated at a distance N from the root.
Two results are established:
1.
(1) for any error distribution, if
is stochastically more informed than
, then
is stochastically more efficient than
, and
2.
(2) if the probability that the relative error be bounded away from zero is greater than 1/m, then the average complexity of
is exponential with N, where as if the probability of zero error is greater than 1‚Äì1/m, the average complexity is O(N)."
1980,Storage space versus validity of answers in probabilistic question-answering systems.,"Abstract:
The trade-offs between the required storage space and the validity of answers produced by probabilistic question-answering (PQA) systems are studies. If the correct answer to a given query is ""yes"" and the system, instead, issues the estimate that the query has a probability \pi of being true, then the economic loss to the user can be measured by a distortion function that decreases monotonically with \pi . A system will be called elastic if a drastic memory saving may be achieved by tolerating a small level of average distortion. The main result reported is that for a large class of distortion measures (i.e., measures exceeding (1-\pi)^{\alpha} with \alpha>0) , if a binary QA system (true-false answers) is inelastic then the corresponding probabilistic QA system must also be inelastic. This result implies, for example, that a PQA system that is designed to answer all binary questions on an arbitrary dataset is inelastic. Similarly a PQA system admitting singly conjunctive questions such as ""Are both x and y in the dataset?"" is also inelastic."
1980,SCOUT: A Simple Game-Searching Algorithm with Proven Optimal Properties.,"This paper describes a new algorithm for searching games which is conceptually simple, space efficient, and analytically tractable. It possesses optimal asymptotic properties and may offer practical advantages over alpha-beta for deep searches."
1980,Asymptotic Complexity of Game-Searching Procedures.,n/a
1979,Capacity and Error Estimates for Boolean Classifiers with Limited Complexity.,"Abstract:
This paper extends the notions of capacity and distribution-free error estimation to nonlinear Boolean classifiers on patterns with binary-valued features. We establish quantitative relationships between the dimensionality of the feature vectors (d), the combinational complexity of the decision rule (c), the number of samples in the training set (n), and the classification performance of the resulting classifier. Our results state that the discriminating capacity of Boolean classifiers is given by the product dc, and the probability of ambiguous generalization is asymptotically given by (n/dc-1) -1 0(log d)/d) for large d, and n=0(dc). In addition we show that if a fraction ŒΩ of the training samples is misclassified then the probability of error (œÄ) in subsequent samples satisfies P(|œÄ-ŒΩ| ‚©æ œµ) ‚â≤<2.773 exp (dc-e 2 n/8) for all distributions, regardless of how the classifier was discovered."
1979,Asymptotic Properties of Discrete Unitary Transforms.,"Abstract:
A method for studying the asymptotic behavior of discrete transformations is developed using numerical quadrature theory. This method allows a more convenient examination of the correlation properties of common unitary transforms for large block sizes. As a practical result of this method it is shown that the discrete cosine transform is asymptotically optimal for all finite-order Markov signals."
1979,Asymptotic rate-distortion functions for coding precedence relations (Corresp.).,"Abstract:
An improved analysis for an information system is described in which the input and output alphabets consist of (equiprobable) ordered lists of m items, with two distortion criteria: 1) the fraction of item-pairs found out of order and 2) the fraction of items found in wrong positions. For the former it is shown that, as m \rightarrow \infty , the rate-distortion function R_{1} (D) is equivalent to mG(D) with, for small D , G(D)=\log (2/D)-2+ O(D) . For the latter, R_{2} (D) = m \log m(1 - D) + m(l - D)[\log(1 - D) - 1] + o(m) ."
1979,Bounds on memory versus error trade-offs in question-answering systems.,"Abstract:
A question-answering (QA) system answers queries from a given set about data sets drawn from a given ensemble. Shannon's rate-distortion function R(D) provides the minimum amount of memory that a QA system must employ in order to achieve an average distortion less than D (the distortion can, for example, be the average proportion of erroneous answers produced by the system). The ability of a system to convert an amount D of distortion into a saving in memory is measured by the ratio R(D)/R(0) . A system will be called elastic if this ratio goes to zero as M , the size of the data set ensemble, tends to infinity. The asymptotic bounds to R(D) are derived, giving rise to elasticity conditions which involve only general system parameters."
1979,Elasticity conditions for storage versus error exchange in question-Answering systems.,"Abstract:
It has been conjectured that error-allowance could improve dramatically the performance of data processing systems. This hypothesis is tested in the framework of question-answering (QA) systems with storage requirements as a complexity measure. Shannon's rate distortion function R(D) represents the minimum amount of memory a system must employ in order to achieve an average distortion less than D (the distortion can be, for example, the average proportion of erroneous answers produced by the system). The ability of a system to convert an amount D of distortion into memory savings is measured by the ratio R(D)/R(O) . A system will be called elastic if this ratio goes to zero as the size of the dataset ensemble goes to infinity. Asymptotic bounds to R(D) are derived giving rise to elasticity conditions invoking the structure of the distortion matrix associated with the system. The bounds established represent a marked Improvement over former results by narrowing the gap between the necessary and sufficient conditions for elasticity. Moreover, conditions are established under which the amount of computation required for testing elasticity can be substantially reduced."
1977,On summarizing data using probabilistic assertions.,"Abstract:
The complexity of question-answering systems which are permitted to submit probabilistic estimates of the truth of certain propositions is considered. Bounds are derived on the expected memory space and computational work required to produce probabilistic estimates of a specified quality. The general features of the bounds are similar to those of question-answering systems constrained to true-false type answers."
1977,A Framework for Processing Value Judgments.,"Abstract:
Traditional decision-analytic practice emphasizes the distinction between probability assessments and value (or utility) judgments. Whereas techniques for elicitation and integration of subjective probabilities often can be submitted to empirical tests of validity, the fidelity of encoding value judgments has so far defied measurements. A unified approach to the treatment of the two types of judgments is presented; value judgments are interpreted as conditional probability statements. Such formulation leads to rational methodologies and procedures for solving the following tasks: 1) empirical validation and refinement of value judgments and 2) aggregating value judgments obtained from a panel of experts."
1977,An Interactive Program Fo Conversational Elicitation of Decision Structures.,"Abstract:
An interactive computer program has been designed and implemented that elicits a decision tree from a decisionmaker in an English-like conversational mode. It emulates a decision analyst who guides the decisionmaker in structuring and organizing his knowledge about a particular problem domain. The objectives of the research were: 1) to provide the decision analysis industry with a practical automated tool for eliciting decision structures where manual elicitation techniques are either infeasible or uneconomical, 2) to cast the decision analyst's behavior into a formal framework in order to examine the principles governing the elicitation procedure and gain a deeper understanding of the analysis process itself, and 3) to provide experimental psychologists with an automated research tool for coding subjects' perception of problem situations into a standard and formal representation. The approach centers on the realization that the process of conducting an elicitation dialogue is structurally identical to conducting a heuristic search on game trees, as is commonly practiced in artificial intelligence programs. Heuristic search techniques, when applied to tree elicitation, permit real-time rollback and sensitivity analysis as the tree is being formulated. Thus it is possible to concentrate effort on expanding those parts of the tree which are crucial for the resolution of the solution plan. The program requires the decisionmaker to provide provisional values at each intermediate stage in the tree construction, which estimate the promise of future opportunities open to him from that stage."
1977,An Interactive Program for Conversational Elecitation of Decision Structures.,"An interactive computer program has been designed and implemented that elicits a decision tree from a decisionmaker in an English-like conversational mode. It emulates a decision analyst who guides the decisionmaker in structuring and organizing his knowledge about a particular problem domain. The objectives of the research were: 1) to provide the decision analysis industry with a practical automated tool for eliciting decision structures where manual elicitation techniques are either infeasible or uneconomical, 2) to cast the decision analyst's behavior into a formal framework in order to examine the principles governing the elicitation procedure and gain a deeper understanding of the analysis process itself, and 3) to provide experimental psychologists with an automated research tool for coding subjects' perception of problem situations into a standard and formal representation. The approach centers on the realization that the process of conducting an elicitation dialogue is structurally identical to conducting a heuristic search on game trees, as is commonly practiced in artificial intelligence programs. Heuristic search techniques, when applied to tree elicitation, permit real-time rollback and sensitivity analysis as the tree is being formulated. Thus it is possible to concentrate effort on expanding those parts of the tree which are crucial for the resolution of the solution plan. The program requires the decisionmaker to provide provisional values at each intermediate stage in the tree construction, which estimate the promise of future opportunities open to him from that stage."
1976,An application of rate-distortion theory to pattern recognition and classification.,"Abstract
This paper establishes absolute bounds on the amount of memory required to perform pattern-recognition tasks with a misclassification rate not exceeding a specified level. Pattern-recognition activities are viewed as communication channels with sample sets as inputs and decision rules as outputs. Shannon's rate-distortion theory gives rise to language independent bounds on the complexity of class descriptions as a function of the number of samples, the size of the data space, and the error level tolerated."
1976,Memory Versus Error Characteristics for Inexact Representations of Linear Orders.,"Abstract:
This paper addresses the following question. A computer system is presented with a very long list of ordered items and files a summarized description of the data to facilitate answering queries about the order of the items in the list. What is the minimum storage space required to guarantee that the fraction of incorrect answers remains below a specified level?"
1976,On coding precedence relations with a pair-ordering fidelity criterion (Corresp.).,"Abstract:
In this correspondence, the rate-distortion function is calculated for a source emitting equiprobable precedence relations among m objects, using the fraction of pairs reproduced out-of-order as a distortion measure. For large m , the rate is approximately given by R(D) = m \log(1/D) . The rate-distortion characteristics of cluster coding is seen to be nearly ideal for low-distortion conditions."
1976,Theoretical bounds on the complexity of inexact computations.,"Abstract:
This paper considers the reduction in algorithmic complexity that can be achieved by permitting approximate answers to computational problems. It is shown that Shannon's rate-distortion function could, under quite general conditions, provide lower bounds on the mean complexity of inexact computations. As practical examples of this approach, we show that partial sorting of N items, insisting on matching any nonzero fraction of the terms with their correct successors, requires O (N \log N) comparisons. On the other hand, partial sorting in linear time is feasible (and necessary) if one permits any finite fraction of pairs to remain out of order. It is also shown that any error tolerance below 50 percent can neither reduce the state complexity of binary N -sequences from the zero-error value of O(N) nor reduce the combinational complexity of N -variable Boolean functions from the zero-error level of O(2^{N}/N) ."
1975,On the Complexity of Inexact Computations.,n/a
1975,Optimal Dyadic Models of Time-Invariant Systems.,"Abstract:
The ease of simulating a dyadic model on digital computers suggests approximating linear time-invariant (LTI) systems by dyadic models. Methods for calculating the best such approximations are provided. It is shown that the matrices characterizing the LTI system and its optimal dyadic approximant have identical diagonal elements in the Walsh domain. This fact is used to derive a direct transformation between the impulse-response function of an LTI system and that of its dyadic approximant. The transformation can be accomplished in less than N log N additions."
1975,On the residual correlation of finite-dimensional discrete Fourier transforms of stationary signals (Corresp.).,"Abstract:
The covariance matrix of the Fourier coefficients of N - sampled stationary random signals is studied. Three theorems are established. 1) If the covariance sequence is summable, the magnitude of every off-diagonal covariance element converges to zero as N \rightarrow \infty . 2) If the covariance sequence is only square summable, the magnitude of the covariance elements sufficiently far from the diagonal converges to zero as N \rightarrow \infty . 3) If the covariance sequence is square summable, the weak norm of the matrix containing only the off-diagonal elements converges to zero as N \rightarrow \infty . The rates of convergence are also determined when the covariance sequence satisfies additional conditions."
1975,On the Storage Economy of Inferential Question-Answering Systems.,"Abstract:
The possibility of gaining storage space is an argument often advanced in favor of permitting question-answering systems to make occasional errors. Absolute bounds are established on the amount of memory savings that is achievable with a specified error level for certain types of question-answering systems. Question-answering systems are treated as communication channels carrying information concerning the acceptable answers to an admissible set of queries. Shannon's rate-distortion theory is used to calculate bounds on the memory required for several question-answering tasks. For data retrieval, pattern classification, and position-matching systems, it was found that only small memory gains could be materialized from error tolerance. In pair-ordering tasks, on the other hand, more significant memory savings could be accomplished if small error rates are tolerated."
1975,On The Storage Economy Of Error-Tolerating Question-Answering Systems.,"The possibility of gaining storage space is an argument often advanced in favor of permitting question-answering systems to make occasional errors. Absolute bounds are established on the amount of memory savings that is achievable with a specified error level for certain types of question-answering systems. Question-answering systems are treated as communication channels carrying information concerning the acceptable answers to an admissible set of queries. Shannon's rate-distortion theory is used to calculate bounds on the memory required for several question-answering tasks. For data retrieval, pattern classification, and position-matching systems, it was found that only small memory gains could be materialized from error tolerance. In pair-ordering tasks, on the other hand, more significant memory savings could be accomplished if small error rates are tolerated."
1973,"Time, frequency, sequency, and their uncertainty relations (Corresp.).","Abstract:
We study the form assumed by the classical time-frequency uncertainty relations in discrete as well as nontrigonometric spectral analysis. In particular we find that if an N -sample time signal is to contain a fraction \gamma of its energy in T consecutive samples, then the minimum number of frequency components containing that same energy fraction must be greater than N/T(2\gamma - 1)^2 . It is also found that the discrete Walsh transform permits greater energy concentration (less uncertainty) than the discrete Fourier transform."
1973,On coding and filtering stationary signals by discrete Fourier transforms (Corresp.).,"Abstract:
This correspondence concerns real-time Fourier processing of stationary data and examines the widespread belief that coefficients of the discrete Fourier transform (DFT) are ""almost"" uncorrelated. We first show that any uniformly bounded N \times N Toeplitz covariance matrix T_N is asymptotically equivalent to a nonstandard circulant matrix C_N derived from the DFT of T_N . We then derive bounds on a normed distance between T_N and C_N for finite N , and show that \mid T_N - C_N \mid ^ 2 = O(1/N) for finite-order Markov processes. Finally we demonstrate that the performance degradation resulting from the use of DFT (as opposed to Karhunen-Lo√®ve expansion) in coding and filtering is proportional to \mid T_N - C_N \mid and therefore vanishes as the inverse square root of the block size N when N \rightarrow \infty ."
1971,Basis-restricted transformations and performance measures for spectral representations (Corresp.).,"Abstract:
This correspondence develops tools for comparative evaluation of the effectiveness of unitary transformations in signal processing applications. Performance measures for the tasks of optimal filters and optimal coding are exemplified, and measures of similarity among representations are proposed."
1971,Application of Walsh Transform to Statistical Analysis.,"Abstract:
Harmonic analysis of probability distribution functions has long served an important function in the treatment of stochastic systems. The tasks of generating moments and distributions of sums have been effectively executed in the Fourier spectrum. The properties of the Walsh-Hadamard transform of probability functions of discrete random variables is explored. Many analogies can be drawn between Fourier and Walsh analysis. In particular, it is shown that moments can be generated taking the Gibb's derivative of the Walsh spectrum and that products of Walsh spectra yield the distribution of dyadic sums. Stochastic systems with dyadic symmetry would benefit most from the properties of Walsh analysis and the computational advantages it offers. Some applications in the areas of information theory and pattern recognition are demonstrated."
