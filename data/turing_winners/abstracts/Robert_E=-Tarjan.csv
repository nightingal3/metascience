2019,Randomized Concurrent Set Union and Generalized Wake-Up.,"We consider the disjoint set union problem in the asynchronous shared memory multiprocessor computation model. We design a randomized algorithm that performs at most O(log n) work per operation (with high probability), and performs at most O(m #8226; (Î±(n, m/(np)) + log(np/m + 1)) total work in expectation for a problem instance with m operations on n elements solved by p processes. Our algorithm is the first to have work bounds that grow sublinearly with p against an adversarial scheduler.
We use Jayanti's Wake Up problem and our newly defined Generalized Wake Up problem to prove several lower bounds on concurrent set union. We show an Î©(log min {n,p}) expected work lower bound on the cost of any single operation on a set union algorithm. This shows that our single-operation upper bound is optimal across all algorithms when p = nÎ©(1). Furthermore, we identify a class of ""symmetric algorithms'' that captures the complexities of all the known algorithms for the disjoint set union problem, and prove an Î©(mâ€¢(Î±(n, m(np)) + log(np/m + 1))) expected total work lower bound on algorithms of this class, thereby showing that our algorithm has optimal total work complexity for this class. Finally, we prove that any randomized algorithm, symmetric or not, cannot breach an Î©(m â€¢(Î±(n, m/n) + log log(np/m + 1))) expected total work lower bound."
2019,Simple Concurrent Labeling Algorithms for Connected Components.,"We present new concurrent labeling algorithms for finding connected components, and we study their theoretical efficiency. Even though many such algorithms have been proposed and many experiments with them have been done, our algorithms are simpler. We obtain an O(lg n) step bound for two of our algorithms using a novel multi-round analysis. We conjecture that our other algorithms also take O(lg n) steps but are only able to prove an O(lg^2 n) bound. We also point out some gaps in previous analyses of similar algorithms. Our results show that even a basic problem like connected components still has secrets to reveal."
2019,A New Path from Splay to Dynamic Optimality.,"Consider the task of performing a sequence of searches in a binary search tree. After each search, an algorithm is allowed to arbitrarily restructure the tree, at a cost proportional to the amount of restructuring performed. The cost of an execution is the sum of the time spent searching and the time spent optimizing those searches with restructuring operations. This notion was introduced by Sleator and Tarjan in 1985 [27], along with an algorithm and a conjecture. The algorithm, Splay, is an elegant procedure for performing adjustments while moving searched items to the top of the tree. The conjecture, called dynamic optimality, is that the cost of splaying is always within a constant factor of the optimal algorithm for performing searches. The conjecture stands to this day.

We offer the first systematic proposal for settling the dynamic optimality conjecture. At the heart of our methods is what we term a simulation embedding: a mapping from executions to lists of keys that induces a target algorithm to simulate the execution. We build a simulation embedding for Splay by inducing it to perform arbitrary subtree transformations, and use this to show that if the cost of splaying a sequence of items is an upper bound on the cost of splaying every subsequence thereof, then Splay is dynamically optimal. We call this the subsequence property. Building on this machinery, we show that if Splay is dynamically optimal, then with respect to optimal costs, its additive overhead is at most linear in the sum of initial tree size and number of requests. As a corollary, the subsequence property is also a necessary condition for dynamic optimality. The subsequence property also implies both the traversal [27] and deque [30] conjectures.

The notions of simulation embeddings and bounding additive overheads should be of general interest in competitive analysis. For readers especially interested in dynamic optimality, we provide an outline of a proof that a lower bound on search costs by Wilber [32] has the subsequence property, and extensive suggestions for adapting this proof to Splay.
"
2019,Splaying Preorders and Postorders.,"Abstract
Let T be a binary search tree of n nodes with root r, left subtree
L=left(r)
L
, and right subtree
R=right(r)
R
. The preorder and postorder of T are defined as follows: the preorder and postorder of the empty tree is the empty sequence, and
preorder(T)
postorder(T)
=(r)âŠ•preorder(L)âŠ•preorder(R)
=postorder(L)âŠ•postorder(R)âŠ•(r),
where
âŠ•
denotes sequence concatenation. (We will refer to any such sequence as a preorder or a postorder). We prove the following results about the behavior of splaying [21] preorders and postorders:
1.
Inserting the nodes of preorder(T) into an empty tree via splaying costs O(n). (Theorem 2.)
 2.
Inserting the nodes of postorder(T) into an empty tree via splaying costs O(n). (Theorem 3.)
 3.
If
T
â€²
T
has the same keys as T and T is weight-balanced [18] then splaying either preorder(T) or postorder(T) starting from
T
â€²
T
costs O(n). (Theorem 4.)
 For 1 and 2, we use the fact that preorders and postorders are pattern-avoiding: i.e. they contain no subsequences that are order-isomorphic to (2, 3, 1) and (3, 1, 2), respectively. Pattern-avoidance implies certain constraints on the manner in which items are inserted. We exploit this structure with a simple potential function that counts inserted nodes lying on access paths to uninserted nodes. Our methods can likely be extended to permutations that avoid more general patterns. The proof of 3 uses the fact that preorders and postorders of balanced search trees do not contain many large â€œjumpsâ€ in symmetric order, and exploits this fact using the dynamic finger theorem [5, 6]. Items 2 and 3 are both novel. Item 1 was originally proved by Chaudhuri and HÃ¶ft [4]; our proof simplifies theirs. These results provide further evidence in favor of the elusive dynamic optimality conjecture [21]."
2019,Zip Trees.,"Abstract
We introduce the zip tree, (Zip: â€œTo move very fast.â€) a form of randomized binary search tree that integrates previous ideas into one practical, performant, and pleasant-to-implement package. A zip tree is a binary search tree in which each node has a numeric rank and the tree is (max)-heap-ordered with respect to ranks, with ties broken in favor of smaller keys. Zip trees are essentially treaps [8], except that ranks are drawn from a geometric distribution instead of a uniform distribution, and we allow rank ties. These changes enable us to use fewer random bits per node.
We perform insertions and deletions by unmerging and merging paths (unzipping and zipping) rather than by doing rotations, which avoids some pointer changes and improves efficiency. The methods of zipping and unzipping take inspiration from previous top-down approaches to insertion and deletion by Stephenson [10], MartÃ­nez and Roura [5], and Sprugnoli [9].
From a theoretical standpoint, this work provides two main results. First, zip trees require only
O(loglogn)
O
bits (with high probability) to represent the largest rank in an n-node binary search tree; previous data structures require
O(logn)
O
bits for the largest rank. Second, zip trees are naturally isomorphic to skip lists [7], and simplify Dean and Jonesâ€™ mapping between skip lists and binary search trees [2]."
2017,Minimum-Cost Flows in Unit-Capacity Networks.,"We consider combinatorial algorithms for the minimum-cost flow problem on networks with unit capacities, and special cases of the problem. Historically, researchers have developed special-purpose algorithms that exploit unit capacities. In contrast, for the maximum flow problem, the classical blocking flow and push-relabel algorithms for the general case also have the best bounds known for the special case of unit capacities. In this paper we show that the classical blocking flow push-relabel cost-scaling algorithms of Goldberg and Tarjan (Math. Oper. Res. 15, 430–466, 1990) for general minimum-cost flow problems achieve the best known bounds for unit-capacity problems as well. We also develop a cycle-canceling algorithm that extends Goldberg’s shortest path algorithm (Goldberg SIAM J. Comput. 24, 494–504, 1995) to minimum-cost, unit-capacity flow problems. Finally, we combine our ideas to obtain an algorithm that solves the minimum-cost bipartite matching problem in   O(r1/2mlogC)  time, where m is the number of edges, C is the largest arc cost (assumed to be greater than 1), and r is the number of vertices on the small side of the vertex bipartition. This result generalizes (and simplifies) a result of Duan et al. (2011) and solves an open problem of Ramshaw and Tarjan (2012)."
2017,Hollow Heaps.,"We introduce the hollow heap, a very simple data structure with the same amortized efficiency as the classical Fibonacci heap. All heap operations except delete and delete-min take O(1) time, worst case as well as amortized; delete and delete-min take O(log n) amortized time on a heap of n items. Hollow heaps are the simplest structure to achieve these bounds. Hollow heaps combine two novel ideas: the use of lazy deletion and re-insertion to do decrease-key operations and the use of a dag (directed acyclic graph) instead of a tree or set of trees to represent a heap. Lazy deletion produces hollow nodes (nodes without items), giving the data structure its name."
2016,Amortized rotation cost in AVL trees.,"
Resolved open problem on performance of AVL trees in terms of rotations.
â€¢
n mixed insertions and deletions can cause
rotations.
â€¢
We show a class of AVL trees that allow such expensive operations."
2016,Dominator Tree Certification and Divergent Spanning Trees.,"How does one verify that the output of a complicated program is correct? One can formally prove that the program is correct, but this may be beyond the power of existing methods. Alternatively, one can check that the output produced for a particular input satisfies the desired input--output relation by running a checker on the input--output pair. Then one only needs to prove the correctness of the checker. For some problems, however, even such a checker may be too complicated to formally verify. There is a third alternative: augment the original program to produce not only an output but also a correctness certificate, with the property that a very simple program (whose correctness is easy to prove) can use the certificate to verify that the input--output pair satisfies the desired input--output relation.
We consider the following important instance of this general question: How does one verify that the dominator tree of a flow graph is correct? Existing fast algorithms for finding dominators are complicated, and even verifying the correctness of a dominator tree in the absence of additional information seems complicated. We define a correctness certificate for a dominator tree, show how to use it to easily verify the correctness of the tree, and show how to augment fast dominator-finding algorithms so that they produce a correctness certificate. We also relate the dominator certificate problem to the problem of finding divergent spanning trees in a flow graph, and we develop algorithms to find such trees. All our algorithms run in linear time. Previous algorithms apply just to the special case of only trivial dominators, and they take at least quadratic time."
2016,A New Approach to Incremental Cycle Detection and Related Problems.,"We consider the problem of detecting a cycle in a directed graph that grows by arc insertions and the related problems of maintaining a topological order and the strong components of such a graph. For these problems, we give two algorithms, one suited to sparse graphs, the other to dense graphs. The former takes O(min {m1/2, n2/3}m) time to insert m arcs into an n-vertex graph; the latter takes O(n2log n) time. Our sparse algorithm is substantially simpler than a previous O(m3/2)-time algorithm; it is also faster on graphs of sufficient density. The time bound of our dense algorithm beats the previously best time bound of O(n5/2) for dense graphs. Our algorithms rely for their efficiency on vertex numberings weakly consistent with topological order: we allow ties. Bounds on the size of the numbers give bounds on running time."
2016,Deletion Without Rebalancing in Binary Search Trees.,"We address the vexing issue of deletions in balanced trees. Rebalancing after a deletion is generally more complicated than rebalancing after an insertion. Textbooks neglect deletion rebalancing, and many B-tree--based database systems do not do it. We describe a relaxation of AVL trees in which rebalancing is done after insertions but not after deletions, yet worst-case access time remains logarithmic in the number of insertions. For any application of balanced trees in which the number of updates is polynomial in the tree size, our structure offers performance competitive with that of classical balanced trees. With the addition of periodic rebuilding, the performance of our structure is theoretically superior to that of many, if not all, classic balanced tree structures. Our structure needs lg lg m+ 1 bits of balance information per node, where m is the number of insertions and lg is the base-two logarithm, or lg lg n+ O(1) with periodic rebuilding, where n is the number of nodes. An insertion takes up to two rotations and O(1) amortized time, not counting the time to find the insertion position. This is the same as in standard AVL trees. Using an analysis that relies on an exponential potential function, we show that rebalancing steps occur with a frequency that is exponentially small in the height of the affected node. Our techniques apply to other types of balanced trees, notably B-trees, as we show in a companion article, and particularly red-black trees, which can be viewed as a special case of B-trees."
2016,A Randomized Concurrent Algorithm for Disjoint Set Union.,"Disjoint set union is a basic problem in data structures with a wide variety of applications. We extend a known efficient sequential algorithm for this problem to obtain a simple and efficient concurrent wait-free algorithm running on an asynchronous parallel random access machine (APRAM). Crucial to our result is the use of randomization. Under a certain independence assumption, for a problem instance in which there are n elements, m operations, and l processes, our algorithm does Î¸(m (Î±{n, m/nl) + log (nl/m + 1 ))) expected work, where the expectation is over the random choices made by the algorithm and Î± is a functional inverse of Ackermann's function. In addition, each operation takes O(log n) steps with high probability.
We point out some gaps in the earlier work on this problem by Anderson and Woll [1]. More importantly, our algorithm is significantly simpler than theirs. Finally, under our independence assumption our algorithm achieves speedup close to linear for applications in which all or most of the processes can be kept busy, thereby partially answering an open problem posed by them."
2015,Rank-Balanced Trees.,"Since the invention of AVL trees in 1962, many kinds of binary search trees have been proposed. Notable are red-black trees, in which bottom-up rebalancing after an insertion or deletion takes O(1) amortized time and O(1) rotations worst-case. But the design space of balanced trees has not been fully explored. We continue the exploration. Our contributions are three: We systematically study the use of ranks and rank differences to define height-based balance in binary trees. Different invariants on rank differences yield AVL trees, red-black trees, and other kinds of balanced trees. By relaxing AVL trees, we obtain a new kind of balanced binary tree, the weak AVL tree (wavl tree), whose properties we develop. Bottom-up rebalancing after an insertion or deletion takes O(1) amortized time and at most two rotations, improving the three or more rotations per deletion needed in all other kinds of balanced trees of which we are aware. The height bound of a wavl tree degrades gracefully from that of an AVL tree as the number of deletions increases and is never worse than that of a red-black tree. Wavl trees also support top-down, fixed look-ahead rebalancing in O(1) amortized time. Finally, we use exponential potential functions to prove that in wavl trees rebalancing steps occur exponentially infrequently in rank. Thus, most of the rebalancing is at the bottom of the tree, which is crucial in concurrent applications and in those in which rotations take time that depends on the subtree size."
2015,Faster and More Dynamic Maximum Flow by Incremental Breadth-First Search.,"Abstract
We introduce the Excesses Incremental Breadth-First Search (Excesses IBFS) algorithm for maximum flow problems. We show that Excesses IBFS has the best overall practical performance on real-world instances, while maintaining the same polynomial running time guarantee of O(mn2) as IBFS, which it generalizes. Some applications, such as video object segmentation, require solving a series of maximum flow problems, each only slightly different than the previous. Excesses IBFS naturally extends to this dynamic setting and is competitive in practice with other dynamic methods."
2015,Hollow Heaps.,"Abstract
We introduce the hollow heap, a very simple data structure with the same amortized efficiency as the classical Fibonacci heap. All heap operations except delete and delete-min take O(1) time, worst case as well as amortized; delete and delete-min take
O(logn)
O
amortized time. Hollow heaps are by far the simplest structure to achieve this. Hollow heaps combine two novel ideas: the use of lazy deletion and re-insertion to do decrease-key operations, and the use of a dag (directed acyclic graph) instead of a tree or set of trees to represent a heap. Lazy deletion produces hollow nodes (nodes without items), giving the data structure its name."
2015,Minimum Cost Flows in Graphs with Unit Capacities.,"We consider the minimum cost flow problem on graphs with unit capacities and its special cases. In previous studies, special purpose algorithms exploiting the fact that capacities are one have been developed. In contrast, for maximum flow with unit capacities, the best bounds are proven for slight modifications of classical blocking flow and push-relabel algorithms. In this paper we show that the classical cost scaling algorithms of Goldberg and Tarjan (for general integer capacities) applied to a problem with unit capacities achieve or improve the best known bounds. For weighted bipartite matching we establish a bound of O(\sqrt{rm}\log C) on a slight variation of this algorithm. Here r is the size of the smaller side of the bipartite graph, m is the number of edges, and C is the largest absolute value of an arc-cost. This simplifies a result of [Duan et al. 2011] and improves the bound, answering an open question of [Tarjan and Ramshaw 2012]. For graphs with unit vertex capacities we establish a novel O(\sqrt{n}m\log(nC)) bound. We also give the first cycle canceling algorithm for minimum cost flow with unit capacities. The algorithm naturally generalizes the single source shortest path algorithm of [Goldberg 1995]."
2014,Efficient maximum flow algorithms.,"Though maximum flow algorithms have a long history, revolutionary progress is still being made."
2014,The CB tree: a practical concurrent self-adjusting search tree.,"We present the CB tree, a counting-based self-adjusting binary search tree in which, as in splay trees, more-frequently accessed items move closer to the root. In a sequential execution, after   m  operations of which   c(v)  access item   v , an access of   v  traverses a path of length   O(1+logmc(v))  while doing few if any rotations. Unlike the original splay tree, in which each access moves the accessed item all the way to the root via a sequence of rotations, accesses in a CB tree do very few rotations, specifically   O(n+nlogmn) , during a sequence of   m  operations of which   n  are insertions. This is   o(1)  (subconstant) amortized per operation if   m?n . We adapt the CB tree into a scalable concurrent self-adjusting BST. We show experimentally that the concurrent CB tree scales well because it, too, performs few rotations, and therefore self-adjusts without having rotations create a bottleneck. Our evaluation shows that the concurrent CB tree performs better than existing concurrent search trees on non-uniform access sequences derived from real workloads.
"
2014,Deletion without rebalancing in multiway search trees.,"Some database systems that use a form of B-tree for the underlying data structure do not do rebalancing on deletion. This means that a bad sequence of deletions can create a very unbalanced tree. Yet such databases perform well in practice. Avoidance of rebalancing on deletion has been justified empirically and by average-case analysis, but to our knowledge, no worst-case analysis has been done. We do such an analysis. We show that the tree height remains logarithmic in the number of insertions, independent of the number of deletions. Furthermore, the amortized time for an insertion or deletion, excluding the search time, is O(1), and nodes are modified by insertions and deletions with a frequency that is exponentially small in their height. The latter results do not hold for standard B-trees. By adding periodic rebuilding of the tree, we obtain a data structure that is theoretically superior to standard B-trees in many ways. Our results suggest that rebalancing on deletion not only is unnecessary but may be harmful."
2014,A Back-to-Basics Empirical Study of Priority Queues.,"The theory community has proposed several new heap variants in the recent past which have remained largely untested experimentally. We take the field back to the drawing board, with straightforward implementations of both classic and novel structures using only standard, well-known optimizations. We study the behavior of each structure on a variety of inputs, including artificial workloads, workloads generated by running algorithms on real map data, and workloads from a discrete event simulator used in recent systems networking research. We provide observations about which characteristics are most correlated to performance. For example, we find that the L1 cache miss rate appears to be strongly correlated with wallclock time. We also provide observations about how the input sequence affects the relative performance of the different heap variants. For example, we show (both theoretically and in practice) that certain random insertion-deletion sequences are degenerate and can lead to misleading results. Overall, our findings suggest that while the conventional wisdom holds in some cases, it is sorely mistaken in others.
"
2014,Nested Set Union.,"Abstract
We consider a version of the classic disjoint set union (union-find) problem in which there are two partitions of the elements, rather than just one, but restricted such that one partition is a refinement of the other. We call this the nested set union problem. This problem occurs in a new algorithm to find dominators in a flow graph. One can solve the problem by using two instances of a data structure for the classical problem, but it is natural to ask whether these instances can be combined. We show that the answer is yes: the nested problem can be solved by extending the classic solution to support two nested partitions, at the cost of at most a few bits of storage per element and a small constant overhead in running time. Our solution extends to handle any constant number of nested partitions."
2014,Disjoint Set Union with Randomized Linking.,"A classic result in the analysis of data structures is that path compression with linking by rank solves the disjoint set union problem in almost-constant amortized time per operation. Recent experiments suggest that in practice, a naïve linking method works just as well if not better than linking by rank, in spite of being theoretically inferior. How can this be? We prove that randomized linking is asymptotically as efficient as linking by rank. This result provides theory that matches the experiments, which implicitly do randomized linking as a result of the way the input instances are generated.
"
2014,Better Approximation Algorithms for the Graph Diameter.,"he diameter is a fundamental graph parameter and its computation is necessary in many applications. The fastest known way to compute the diameter exactly is to solve the All-Pairs Shortest Paths (APSP) problem.

In the absence of fast algorithms, attempts were made to seek fast algorithms that approximate the diameter. In a seminal result Aingworth, Chekuri, Indyk and Motwani [SODA'96 and SICOMP'99] designed an algorithm that computes in  time an estimate  for the diameter D in directed graphs with nonnegative edge weights, such that ?? · D? – (M – 1) ?  ? D, where M is the maximum edge weight in the graph. In recent work, Roditty and Vassilevska W. [STOC 13] gave a Las Vegas algorithm that has the same approximation guarantee but improves the (expected) runtime to . Roditty and Vassilevska W. also showed that unless the Strong Exponential Time Hypothesis fails, no (n2??) time algorithm for sparse unweighted undirected graphs can achieve an approximation ratio better than . Thus their algorithm is essentially tight for sparse unweighted graphs. For weighted graphs however, the approximation guarantee can be meaningless, as M can be arbitrarily large.

In this paper we exhibit two algorithms that achieve a genuine -approximation for the diameter, one running in  time, and one running in  time. Furthermore, our algorithms are deterministic, and thus we present the first deterministic (2 – ?)-approximation algorithm for the diameter that takes subquadratic time in sparse graphs.

In addition, we address the question of obtaining an additive c-approximation for the diameter, i.e. an estimate  such that D – c ?  ? D. An extremely simple  time algorithm achieves an additive n?-approximation; no better results are known. We show that for any ? > 0, getting an additive n?-approximation algorithm for the diameter running in (n2??) time for any ? > 2? would falsify the Strong Exponential Time Hypothesis. Thus the simple algorithm is probably essentially tight for sparse graphs, and moreover, obtaining a subquadratic time additive c-approximation for any constant c is unlikely.

Finally, we consider the problem of computing the eccentricities of all vertices in an undirected graph, i.e. the largest distance from each vertex. Roditty and Vassilevska W. [STOC 13] show that in  time, one can compute for each v ? V in an undirected graph, an estimate ?(v) for the eccentricity ?(v) such that max {R,  · ?(v)} ? ?(v) ? min {D,  · ?(v)} where R = minv ?(v) is the radius of the graph. Here we improve the approximation guarantee by showing that a variant of the same algorithm can achieve estimates ??(v) with  · ?(v) ? ??(v) ? ?(v).




"
2014,"Loop Nesting Forests, Dominators, and Applications.","Abstract
Loop nesting forests and dominator trees are important tools in program optimization and code generation, and they have applications in other diverse areas. In this work we first present carefully engineered implementations of efficient algorithms for computing a loop nesting forest of a given directed graph, including a very efficient algorithm that computes the forest in a single depth-first search. Then we revisit the problem of computing dominators and present efficient implementations of the algorithms recently proposed by Fraczak et al. [12], which include an algorithm for acyclic graphs and an algorithm that computes both the dominator tree and a loop nesting forest. We also propose a new algorithm than combines the algorithm of Fraczak et al. for acyclic graphs with the algorithm of Lengauer and Tarjan. Finally, we provide fast algorithms for the following related problems: computing bridges and testing 2-edge connectivity, verifying dominators and testing 2-vertex connectivity, and computing a low-high order and two independent spanning trees. We exhibit the efficiency of our algorithms experimentally on large graphs taken from a variety of application areas."
2013,Finding dominators via disjoint set union.,"Abstract
The problem of finding dominators in a directed graph has many important applications, notably in global optimization of computer code. Although linear and near-linear-time algorithms exist, they use sophisticated data structures. We develop an algorithm for finding dominators that uses only a â€œstatic treeâ€ disjoint set data structure in addition to simple lists and maps. The algorithm runs in near-linear or linear time, depending on the implementation of the disjoint set data structure. We give several versions of the algorithm, including one that computes loop nesting information (needed in many kinds of global code optimization) and that can be made self-certifying, so that the correctness of the computed dominators is very easy to verify."
2013,Soft Heaps Simplified.,"In 1998, Chazelle [J. ACM, 47 (2000), pp. 1012--1027] introduced a new kind of meldable heap (priority queue) called the soft heap. Soft heaps trade accuracy for speed: the heap operations are allowed to increase the keys of certain items, thereby making these items bad, as long as the number of bad items in the data structure is at most $\varepsilon m$, where $m$ is the total number of insertions performed so far, and $\varepsilon$ is an error parameter. The amortized time per heap operation is $O(\lg \frac{1}{\varepsilon})$, reduced from $O(\lg n)$, where $n$ is the number of items in the heap. Chazelle used soft heaps in several applications, including a faster deterministic minimum-spanning-tree algorithm and a new deterministic linear-time selection algorithm. We give a simplified implementation of soft heaps that uses less space and avoids Chazelle's dismantling operations. We also give a simpler, improved analysis that yields an amortized time bound of $O(\lg \frac{1}{\varepsilon})$ for each deletion, $O(1)$ for each other operation.
"
2013,Dominator Certification and Independent Spanning Trees: An Experimental Study.,"Abstract
We present the first implementations of certified algorithms for computing dominators, and exhibit their efficiency experimentally on graphs taken from a variety of applications areas. The certified algorithms are obtained by augmenting dominator-finding algorithms to compute a certificate of correctness that is easy to verify. A suitable certificate for dominators is obtained from the concepts of low-high orders and independent spanning trees. Therefore, our implementations provide efficient constructions of these concepts as well, which are interesting in their own right. Furthermore, we present an experimental study of efficient algorithms for computing dominators on large graphs."
2012,An Optimal Dynamic Data Structure for Stabbing-Semigroup Queries.,"Let S be a set of n intervals in $\mathbb{R}$, and let $(\mathbf{S}, +)$ be any commutative semigroup. We assign a weight $\omega(s) \in \mathbf{S}$ to each interval in S. For a point $x \in \mathbb{R}$, let $S(x) \subseteq S$ be the set of intervals that contain x. Given a point $q \in \mathbb{R}$, the stabbing-semigroup query asks for computing $\sum_{s \in S(q)} \omega(s)$. We propose a linear-size dynamic data structure, under the pointer-machine model, that answers queries in worst-case $O(\log n)$ time and supports both insertions and deletions of intervals in amortized $O(\log n)$ time. It is the first data structure that attains the optimal $O(\log n)$ bound for all three operations. Furthermore, our structure can easily be adapted to external memory, where we obtain a linear-size structure that answers queries and supports updates in $O(\log_B n)$ I/Os, where B is the disk block size. For the restricted case of a nested family of intervals (either every pair of intervals is disjoint or one contains the other), we present a simpler solution based on dynamic trees.
"
2012,"Incremental Cycle Detection, Topological Ordering, and Strong Component Maintenance.","We present two online algorithms for maintaining a topological order of a directed n-vertex acyclic graph as arcs are added, and detecting a cycle when one is created. Our first algorithm handles m arc additions in O(m3/2) time. For sparse graphs (m/n = O(1)), this bound improves the best previous bound by a logarithmic factor, and is tight to within a constant factor among algorithms satisfying a natural locality property. Our second algorithm handles an arbitrary sequence of arc additions in O(n5/2) time. For sufficiently dense graphs, this bound improves the best previous bound by a polynomial factor. Our bound may be far from tight: we show that the algorithm can take Î©(n22âˆš2 lg n) time by relating its performance to a generalization of the k-levels problem of combinatorial geometry. A completely different algorithm running in Î˜(n2 log n) time was given recently by Bender, Fineman, and Gilbert. We extend both of our algorithms to the maintenance of strong components, without affecting the asymptotic time bounds."
2012,An Algorithmic View of the Universe.,"In the years since Alan Turing, and following his lead, computer scientists advanced their understanding of computational phenomena by developing a very specialized, original and penetrating way of rigorous thinking. Now it turns out that this ""algorithmic"" way of thinking can be applied productively to the study of important phenomena outside computation proper (examples: the cell, the brain, the market, the universe, indeed mathematical truth itself). This development is an exquisite unintended consequence of the fact that there is latent computation underlying each of these phenomena, or the ways in which science studies them."
2012,A Weight-Scaling Algorithm for Min-Cost Imperfect Matchings in Bipartite Graphs.,"Abstract:
Call a bipartite graph G = (X, Y ; E) balanced when |X| = |Y |. Given a balanced bipartite graph G with edge costs, the assignment problem asks for a perfect matching in G of minimum total cost. The Hungarian Method can solve assignment problems in time O(mn+n 2 log n), where n := |X| = |Y | and m := |E|. If the edge weights are integers bounded in magnitude by C >; 1, then algorithms using weight scaling, such as that of Gabow and Tarjan, can lower the time to O(mâˆšn log(nC)). There are important applications in which G is unbalanced, with |X| â‰  |Y |, and we require a min-cost matching of size r := min(|X|, |Y |) or, more generally, of some specified size s â‰¤ r. The Hungarian Method extends easily to find such a matching in time O(ms + s 2 log r), but weightscaling algorithms do not extend so easily. We introduce new machinery to find such a matching in time O(mâˆšs log(sC)) via weight scaling. Our results provide some insight into the design space of efficient weight-scaling matching algorithms."
2012,"Dominators, Directed Bipolar Orders, and Independent Spanning Trees.","Abstract
We consider problems related to dominators and independent spanning trees in flowgraphs and provide linear-time algorithms for their solutions. We introduce the notion of a directed bipolar order, generalizing a previous notion of Plein and Cheriyan and Reif. We show how to construct such an order from information computed by several known algorithms for finding dominators. We show how to concurrently verify the correctness of a dominator tree D and a directed bipolar order O very simply, and how to construct from D and O two spanning trees whose paths are disjoint except for common dominators. Finally, we describe alternative ways to verify dominators without using a directed bipolar order."
2012,Strict fibonacci heaps.,"We present the first pointer-based heap implementation with time bounds matching those of Fibonacci heaps in the worst case. We support make-heap, insert, find-min, meld and decrease-key in worst-case O(1) time, and delete and delete-min in worst-case O(lg n) time, where n is the size of the heap. The data structure uses linear space.
A previous, very complicated, solution achieving the same time bounds in the RAM model made essential use of arrays and extensive use of redundant counter schemes to maintain balance. Our solution uses neither. Our key simplification is to discard the structure of the smaller heap when doing a meld. We use the pigeonhole principle in place of the redundant counter mechanism."
2012,CBTree: A Practical Concurrent Self-Adjusting Search Tree.,"Abstract
We present the CBTree, a new counting-based self-adjusting binary search tree that, like splay trees, moves more frequently accessed nodes closer to the root. After m operations on n items, c of which access some item v, an operation on v traverses a path of length
O(log
m
c
)
O
while performing few if any rotations. In contrast to the traditional self-adjusting splay tree in which each accessed item is moved to the root through a sequence of tree rotations, the CBTree performs rotations infrequently (an amortized subconstant o(1) per operation if mâ€‰â‰«â€‰n), mostly at the bottom of the tree. As a result, the CBTree scales with the amount of concurrency. We adapt the CBTree to a multicore setting and show experimentally that it improves performance compared to existing concurrent search trees on non-uniform access sequences derived from real workloads."
2011,Rank-Pairing Heaps.,"We introduce the rank-pairing heap, an implementation of heaps that combines the asymptotic efficiency of Fibonacci heaps with much of the simplicity of pairing heaps. Other heap implementations that match the bounds of Fibonacci heaps do so by maintaining a balance condition on the trees representing the heap. In contrast to these structures but like pairing heaps, our trees can evolve to have arbitrary (unbalanced) structure. Also like pairing heaps, our structure requires at most one cut and no other restructuring per key decrease, in the worst case: the only changes that can cascade during a key decrease are changes in node ranks. Although our data structure is simple, its analysis is not.
"
2011,Data structures for mergeable trees.,"Motivated by an application in computational geometry, we consider a novel variant of the problem of efficiently maintaining a forest of dynamic rooted trees. This variant includes an operation that merges two tree paths. In contrast to the standard problem, in which a single operation can only add or delete one arc, one merge can add and delete up to a linear number of arcs. In spite of this, we develop three different methods that need only polylogarithmic time per operation. The first method extends a solution of Farach and Thorup [1998] for the special case of paths. Each merge takes O(log2n) amortized time on an n-node forest and each standard dynamic tree operation takes O(log n) time; the latter bound is amortized, worst case, or randomized depending on the underlying data structure. For the special case that occurs in the motivating application, in which arbitrary arc deletions (cuts) do not occur, we give a method that takes O(log n) time per operation, including merging. This is best possible in a model of computation with an Î©(n log n) lower bound for sorting n numbers, since such sorting can be done in O(n) tree operations. For the even-more-special case in which there are no cuts and no parent queries, we give a method that uses standard dynamic trees as a black box: each mergeable tree operation becomes a constant number of standard dynamic tree operations. This third method can also be used in the motivating application, but only by changing the algorithm in the application. Each of our three methods needs different analytical tools and reveals different properties of dynamic trees."
2011,Maximum Flows by Incremental Breadth-First Search.,"Abstract
Maximum flow and minimum s-t cut algorithms are used to solve several fundamental problems in computer vision. These problems have special structure, and standard techniques perform worse than the special-purpose Boykov-Kolmogorov (BK) algorithm. We introduce the incremental breadth-first search (IBFS) method, which uses ideas from BK but augments on shortest paths. IBFS is theoretically justified (runs in polynomial time) and usually outperforms BK on vision problems."
2011,Theory vs. Practice in the Design and Analysis of Algorithms.,"Abstract
In this talk Iâ€™ll explore gaps between the theoretical study of algorithms and the use of algorithms in practice. Examples will be drawn from my own experiences in industry and academia, and will include data structures and network algorithms. Based on these examples Iâ€™ll try to draw conclusions to help guide the work of theoreticians and experimentalists, in an effort to make this work more relevant to the needs of practitioners."
2010,HP Transforms Product Portfolio Management with Operations Research.,"Hewlett-Packard (HP) offers many innovative products to meet diverse customer needs. The breadth of its product offering has helped the company achieve unparalleled market reach; however, it has come with significant costs and challenges. By offering multiple similar products, a manufacturer increases its overall demand volatility, reduces forecast accuracy, and can adversely affect revenue and costs across the entire product life cycle. At HP, these impacts included increases in inventory-driven costs and order-cycle time; liabilities to channel partners; and costs of operations, research and development, marketing, and administration. Furthermore, complexity in HP's product lines confused customers, sales representatives, and channel partners, sometimes driving business to competitors. HP developed two powerful operations research-based solutions for managing product variety. The first, a framework for screening new products, uses custom-built return-on-investment (ROI) calculators to evaluate each proposed new product before introduction; those that do not meet a threshold ROI level are targeted for exclusion from the proposed lineup. The second, HP's Revenue Coverage Optimization (RCO) tool, which is based on a fast, new maximum-flow algorithm, is used to manage product variety after introduction. By identifying a core portfolio of products that are important to order coverage, RCO enables HP businesses to increase operational focus on their most critical products. These tools have enabled HP to increase its profits across business units by more than $500 million since 2005. Moreover, HP has streamlined its product offerings, improved execution, achieved faster delivery, lowered overhead, and increased customer satisfaction and market share.
"
2010,Deletion Without Rebalancing in Balanced Binary Trees.,"We address the vexing issue of deletions in balanced trees. Rebalancing after a deletion is generally more complicated than rebalancing after an insertion. Textbooks neglect deletion rebalancing, and many database systems do not do it. We describe a relaxation of AVL trees in which rebalancing is done after insertions but not after deletions, yet access time remains logarithmic in the number of insertions. For many applications of balanced trees, our structure offers performance competitive with that of classical balanced trees. With the addition of periodic rebuilding, the performance of our structure is theoretically superior to that of many if not all classic balanced tree structures. Our structure needs O(log log m) bits of balance information per node, where m is the number of insertions, or O(log log n) with periodic rebuilding, where n is the number of nodes. An insertion takes up to two rotations and O(1) amortized time. Using an analysis that relies on an exponential potential function, we show that rebalancing steps occur with a frequency that is exponentially small in the height of the affected node."
2009,Shortest-path feasibility algorithms: An experimental evaluation.,"This is an experimental study of algorithms for the shortest-path feasibility problem: Given a directed weighted graph, find a negative cycle or present a short proof that none exists. We study previously known and new algorithms. Our testbed is more extensive than those previously used, including both static and incremental problems, as well as worst-case instances. We show that, while no single algorithm dominates, a small subset (including new algorithms) has very robust performance in practice. Our work advances the state of the art in the area."
2009,Dynamic trees in practice.,"Dynamic tree data structures maintain forests that change over time through edge insertions and deletions. Besides maintaining connectivity information in logarithmic time, they can support aggregation of information over paths, trees, or both. We perform an experimental comparison of several versions of dynamic trees: ST-trees, ET-trees, RC-trees, and two variants of top trees (self-adjusting and worst-case). We quantify their strengths and weaknesses through tests with various workloads, most stemming from practical applications. We observe that a simple, linear-time implementation is remarkably fast for graphs of small diameter, and that worst-case and randomized data structures are best when queries are very frequent. The best overall performance, however, is achieved by self-adjusting ST-trees."
2009,Efficiently Generating k-Best Solutions to Procurement Auctions.,"Abstract
Procurement executives often find it difficult to articulate their preferences and constraints regarding auctions, making it difficult to cast procurement decisions as straightforward optimization problems. This paper presents an efficient algorithm to aid decision support in such situations. Instead of trying to compute a single optimal solution for the auction winner determination problem, we generate many candidate solutions in ascending order of buyer expenditure. Standard techniques such as clustering and dominance pruning can then trim this list to a compact yet diverse menu of alternatives; other analyses can illuminate the cost of constraints and the competitive landscape. Our efficient solution-generation algorithm addresses sealed-bid procurement auctions with multiple suppliers and multiple types of goods available in multiple units. It supports multi-sourcing and volume discounts/surcharges in bids. Our algorithm may optionally incorporate certain classes of hard constraints, generating only solutions that satisfy them."
2009,An Experimental Study of Minimum Mean Cycle Algorithms.,"We study algorithms for the minimum mean cycle problem, a parametric version of shortest path feasibility (SPF). The three basic approaches to the problem are cycle-based, binary search, and tree-based. The first two use an SPF algorithm as a subroutine, while the latter uses a parametric approach. When implementing the SPF-based methods, one has a choice of SPF algorithms and incremental optimization strategies. There are also several ways to handle precision issues. This leads to dozens of variants, which we systematically compare. Our experimental setup is more comprehensive than in previous studies. In our experiments, the tree-based method and two implementations of the cycle-based method outperformed other approaches, including binary search.
"
2009,Rank-Pairing Heaps.,"Abstract
We introduce the rank-pairing heap, a heap (priority queue) implementation that combines the asymptotic efficiency of Fibonacci heaps with much of the simplicity of pairing heaps. Unlike all other heap implementations that match the bounds of Fibonacci heaps, our structure needs only one cut and no other structural changes per key decrease; the trees representing the heap can evolve to have arbitrary structure. Our initial experiments indicate that rank-pairing heaps perform almost as well as pairing heaps on typical input sequences and better on worst-case sequences."
2009,Deletion without Rebalancing in Multiway Search Trees.,"Abstract
Many database systems that use a Bâ€‰+â€‰ tree as the underlying data structure do not do rebalancing on deletion. This means that a bad sequence of deletions can create a very unbalanced tree. Yet such databases perform well in practice. Avoidance of rebalancing on deletion has been justified empirically and by average-case analysis, but to our knowledge no worst-case analysis has been done. We do such an analysis. We show that the tree height remains logarithmic in the number of insertions, independent of the number of deletions. Furthermore the amortized time for an insertion or deletion, excluding the search time, is O(1), and nodes are modified by insertions and deletions with a frequency that is exponentially small in their height. The latter results do not hold for standard Bâ€‰+â€‰ trees. By adding periodic rebuilding of the tree, we obtain a data structure that is theoretically superior to standard Bâ€‰+â€‰ trees in many ways. We conclude that rebalancing on deletion can be considered harmful."
2009,Rank-Balanced Trees.,"Abstract
Since the invention of AVL trees in 1962, a wide variety of ways to balance binary search trees have been proposed. Notable are red-black trees, in which bottom-up rebalancing after an insertion or deletion takes O(1) amortized time and O(1) rotations worst-case. But the design space of balanced trees has not been fully explored. We introduce the rank-balanced tree, a relaxation of AVL trees. Rank-balanced trees can be rebalanced bottom-up after an insertion or deletion in O(1) amortized time and at most two rotations worst-case, in contrast to red-black trees, which need up to three rotations per deletion. Rebalancing can also be done top-down with fixed lookahead in O(1) amortized time. Using a novel analysis that relies on an exponential potential function, we show that both bottom-up and top-down rebalancing modify nodes exponentially infrequently in their heights."
2008,Planarity Algorithms via PQ-Trees (Extended Abstract).,"Abstract
We give an abstract vertex-addition method for planarity testing that encompasses the algorithms of Lempel, Even, and Cederbaum, Shih and Hsu, and Boyer and Myrvold. The main difference between the former and the latter two is the order of vertex addition; the latter two differ only in implementation details. For the general method we give a direct proof of correctness that avoids the use of Kuratowski's theorem. We give a linear-time implementation that simplifies and unifies the Shih-Hsu and Boyer-Myrvold methods. Our algorithm extends to generate embeddings uniformly at random, to count embeddings, to represent all embeddings, and to produce a Kuratowski subgraph of a non-planar graph. Our algorithm keeps track of all possible embeddings by reinterpreting Booth and Lueker's PQ-tree data structure to represent circular instead of linear orders. This interpretation of PQ-trees gives the PC-trees of Shih and Hsu and leads to a simpler, more-symmetric form of PQ-tree reduction."
2008,Finding Strongly Knit Clusters in Social Networks.,"Social networks are ubiquitous. The discovery of closely knit clusters in these networks is of fundamental and practical interest. Existing clustering criteria are limited in that clusters typically do not overlap, all vertices are clustered, and/or external sparsity is ignored. We introduce a new criterion that overcomes these limitations by combining internal density with external sparsity in a natural way. This paper explores combinatorial properties of internally dense and externally sparse clusters. A simple algorithm is given for provably finding such clusters assuming a sufficiently large gap between internal density and external sparsity. Exoerimental results show that the algorithm is able to identify over 90% of the clusters in real graphs, assuming conditions on external sparsity."
2008,Finding a feasible flow in a strongly connected network.,"Abstract
We give a linear-time algorithm to find a feasible flow in a strongly connected network with fixed supplies and demands, each summing to a common value that is at most the minimum arc capacity. This algorithm speeds up the Goldbergâ€“Rao maximum flow method by a constant factor."
2008,Linear-Time Algorithms for Dominators and Other Path-Evaluation Problems.,"We present linear-time algorithms for the classic problem of finding dominators in a flowgraph, and for several other problems whose solutions require evaluating a function defined on paths in a tree. Although all these problems had linear-time solutions previously, our algorithms are simpler, in some cases substantially. Our improvements come from three new ideas: a refined analysis of path compression that gives a linear bound if the compressions favor certain nodes; replacement of random-access table look-up by a radix sort; and a more careful partitioning of a tree into easily managed parts. In addition to finding dominators, our algorithms find nearest common ancestors off-line, verify and construct minimum spanning trees, do interval analysis of a flowgraph, and build the component tree of a weighted tree. Our algorithms do not require the power of a random-access machine; they run in linear time on a pointer machine. The genesis of our work was the discovery of a subtle error in the analysis of a previous allegedly linear-time algorithm for finding dominators. That algorithm was an attempt to simplify a more complicated algorithm, which itself was intended to correct errors in a yet earlier algorithm. Our work provides a systematic study of the subtleties in the dominators problem, the techniques needed to solve it in linear time, and the range of application of the resulting methods. We have tried to make our techniques as simple and as general as possible and to understand exactly how earlier approaches to the dominators problem were either incorrect or overly complicated.
"
2008,"Thin heaps, thick heaps.","The Fibonacci heap was devised to provide an especially efficient implementation of Dijkstra's shortest path algorithm. Although asyptotically efficient, it is not as fast in practice as other heap implementations. Expanding on ideas of HÃ¸yer [1995], we describe three heap implementations (two versions of thin heaps and one of thick heaps) that have the same amortized efficiency as Fibonacci heaps, but need less space and promise better practical performance. As part of our development, we fill in a gap in HÃ¸yer's analysis."
2008,Shortest Path Feasibility Algorithms: An Experimental Evaluation.,"This is an experimental study of algorithms for the shortest path feasibility problem: Given a directed weighted graph, find a negative cycle or present a short proof that none exists. We study previously known and new algorithms. Our testbed is more extensive than those previously used, including both static and incremental problems, as well as worst-case instances. We show that, while no single algorithm dominates, a small subset (including a new algorithm) has very robust performance in practice. Our work advances state of the art in the area.
"
2008,Faster Algorithms for Incremental Topological Ordering.,"Abstract
We present two online algorithms for maintaining a topological order of a directed acyclic graph as arcs are added, and detecting a cycle when one is created. Our first algorithm takes O(m 1/2) amortized time per arc and our second algorithm takes O(n 2.5/m) amortized time per arc, where n is the number of vertices and m is the total number of arcs. For sparse graphs, our O(m 1/2) bound improves the best previous bound by a factor of logn and is tight to within a constant factor for a natural class of algorithms that includes all the existing ones. Our main insight is that the two-way search method of previous algorithms does not require an ordered search, but can be more general, allowing us to avoid the use of heaps (priority queues). Instead, the deterministic version of our algorithm uses (approximate) median-finding; the randomized version of our algorithm uses uniform random sampling. For dense graphs, our O(n 2.5/m) bound improves the best previously published bound by a factor of n 1/4 and a recent bound obtained independently of our work by a factor of logn. Our main insight is that graph search is wasteful when the graph is dense and can be avoided by searching the topological order space instead. Our algorithms extend to the maintenance of strong components, in the same asymptotic time bounds."
2008,Reachability Problems on Directed Graphs.,"Abstract
I will present recent work and open problems on two directed graph reachability problems, one dynamic, one static. The dynamic problem is to detect the creation of a cycle in a directed graph as arcs are added. Much progress has been made recently on this problem, but intriguing questions remain. The static problem is to compute dominators and related information on a flowgraph. This problem has been solved, but the solution is complicated, and there are related problems that are not so well understood. The work to be discussed is by colleagues, other researchers, and the speaker."
2008,Fast exact and heuristic methods for role minimization problems.,"We describe several new bottom-up approaches to problems in role engineering for Role-Based Access Control (RBAC). The salient problems are all NP-complete, even to approximate, yet we find that in instances that arise in practice these problems can be solved in minutes. We first consider role minimization, the process of finding a smallest collection of roles that can be used to implement a pre-existing user-to-permission relation. We introduce fast graph reductions that allow recovery of the solution from the solution to a problem on a smaller input graph. For our test cases, these reductions either solve the problem, or reduce the problem enough that we find the optimum solution with a (worst-case) exponential method. We introduce lower bounds that are sharp for seven of nine test cases and are within 3.4% on the other two. We introduce and test a new polynomial-time approximation that on average yields 2% more roles than the optimum. We next consider the related problem of minimizing the number of connections between roles and users or permissions, and we develop effective heuristic methods for this problem as well. Finally, we propose methods for several related problems."
2007,Server Allocation Algorithms for Tiered Systems.,"Abstract
Many web-based systems have a tiered application architecture, in which a request needs to transverse all the tiers before finishing its processing. One of the most important QoS metrics for these applications is the expected response time for the user. Since the expected response time in any tier depends upon the number of servers allocated to this tier, and a request's total response time is the sum of the response times over all the tiers, many different configurations (number of servers allocated to each tier) can satisfy the expected response-time requirement. Naturally, one would like to find the configuration that minimizes the total system cost while satisfying the total response-time requirement. This is modeled as a non-linear optimization problem using an open-queuing network model of response time, which we call the server allocation problem for tiered systems (SAPTS). In this paper we study the computational complexity of SAPTS and design efficient algorithms to solve it. For a variable number of tiers, we show that the decision version of SAPTS is NP-complete. Then we design a simple two-approximation algorithm and a fully polynomial-time approximation scheme (FPTAS). If the number of tiers is a constant, we show that SAPTS is polynomial-time solvable. Furthermore, we design a fast polynomial-time exact algorithm to solve the important two-tier case. Most of our results extend to the general case in which each tier has an arbitrary response-time function."
2007,Clustering Social Networks.,"Abstract
Social networks are ubiquitous. The discovery of close-knit clusters in these networks is of fundamental and practical interest. Existing clustering criteria are limited in that clusters typically do not overlap, all vertices are clustered and/or external sparsity is ignored. We introduce a new criterion that overcomes these limitations by combining internal density with external sparsity in a natural way. An algorithm is given for provably finding the clusters, provided there is a sufficiently large gap between internal density and external sparsity. Experiments on real social networks illustrate the effectiveness of the algorithm."
2007,Dynamic Trees in Practice.,"Abstract
Dynamic tree data structures maintain forests that change over time through edge insertions and deletions. Besides maintaining connectivity information in logarithmic time, they can support aggregation of information over paths, trees, or both. We perform an experimental comparison of several versions of dynamic trees: ST-trees, ET-trees, RC-trees, and two variants of top trees (self-adjusting and worst-case). We quantify their strengths and weaknesses through tests with various workloads, most stemming from practical applications. We observe that a simple, linear-time implementation is remarkably fast for graphs of small diameter, and that worst-case and randomized data structures are best when queries are very frequent. The best overall performance, however, is achieved by self-adjusting ST-trees."
2007,Experimental Evaluation of Parametric Max-Flow Algorithms.,"Abstract
The parametric maximum flow problem is an extension of the classical maximum flow problem in which the capacities of certain arcs are not fixed but are functions of a single parameter. Gallo et al. [6] showed that certain versions of the push-relabel algorithm for ordinary maximum flow can be extended to the parametric problem while only increasing the worst-case time bound by a constant factor. Recently Zhang et al. [14,13] proposed a novel, simple balancing algorithm for the parametric problem on bipartite networks. They claimed good performance for their algorithm on networks arising from a real-world application. We describe the results of an experimental study comparing the performance of the balancing algorithm, the GGT algorithm, and a simplified version of the GGT algorithm, on networks related to those of the application of Zhang et al. as well as networks designed to be hard for the balancing algorithm. Our implementation of the balancing algorithm beats both versions of the GGT algorithm on networks related to the application, thus supporting the observations of Zhang et al. On the other hand, the GGT algorithm is more robust; it beats the balancing algorithm on some natural networks, and by asymptotically increasing amount on networks designed to be hard for the balancing algorithm."
2006,Finding Dominators in Practice.,"The computation of dominators in a flowgraph has applications in several areas, including program optimization, circuit testing, and theoretical biology. Lengauer and Tarjan [] proposed two versions of a fast algorithm for finding dominators and compared them experimentally with an iterative bit-vector algorithm. They concluded that both versions of their algorithm were much faster even on graphs of moderate size. Recently Cooper et al. [] have proposed a new, simple, tree-based implementation of an iterative algorithm. Their experiments suggested that it was faster than the simple version of the Lengauer-Tarjan algorithm on graphs representing computer program control flows. Motivated by the work of Cooper et al., we present an experimental study comparing their algorithm (and some variants) with careful implementations of both versions of the Lengauer-Tarjan algorithm and with a new hybrid algorithm. Our results suggest that, although the performance of all the algorithms is similar, the most consistently fast are the simple Lengauer-Tarjan algorithm and the hybrid algorithm, and their advantage increases as the graph gets bigger or more complicated."
2006,Melding priority queues.,"We show that any priority queue data structure that supports insert, delete, and find-min operations in pq(n) amortized time, where n is an upper bound on the number of elements in the priority queue, can be converted into a priority queue data structure that also supports fast meld operations with essentially no increase in the amortized cost of the other operations. More specifically, the new data structure supports insert, meld and find-min operations in O(1) amortized time, and delete operations in O(pq(n) + Î±(n)) amortized time, where Î±(n) is a functional inverse of the Ackermann function, and where n this time is the total number of operations performed on all the priority queues. The construction is very simple. The meldable priority queues are obtained by placing a nonmeldable priority queues at each node of a union-find data structure. We also show that when all keys are integers in the range [1, N], we can replace n in the bound stated previously by min{n, N}.Applying this result to the nonmeldable priority queue data structures obtained recently by Thorup [2002b] and by Han and Thorup [2002] we obtain meldable RAM priority queues with O(log log n) amortized time per operation, or O(&sqrt;log log n) expected amortized time per operation, respectively. As a by-product, we obtain improved algorithms for the minimum directed spanning tree problem on graphs with integer edge weights, namely, a deterministic O(m log log n)-time algorithm and a randomized O(m&sqrt;log log n)-time algorithm. For sparse enough graphs, these bounds improve on the O(m + n log n) running time of an algorithm by Gabow et al. [1986] that works for arbitrary edge weights."
2006,Balancing Applied to Maximum Network Flow Problems.,"Abstract
We explore balancing as a definitional and algorithmic tool for finding minimum cuts and maximum flows in ordinary and parametric networks. We show that a standard monotonic parametric maximum flow problem can be formulated as a problem of computing a particular maximum flow that is balanced in an appropriate sense. We present a divide-and-conquer algorithm to compute such a balanced flow in a logarithmic number of ordinary maximum-flow computations. For the special case of a bipartite network, we present two simple, local algorithms for computing a balanced flow. The local balancing idea becomes even simpler when applied to the ordinary maximum flow problem. For this problem, we present a round-robin arc-balancing algorithm that computes a maximum flow on an n-vertex, m-arc network with integer arc capacities of at most U in O(n 2 m log(nU)) time. Although this algorithm is slower by at least a factor of n than other known algorithms, it is extremely simple and well-suited to parallel and distributed implementation."
2006,Design of data structures for mergeable trees.,"Motivated by an application in computational topology, we consider a novel variant of the problem of efficiently maintaining dynamic rooted trees. This variant allows an operation that merges two tree paths. In contrast to the standard problem, in which only one tree arc at a time changes, a single merge operation can change many arcs. In spite of this, we develop a data structure that supports merges and all other standard tree operations in O(log2 n) amortized time on an n-node forest. For the special case that occurs in the motivating application, in which arbitrary arc deletions are not allowed, we give a data structure with an O(log n) amortized time bound per operation, which is asymptotically optimal. The analysis of both algorithms is not straightforward and requires ideas not previously used in the study of dynamic trees. We explore the design space of algorithms for the problem and also consider lower bounds for it."
2006,Results and Problems on Self-adjusting Search Trees and Related Data Structures.,"Abstract
The splay tree is a form of self-adjusting search tree invented almost 25 years ago. Splay trees are remarkably efficient in both theory and practice, but many questions concerning splay trees and related data structures remain open. Foremost among these is the dynamic optimality conjecture, which states that the amortized efficiency of splay trees is optimum to within a constant factor among all kinds of binary search trees. That is, are splay trees constant-competitive? A broader question is whether there is any form of binary search tree that is constant-competitive. Recently, three different groups of researchers have devised kinds of search trees that are loglog-competitive, improving on the log-competitiveness of balanced trees. At least one of these data structures, the multisplay tree, has many if not all of the nice asymptotic properties of splay trees (even though it is more complicated than splay trees). We review this recent work and look at remaining open problems, of which there are many, including resolving the question of whether splay trees themselves are loglog-competitive."
2005,Server Allocation Algorithms for Tiered Systems.,"Abstract
Many web-based systems have a tiered application architecture, in which a request needs to transverse all the tiers before finishing its processing. One of the most important QoS metrics for these applications is the expected response time for the user. Since the expected response time in any tier depends upon the number of servers allocated to this tier, and a requestâ€™s total response time is the sum of the response times at all the tiers, many different configurations (number of servers allocated to each tier) can satisfy the expected response time requirement. Naturally, one would like to find the configuration to minimize the total system cost while satisfying the total response time requirement. This is modeled as a non-linear optimization problem using an open-queuing network model of response time, which we call the server allocation problem for tiered systems (SAPTS).
In this paper we study the computational complexity of SAPTS and design efficient algorithms to solve it. For a variable number of tiers, we show that the decision problem of SAPTS is NP-complete. Then we design a simple two-approximation algorithm and a fully polynomial time approximation scheme (FPTAS). If the number of tiers is a constant, we show that SAPTS is polynomial-time solvable. Furthermore, we design a fast polynomial-time exact algorithm to solve for the important two-tier case. Most of our results extend to the general case where each tier has an arbitrary response time function."
2005,Deadline scheduling for animation rendering.,n/a
2005,Dominator tree verification and vertex-disjoint paths.,"We present a linear-time algorithm that given a flowgraph G = (V,A,r) and a tree T, checks whether T is the dominator tree of G. Also we prove that there exist two spanning trees of G, T1 and T2, such that for any vertex <u>v</u> the paths from r to <u>v</u> in T1 and T2 intersect only at the vertices that dominate <u>v</u>. The proof is constructive and our algorithm can build the two spanning trees in linear time. Simpler versions of our two algorithms run in O(mÎ±(m, n))-time, where n is the number of vertices and m is the number of arcs in G. The existence of such two spanning trees implies that we can order the calculations of the iterative algorithm for finding dominators, proposed by Allen and Cocke [2], so that it builds the dominator tree in a single iteration."
2005,Self-adjusting top trees.,"The dynamic trees problem is that of maintaining a forest that changes over time through edge insertions and deletions. We can associate data with vertices or edges, and manipulate this data individually or in bulk, with operations that deal with whole paths or trees. Efficient solutions to this problem have numerous applications, particularly in algorithms for network flows and dynamic graphs in general. Several data structures capable of logarithmic-time dynamic tree operations have been proposed. The first was Sleator and Tarjan's ST-tree [16, 17], which represents a partition of the tree into paths. Although reasonably fast in practice, adapting ST-trees to different applications is nontrivial. Topology trees [9], top trees [3], and RC-trees [1] are based on tree contractions: they progressively combine vertices or edges to obtain a hierarchical representation of the tree. This approach is more flexible in theory, but all known implementations assume the trees have bounded degree; arbitrary trees are supported only after ternarization. We show how these two approaches can be combined (with very little overhead) to produce a data structure that is as generic as any other, very easy to adapt, and as practical as ST-trees."
2005,Value-maximizing deadline scheduling and its application to animation rendering.,"We describe a new class of utility-maximization scheduling problem with precedence constraints, the disconnected staged scheduling problem (DSSP). DSSP is a nonpreemptive multiprocessor deadline scheduling problem that arises in several commercially-important applications, including animation rendering, protein analysis, and seismic signal processing. DSSP differs from most previously-studied deadline scheduling problems because the graph of precedence constraints among tasks within jobs is disconnected, with one component per job. Another difference is that in practice we often lack accurate estimates of task execution times, and so purely offline solutions are not possible. However we do know the set of jobs and their precedence constraints up front and therefore some offline planning is possible.Our solution decomposes DSSP into an offline job selection phase followed by an online task dispatching phase. We model the former as a knapsack problem and explore several solutions to it, describe a new dispatching algorithm for the latter, and compare both with existing methods. Our theoretical results show that while DSSP is NP-hard and inapproximable in general, our two-phase scheduling method guarantees a good performance bound for many special cases. Our empirical results include an evaluation of scheduling algorithms on a real animation-rendering workload; we present a characterization of this workload in a companion paper. The workload records eight weeks of activity on a 1,000-CPU cluster used to render portions of the full-length animated feature film Shrek 2 in 2004. We show that our improved scheduling algorithms can substantially increase the aggregate value of completed jobs compared to existing practices. Our new task dispatching algorithm LCPF performs well by several metrics, including job completion times as well as the aggregate value of completed jobs."
2004,Finding Dominators in Practice.,"Abstract
The computation of dominators in a flowgraph has applications in program optimization, circuit testing, and other areas. Lengauer and Tarjan [17] proposed two versions of a fast algorithm for finding dominators and compared them experimentally with an iterative bit vector algorithm. They concluded that both versions of their algorithm were much faster than the bit-vector algorithm even on graphs of moderate size. Recently Cooper et al. [9] have proposed a new, simple, tree-based iterative algorithm. Their experiments suggested that it was faster than the simple version of the Lengauer-Tarjan algorithm on graphs representing computer program control flow. Motivated by the work of Cooper et al., we present an experimental study comparing their algorithm (and some variants) with careful implementations of both versions of the Lengauer-Tarjan algorithm and with a new hybrid algorithm. Our results suggest that, although the performance of all the algorithms is similar, the most consistently fast are the simple Lengauer-Tarjan algorithm and the hybrid algorithm, and their advantage increases as the graph gets bigger or more complicated."
2004,Finding dominators revisited: extended abstract.,"The problem of finding dominators in a flowgraph arises in many kinds of global code optimization and other settings. In 1979 Lengauer and Tarjan gave an almost-linear-time algorithm to find dominators. In 1985 Harel claimed a linear-time algorithm, but this algorithm was incomplete; Alstrup et al. [1999] gave a complete and ""simpler"" linear-time algorithm on a random-access machine. In 1998, Buchsbaum et al. claimed a ""new, simpler"" linear-time algorithm with implementations both on a random access machine and on a pointer machine. In this paper, we begin by noting that the key lemma of Buchsbaum et al. does not in fact apply to their algorithm, and their algorithm does not run in linear time. Then we provide a complete, correct, simpler linear-time dominators algorithm. One key result is a linear-time reduction of the dominators problem to a nearest common ancestors problem, implementable on either a random-access machine or a pointer machine."
2004,Melding Priority Queues.,"Abstract
We show that any priority queue data structure that supports insert, delete, and find-min operations in pq(n) time, when there are up to n elements in the priority queue, can be converted into a priority queue data structure that also supports meld operations at essentially no extra cost, at least in the amortized sense. More specifically, the new data structure supports insert, meld and find-min operations in O(1) amortized time, and delete operations in O(pq(n) Î±(n,n/pq(n))) amortized time, where Î±(m,n) is a functional inverse of the Ackermann function. For all conceivable values of pq(n), the term Î±(n,n/pq(n)) is constant. This holds, for example, if pq(n)=Î©(log* n). In such cases, adding the meld operation does not increase the amortized asymptotic cost of the priority queue operations. The result is obtained by an improved analysis of a construction suggested recently by three of the authors in [14]. The construction places a non-meldable priority queue at each node of a union-find data structure. We also show that when all keys are integers in [1,N], we can replace n in all the bounds stated above by N."
2003,Graph Clustering and Minimum Cut Trees.,"In this paper, we introduce simple graph clustering methods based on minimum cuts within the graph. The clustering methods are general enough to apply to any kind of graph but are well suited for graphs where the link structure implies a notion of reference, similarity, or endorsement, such as web and citation graphs. We show that the quality of the produced clusters is bounded by strong minimum cut and expansion criteria. We also develop a framework for hierarchical clustering and present applications to real-world data. We conclude that the clustering algorithms satisfy strong theoretical criteria and perform well in practice."
2003,Dynamic rectangular intersection with priorities.,"We present efficient data structures to maintain dynamic set of rectangles, each with priority assigned to it, such that we can efficiently find the rectangle of maximum priority containing a query point. Our data structures support insertions and deletions of rectangles. In one dimension, when rectangles are intervals, our most efficient data structure supports queries and insertions in O(log n) time, deletions in O(log n loglog n) time and requires linear space. When intervals are guaranteed to be nonoverlapping (but one can be nested within the other) we obtain a simpler data structure that supports all operations in O(log n) time."
2002,Union-find with deletions.,"In the classical union-find problem we maintain a partition of a universe of n elements into disjoint sets subject to the operations union and find. The operation union(A, B, C) replaces sets A and B in the partition by their union, given the name C. The operation find(x) returns the name of the set containing the element x. In this paper we revisit the union-find problem in a context where the underlying partitioned universe is not fixed. Specifically, we allow a delete(x) operation which removes the element x from the set containing it. We consider both worst-case performance and amortized performance. In both settings the challenge is to dynamically keep the size of the structure representing each set proportional to the number of elements in the set which may now decrease as a result of deletions.For any fixed k, we describe a data structure that supports find and delete in O(logkn) worst-case time and union in O(k) worst-case time. This matches the best possible worst-case bounds for find and union in the classical setting. Furthermore, using an incremental global rebuilding technique we obtain a reduction converting any union-find data structure to a union-find with deletions data structure. Our reduction is such that the time bounds for find and union change only by a constant factor. The time it takes to delete an element x is the same as the time it takes to find the set containing x plus the time it takes to unite a singleton set with this set.In an amortized setting a classical data structure of Tarjan supports a sequence of m finds and at most n unions on a universe of n elements in O(n + mÎ±(m + n, n, log n)) time where Î±(m, n, l) = min{k | Ak(âŒŠm/nâŒ‹) > l} and Ai(j) is Ackermann's function as described in [6]. We refine the analysis of this data structure and show that in fact the cost of each find is proportional to the size of the corresponding set. Specifically, we show that one can pay for a sequence of union and find operations by charging a constant to each participating element and O(Î±(m, n, log(l))) for a find of an element in a set of size l. We also show how keep these amortized costs for each find and each participating element while allowing deletions. The amortized cost of deleting an element from a set of l elements is the same as the amortized cost of finding the element; namely, O(Î±(m, n, log(l)))."
2002,Meldable heaps and boolean union-find.,"In the classical meldable heap data type we maintain an item-disjoint collection of heaps under the operations find-min, insert, delete, decrease-key, and meld. In the usual definition decrease-key and delete get the item and the heap containing it as parameters. We consider the modified problem where decrease-key and delete get only the item but not the heap containing it. We show that for this problem one of the operations find-min, decrease-key, or meld must take non-constant time. This is in contrast with the original data type in which data structures supporting all these three operations in constant time are known (both in an amortized and a worst-case setting).To establish our results for meldable heaps we consider a weaker version of the union-find problem that is of independent interest, which we call Boolean union-find. In the Boolean union-find problem the find operation is a binary predicate that gets an item x and a set A and answers positively if and only if &khgr; &egr; A. We prove that the lower bounds which hold for union-find in the cell probe model hold for Boolean union-find as well.We also suggest new heap data structures implementing the modified meldable heap data type that are based on redundant binary counters. Our data structures have good worst-case bounds. The best of our data structures matches the worst-case lower bounds which we establish for the problem. The simplest of our data structures is an interesting generalization of binomial queues."
2001,Unique Maximum Matching Algorithms.,"Abstract
We consider the problem of testing the uniqueness of maximum matchings, both in the unweighted and in the weighted case. For the unweighted case, we have two results. First, given a graph with n vertices and m edges, we can test whether the graph has a unique perfect matching, and find it if it exists, in O(m log4 n) time. This algorithm uses a recent dynamic connectivity algorithm and an old result of Kotzig characterizing unique perfect matchings in terms of bridges. For the special case of planar graphs, we improve the algorithm to run in O(n log n) time. Second, given one perfect matching, we can test for the existence of another in linear time. This algorithm is a modification of Edmonds' blossom-shrinking algorithm implemented using depth-first search. A generalization of Kotzig's theorem proved by Jackson and Whitty allows us to give a modification of the first algorithm that tests whether a given graph has a unique f-factor, and find it if it exists. We also show how to modify the second algorithm to check whether a given f-factor is unique. Both extensions have the same time bounds as their perfect matching counterparts. For the weighted case, we can test in linear time whether a maximum-weight matching is unique, given the output from Edmonds' algorithm for computing such a matching. The method is an extension of our algorithm for the unweighted case."
2001,Dynamic Self-Checking Techniques for Improved Tamper Resistance.,"Abstract
We describe a software self-checking mechanism designed to improve the tamper resistance of large programs. The mechanism consists of a number of testers that redundantly test for changes in the executable code as it is running and report modifications. The mechanism is built to be compatible with copy-specific static watermarking and other tamper-resistance techniques. The mechanism includes several innovations to make it stealthy and more robust."
2001,Faster kinetic heaps and their use in broadcast scheduling.,"We describe several implementations of the kinetic heap, a heap (priority queue) in which the key of each item, instead of being fixed, is a linear function of time. The kinetic heap is a simple example of a kinetic data structure of the kind considered by Basch, Guibas, and Hershberger. Kinetic heaps have many applications in computational geometry, and previous implementations were designed to address these applications. We describe an additional application, to broadcast scheduling. Each of our kinetic heap implementations improves on previous implementations by being simpler or asymptotically faster for some or all applications."
2000,Simple Confluently Persistent Catenable Lists.,"We consider the problem of maintaining persistent lists subject to concatenation and to insertions and deletions at both ends. Updates to a persistent data structure are nondestructive---each operation produces a new list incorporating the change, while keeping intact the list or lists to which it applies. Although general techniques exist for making data structures persistent, these techniques fail for structures that are subject to operations, such as catenation, that combine two or more versions. In this paper we develop a simple implementation of persistent double-ended queues (deques) with catenation that supports all deque operations in constant amortized time. Our implementation is functional if we allow memoization.
"
1999,"Tractability of Parameterized Completion Problems on Chordal, Strongly Chordal, and Proper Interval Graphs.","We study the parameterized complexity of three NP-hard graph completion problems.

The minimum fill-in problem asks if a graph can be triangulated by adding at most k edges. We develop O(ck m) and O(k2mn+f(k)) algorithms for this problem on a graph with n vertices and m edges. Here f(k) is exponential in k and the constants hidden by the big-O notation are small and do not depend on k. In particular, this implies that the problem is fixed-parameter tractable (FPT).

The proper interval graph completion problem, motivated by molecular biology, asks if a graph can be made proper interval by adding no more than k edges. We show that the problem is FPT by providing a simple search-tree-based algorithm that solves it in O(ck m)-time. Similarly, we show that the parameterized version of the strongly chordal graph completion problem is FPT by giving an O(ck m log n)-time algorithm for it.

All of our algorithms can actually enumerate all possible k-completions within the same time bounds.

"
1999,Tight Analyses of Two Local Load Balancing Algorithms.,"This paper presents an analysis of the following load balancing algorithm. At each step, each node in a network examines the number of tokens at each of its neighbors and sends a token to each neighbor with at least 2d+1 fewer tokens, where d is the maximum degree of any node in the network. We show that within $O(\Delta / \alpha)$ steps, the algorithm reduces the maximum difference in tokens between any two nodes to at most $O((d^2 \log n)/\alpha)$, where $\Delta$ is the global imbalance in tokens (i.e., the maximum difference between the number of tokens at any node initially and the average number of tokens), n is the number of nodes in the network, and $\alpha$ is the edge expansion of the network. The time bound is tight in the sense that for any graph with edge expansion $\alpha$, and for any value $\Delta$, there exists an initial distribution of tokens with imbalance $\Delta$ for which the time to reduce the imbalance to even $\Delta/2$ is at least $\Omega(\Delta/\alpha)$. The bound on the final imbalance is tight in the sense that there exists a class of networks that can be locally balanced everywhere (i.e., the maximum difference in tokens between any two neighbors is at most 2d), while the global imbalance remains $\Omega((d^2 \log n) / \alpha)$. Furthermore, we show that upon reaching a state with a global imbalance of $O((d^2 \log n)/\alpha)$, the time for this algorithm to locally balance the network can be as large as $\Omega(n^{1/2})$. We extend our analysis to a variant of this algorithm for dynamic and asynchronous networks. We also present tight bounds for a randomized algorithm in which each node sends at most one token in each step.


"
1999,A Faster and Simpler Algorithm for Sorting Signed Permutations by Reversals.,"We give a quadratic time algorithm for finding the minimum number of reversals needed to sort a signed permutation. Our algorithm is faster than the previous algorithm of Hannenhalli and Pevzner and its faster implementation by Berman and Hannenhalli. The algorithm is conceptually simple and does not require special data structures. Our study also considerably simplifies the combinatorial structures used by the analysis.
"
1999,Unique Maximum Matching Algorithms.,"We consider the problem of testing the uniqueness of maximum matchings, both in the unweighted and in the weighted case. For the unweighted case, we have two results. First, given a graph with n vertices and m edges, we can test whether the graph has a unique perfect matching, and find it if it exists, in O(mlog4n) time. This algorithm uses a recent dynamic connectivity algorithm and an old result of Kotzig characterizing unique perfect matchings in terms of bridges. For the special case of planar graphs, we improve the algorithm to run in O(nlogn) time. Second, given one perfect matching, we can test for the existence of another in linear time. This algorithm is a modification of Edmonds' blossom-shrinking algorithm implemented using depth-first search. A generalization of Kotzig's theorem proved by Jackson and Whitty allows us to give a modification of the first algorithm that tests whether a given graph has a unique f-factor, and find it if it exists. We also show how to modify the second algorithm to check whether a given f-factor is unique. Both extensions have the same time bounds as their perfect matching counterparts. For the weighted case, we can test in linear time whether a maximum-weight matching is unique, given the output from Edmonds' algorithm for computing such a matching. The method is an extension of our algorithm for the unweighted case."
1998,Culturally Induced Information Impactedness: A Prescription for Failure in Software Ventures.,"The impact of effective information flow in software ventures is analyzed through a recent case in which a hot, lucrative technology was lost on its way to the marketplace. The failure occurred despite the fact that the venture had many components crucial to success, including a proprietary intellectual property position, enormous market demand, a well-qualified, committed team, and sufficient funding. One reason for this failure is the lack of information flows among several parties critical to the success of the venture. This case suggests that in software markets that operate at breakneck pace and have short development cycles, effective information flow is a first-order priority. These blockages in information flows can stem from the nature of the cultures that are created to produce software ideas, especially proprietary technologies. The case also suggests that information can be affected by the clash between U.S. software market characteristics and Japanese business culture. Fortunately, there are inexpensive solutions that can substantially improve the return on investment, especially foreign investment, in new software technologies."
1998,Robustness and Security of Digital Watermarks.,"Abstract
Digital watermarking is a nascent but promising technology that offers protection of unencrypted digital content. This paper is a brief technical survey of the multimedia watermarking landscape. The three main technical challenges faced by watermarking algorithms are fidelity, robustness and security. Current watermarking methods offer possibly acceptable fidelity and robustness against certain types of processing, such as data compression and noise addition, but are not sufficiently robust against geometric transforms such as scaling and cropping of images. Theoretical approaches have been developed that could lead to secure watermarking methods, but substantial gaps remain between theory and practice."
1998,Culturally-Induced Information Impactedness: A Prescription for Failure in Software Ventures.,"Abstract:
The impact of effective information flow in software ventures is analyzed through a recent case in which a hot, lucrative technology was lost on its way to the marketplace. The failure occurred despite the fact that the venture had many components crucial to success, including a proprietary intellectual property position, enormous market demand, a well-qualified, committed team and sufficient funding. One reason for this failure is the lack of information flows among several parties critical to the success of the venture. This case suggests that in software markets which operate at breakneck pace and have short development cycles, effective information flow is a first order priority. These blockages in information flows can stem from the nature of the cultures that are created to produce software ideas, especially proprietary technologies. The case also suggests that information can become impacted by the clash between US software market characteristics and Japanese business culture. Fortunately, there are inexpensive solutions that can substantially improve the return on investment, especially foreign investment, in new software technologies."
1998,Simple Confluently Persistent Catenable Lists (Extended Abstract).,"Abstract
We consider the problem of maintaining persistent lists subject to concatenation and to insertions and deletions at both ends. Updates to a persistent data structure are nondestructive-each operation produces a new list incorporating the change while keeping intact the list or lists to which it applies. Although general techniques exist for making data structures persistent, these techniques fail for structures that are subject to operations, such as catenation, that combine two or more versions. In this paper we develop a simple implementation of persistent double-ended queues with catenation that supports all deque operations in constant amortized time."
1997,Optimal Parallel Verification of Minimum Spanning Trees in Logarithmic Time.,"We present the first optimal parallel algorithms for the verification and sensitivity analysis of minimum spanning trees. Our algorithms are deterministic and run inO(logn) time and require linear-work in the CREW PRAM model. These algorithms are used as a subroutine in the linear-work randomized algorithm for finding minimum spanning trees of Cole, Klein, and Tarjan.
"
1997,Toward Efficient Unstructured Multigrid Preprocessing.,"The multigrid method is a general and powerful means of accelerating the convergence of discrete iterative methods for solving partial differential equations (PDEs) and similar problems. The adaptation of the multigrid method to un structured meshes is important in solving problems with complex geometries. Such problems lie on the forefront of many scientific and engineering fields. Unfortunately, multi grid schemes on unstructured meshes require signifi cantly more preprocessing than on structured meshes. In fact, preprocessing can be a major part of the solution task and, for many applications, must be executed repeatedly. In addition, the large computational requirements of real istic PDEs, accurately discretized on unstructured meshes, make such computations candidates for parallel or distributed processing. This adds problem partitioning as a preprocessing task. We propose and examine experi mentally an automatic and unified strategy to perform several unstructured multigrid preprocessing tasks. Our strategy is based on dominating sets in the unstructured meshes. We also suggest several alternative related strategies. Our experiments evaluate the performance of two preprocessing tasks: coarse-mesh generation and domain partitioning. The experiments suggest that our preprocessing strategy produces high-quality meshes that give good multigrid performance. Our strategy also pro duces domain partitions that are reasonably load bal anced with relatively small edge cuts. Overall, we conclude that simple, integrated algorithmic strategies and data structures can make tedious preprocessing tasks more efficient and more automated—a necessary step toward the practical application of unstructured multigrid methods.
"
1997,"Dynamic trees as search trees via Euler tours, applied to the network simplex algorithm.","Abstract
Thedynamic tree is an abstract data type that allows the maintenance of a collection of trees subject to joining by adding edges (linking) and splitting by deleting edges (cutting), while at the same time allowing reporting of certain combinations of vertex or edge values. For many applications of dynamic trees, values must be combined along paths. For other applications, values must be combined over entire trees. For the latter situation, an idea used originally in parallel graph algorithms, to represent trees by Euler tours, leads to a simple implementation with a time of O(logn) per tree operation, wheren is the number of tree vertices. We apply this representation to the implementation of two versions of the network simplex algorithm, resulting in a time of O(logn) per pivot, wheren is the number of vertices in the problem network."
1997,Faster and simpler algorithm for sorting signed permutations by reversals.,We give a quadratic time algorithm for finding the minimum number of reversals needed to sort a signed permutation. Our algorithm is faster than the previous algorithm of Hannenhalli and Pevzner and its faster implementation by Berman and Hannenhalli. The algorithm is conceptually simple and does not require special data structures. Our study also considerably simplifies the combinatorial structures used by the analysis. 
1997,Faster and Simpler Algorithm for Sorting Signed Permutations by Reversals.,We give a quadratic time algorithm for finding the minimum number of reversals needed to sort a signed permutation. Our algorithm is faster than the previous algorithm of Hannenhalli and Pevzner and its faster implementation by Berman and Hannenhalli. The algorithm is conceptually simple and does not require special data structures. Our study also considerably simplifies the combinatorial structures used by the analysis. 
1996,Dominating Sets in Planar Graphs.,"Abstract
Motivated by an application to unstructured multigrid calculations, we consider the problem of asymptotically minimizing the size of dominating sets in triangulated planar graphs. Specifically, we wish to find the smallestÎµsuch that, fornsufficiently large, everyn-vertex planar graph contains a dominating set of size at mostÎµn.We prove that 1/4<Îµ<1/3, and we conjecture thatÎµ=1/4.For triangulated discs we obtain a tight bound ofÎµ=1/3.The upper bound proof yields a linear-time algorithm for finding an(n/3)-size dominating set."
1996,Parallelism in multigrid methods: How much is too much?,"Abstract
Multigrid methods are powerful techniques to accelerate the solution of computationally-intensive problems arising in a broad range of applications. Used in conjunction with iterative processes for solving partial differential equations, multigrid methods speed up iterative methods by moving the computation from the original mesh covering the problem domain through a series of coarser meshes. But this hierarchical structure leaves domain-parallel versions of the standard multigrid algorithms with a deficiency of parallelism on coarser grids. To compensate, several parallel multigrid strategies with more parallelism, but also more work, have been designed. We examine these parallel strategies and compare them to simpler standard algorithms to try to determine which techniques are more efficient and practical. We consider three parallel multigrid strategies: (1) domain-parallel versions of the standard V-cycle and F-cycle algorithms; (2) a multiple coarse grid algorithm, proposed by Fredrickson and McBryan, which generates several coarse grids for each fine grid; and (3) two Rosendale algorithm, which allow computation on all grids simultaneously. We study an elliptic model problem on simple domains, discretized with finite difference techniques on block-structured meshes in two or three dimensions with up to 106 or 109 points, respectively. We analyze performance using three models of parallel computation: the PRAM and two bridging models. The bridging models reflect the salient characteristics of two kinds of parallel computers: SIMD fine-grain computers, which contain a large number of small (bitserial) processors, and SPMD medium-grain computers, which have a more modest number of powerful (single chip) processors. Our analysis suggests that the standard algorithms are substantially more efficient than algorithms utilizing either parallel strategy. Both parallel strategies need too much extra work to compensate for their extra parallelism. They require a highly impractical number of processors to be competitive with simpler, standard algorithms. The analysis also suggests that the F-cycle, with the appropriate optimization techniques, is more efficient than the V-cycle under a broad range of problem, implementation, and machine characteristics, despite the fact that it exhibits even less parallelism than the V-cycle."
1996,Analysis of Multigrid Algorithms on Massively Parallel Computers: Architectural Implications.,"Abstract
We study the potential performance of multigrid algorithms running on massively parallel computers with the intent of discovering whether currently envisioned machines will provide an efficient platform for such algorithms. These algorithms substantially improve the performance of iterative methods of solving partial differential equations. We consider the domain parallel version of the standard V-cycle multigrid algorithm on model problems, discretized using finite difference techniques in two and three dimensions on block-structured grids of size 106and 109, respectively. We develop a set of models of parallel computation which reflect the computing characteristics of the current generation of massively parallel multicomputers. These models are based on an interconnection network of 256 to 16,384 message passing, â€œworkstation sizeâ€ processors executing in a SPMD mode. The models, based on the computing characteristics of an architectural class, provide metrics which balance abstraction with machine specificity. With the medium grain parallelism of the current generation and the high fixed cost of an interprocessor communication, our analysis suggests that an efficient implementation for practical problem sizes requires the machine to support the efficient transmission of long messages (up to 1000 words); otherwise the high initiation cost of a communication must be significantly reduced through an alternative optimization technique. The analysis also suggests that low diameter multistage networks provide little or no advantage over a simple single stage communications network. Finally, the analysis suggests that fine grain parallelism and low fixed communication costs may provide more efficiency than medium grain parallelism with low variable communications costs."
1996,Toward Efficient Unstructured Multigrid Preprocessing (Extended Abstract).,"Abstract
The multigrid method is a general and powerful means of accelerating the convergence of discrete iterative methods for solving partial differential equations (PDEs) and similar problems. The adaptation of the multigrid method to unstructured meshes is important in the solution of problems with complex geometries. Unfortunately, multigrid schemes on unstructured meshes require significantly more preprocessing than on structured meshes. In fact, preprocessing can be a major part of the solution task, and for many applications, must be done repeatedly. In addition, the large computational requirements of realistic PDEs, accurately discretized on unstructured meshes, make such computations candidates for parallel or distributed processing, adding problem partitioning as a preprocessing task.
We report on a project to apply ideas from graph theory and geometry to the solution of the preprocessing tasks required for the parallel implementation of unstructured multigrid methods. Our objective is to provide conceptually simple, efficient, and unified methods. In a previous conference paper, we proposed two bottom-up, graph-based methods and one top-down method. In this paper, we report on several sets of experiments designed to explore the practical aspects of one of the methods, based on independent dominating sets. The experiments studied the empirical properties of the mesh hierarchies generated by the method and the numerical performance of the multigrid method solving Laplace's equation using these mesh hierachies. The experiments also studied the domain partitions generated by the method. Our conclusion based on these preliminary experiments is that our simple, automatic methods provide excellent multigrid performance at low preprocessing cost."
1996,Finding Minimum Spanning Forests in Logarithmic Time and Linear Work Using Random Sampling.,"We describe a randomized CRCW PRAM algorithm that finds a minimum spanning forest of an n-vertex graph in O(log n) time and linear work. This shaves a factor of off the best previous running time for a linear-work algorithm. The novelty in our approach is to divide the computation into two phases, the first of which finds only a partial solution. This idea has been used previously in parallel connected components algorithms."
1996,Purely Functional Representations of Catenable Sorted Lists.,"The power of purely functional programming in the construction of data structures has received much attention not only because functional languages have many desirable properties, but because structures built purely functionally are automatically fully persistent: any and all versions of a structure can coexist indefinitely. Recent results illustrate the surprising power of pure functionality. One such result was the development of a representation of double-ended queues with catenation that supports all operations, including catenation, in worst-case constant time. This paper is a continuation of our study of pure functionality, especially as it relates to persistence. For our purposes, a purely functional data structure is one built only with the LISP functions car, const, cdr. We explore purely functional representations of sorted lists, implemented as finger search trees. We describe three implementations. The most efficient of these achieves logarithmic access, insertion, and deletion time, and double-logarithmic catenation time. It uses one level of structural bootstrapping to obtain its efficiency. The bounds for access, insert, and delete are the same as the best known bounds for an ephemeral implementation of these operations using finger search trees. The representations we present are the first that address the issues of persistence and pure functionality and the first for which fast implementations of catenation and split are presented. They are simple to implement and could be efficient in practice, especially for applications that require worst-case time bounds or persistence."
1995,Lazy Structure Sharing for Query Optimization.,"Abstract
We study lazy structure sharing as a tool for optimizing equivalence testing on complex data types. We investigate a number of strategies for implementing lazy structure sharing and provide upper and lower bounds on their performance (how quickly they effect ideal configurations of our data structure). In most cases when the strategies are applied to a restricted case of the problem, the bounds provide nontrivial improvements over the naÃ¯ve linear-time equivalence-testing strategy that employs no optimization. Only one strategy, however, which employs path compression, seems promising for the most general case of the problem."
1995,A Randomized Linear-Time Algorithm to Find Minimum Spanning Trees.,We present a randomized linear-time algorithm to find a minimum spanning tree in a connected graph with edge weights. The algorithm uses random sampling in combination with a recently discovered linear-time algorithm for verifying a minimum spanning tree. Our computational model is a unit-cost random-access machine with the restriction that the only operations allowed on edge weights are binary comparisons.
1995,Confluently Persistent Deques via Data-Structural Bootstrapping.,"Abstract
We introduce data-structural bootstrapping, a technique to design data structures recursively, and use it to design confluently persistent deques. Our data structure requires O(log* k) worst-case time and space per deletion, where k is the total number of deque operations, and constant worst-case time and space for other operations. Further, the data structure allows a purely functional implementation, with no side effects. This improves a previous result of Driscoll, Sleator, and Tajan (in ""Proceedings 2nd Annual ACM-SIAM Symposium on Discrete Algorithms, 1991,"" pp. 89-99)."
1995,"Data-Structural Bootstrapping, Linear Path Compression, and Catenable Heap-Ordered Double-Ended Queues.","A deque with heap order is a linear list of elements with real-valued keys that allows insertions and deletions of elements at both ends of the list. It also allows the findmin (alternatively findmax) operation, which returns the element of least (greatest) key, but it does not allow a general deletemin (deletemax) operation. Such a data structure is also called a mindeque (maxdeque). Whereas implementing heap-ordered deques in constant time per operation is a solved problem, catenating heap-ordered deques in sublogarithmic time has remained open until now.

This paper provides an efficient implementation of catenable heap-ordered deques, yielding constant amortized time per operation. The important algorithmic technique employed is an idea that we call data-structural bootstrapping; we abstract heap-ordered deques by representing them by their minimum elements, thereby reducing catenation to simple insertion, The efficiency of the resulting data structure depends upon the complexity of a special case of path compression that we prove takes linear time.


"
1995,Computing Minimal Spanning Subgraphs in Linear Time.,"Let P be a property of undirected graphs. We consider the following problem: given a graph G that has property P, find a minimal spanning subgraph of G with property P. We describe general algorithms for this problem and prove their correctness under fairly weak assumptions about P. We establish that the worst-case running time of these algorithms is $\Theta(m + n \log n)$ for 2-edge-connectivity and biconnectivity where n and m denote the number of vertices and edges, respectively, in the input graph. By refining the basic algorithms we obtain the first linear time algorithms for computing a minimal 2-edge-connected spanning subgraph and for computing a minimal biconnected spanning subgraph. We also devise general algorithms for computing a minimal spanning subgraph in directed graphs. These algorithms allow us to simplify an earlier algorithm of Gibbons, Karp, Ramachandran, Soroker, and Tarjan for computing a minimal strongly connected spanning subgraph. We also provide the first tight analysis of the latter algorithm, showing that its worst-case time complexity is $\Theta(m + n \log n)$.

"
1995,Models of parallel computation: a survey and synthesis.,"Abstract:
In the realm of sequential computing, the random access machine has successfully provided an underlying model of computation that has promoted consistency and coordination among algorithm developers, computer architects and language experts. In the realm of parallel computing, however, there has been no similar success. The need for such a unifying parallel model or set of models is heightened by the greater demand for performance and the greater diversity among machines. Yet the modeling of parallel computing still seems to be mired in controversy and chaos. This paper presents a broad range of models of parallel computation and the different roles they serve in algorithm, language and machine design. The objective is to better understand which model characteristics are important to each design community, in order to elucidate the requirements of a unifying paradigm. As an impetus for discussion, we conclude by suggesting a model of parallel computation which is consistent with a model design philosophy that balances simplicity and descriptivity with prescriptivity. We present only the survey of abstract computational models. This introduction should provide insights into the rich array of relevant issues in other disciplines.< >"
1995,Persistent lists with catenation via recursive slow-down.,n/a
1995,Tight analyses of two local load balancing algorithms.,"This paper presents an analysis of the following load balancing algorithm. At each step, each node in a network examines the number of tokens at each of its neighbors and sends a token to each neighbor with at least 2d + 1 fewer tokens, where d is the maximum degree of any node in the network. We show that within O(=) steps, the algorithm reduces the maximum dierence in tokens between any two nodes to at most O((d 2 logn)=), where is the global imbalance in tokens (i.e., the maximum dierence between the number of tokens at any node initially and the average number of tokens), n is the number of nodes in the network, and is the edge expansion of the network. The time bound is tight in the sense that for any graph with edge expansion , and for any value , there exists an initial distribution of tokens with imbalance for which the time to reduce the imbalance to even =2 is at least ›(=). The bound on the nal imbalance is tight in the sense that there exists a class of networks that can be locally balanced everywhere (i.e., the maximum dierence in tokens between any two neighbors is at most 2d), while the global imbalance remains ›((d 2 logn)=). Furthermore, we show that upon reaching a state with a global imbalance of O((d 2 logn)=), the time for this algorithm to locally balance the network can be as large as ›(n 1=2 ). We extend our analysis to a variant of this algorithm for dynamic and asynchronous networks. We also present tight bounds for a randomized algorithm in which each node sends at most one token in each step."
1994,Fully Persistent Lists with Catenation.,"This paper considers the problem of representing stacks with catenation so that any stack, old or new, is available for access or update operations. This problem arises in the implementation of list-based and functional programming languages. A solution is proposed requiring constant time and space for each stack operation except catenation, which requires O(log log k) time and space. Here k is the number of stack operations done before the catenation. All the resource bounds are amortized over the sequence of operations."
1994,A Faster Deterministic Maximum Flow Algorithm.,"Abstract
Cheriyan and Hagerup developed a randomized algorithm to compute the maximum flow in a graph with n nodes and m edges in O(mn + n2 log2n) expected time. The randomization is used to efficiently play a certain combinatorial game that arises during the computation. We give a version of their algorithm where a general version of their game arises. Then we give a strategy for the game that yields a deterministic algorithm for computing the maximum flow in a directed graph with n nodes and m edges that runs in time O(mn(logm/n log nn)). Our algorithm gives an O(mn) deterministic algorithm for all m/n = Î©(nÏµ) for any positive constant Ïµ, and is currently the fastest deterministic algorithm for computing maximum flow as long as m/n = Ï‰(log n)."
1994,Unique Binary-Search-Tree Representations and Equality Testing of Sets and Sequences.,"This paper studies the problem of representing sets over an ordered universe by unique binary search trees, so that dictionary operations can be performed efficiently on any set. Although efficient randomized solutions to the problem are known, its deterministic complexity has been open. The paper exhibits representations that permit the execution of dictionary operations in optimal deterministic time when the dictionary is sufficiently sparse or sufficiently dense. The results demonstrate an exponential separation between the deterministic and randomized complexities of the problem.

Unique representations are applied to obtain efficient data structures for maintaining a dynamic collection of sets/sequences under queries that test the equality of a pair of objects. The data structure for set equality testing tests equality of sets in constant time and processes set updates in $O(\log m)$ amortized time and $O(\log m)$ space, where m denotes the total number of updates performed. It is based on an efficient implementation of cascades of CONS operations on uniquely stored S-expressions. The data structure for sequence equality testing tests equality of sequences in constant time and processes updates in $O(\sqrt {n\log m} = \log m)$ amortized time and $O(\sqrt n )$ amortized space where n denotes the length of the sequence that is updated and m denotes the total number of updates performed.
"
1994,Dynamic Perfect Hashing: Upper and Lower Bounds.,"The dynamic dictionary problem is considered: provide an algorithm for storing a dynamic set, allowing the operations insert, delete, and lookup. A dynamic perfect hashing strategy is given: a randomized algorithm for the dynamic dictionary problem that takes $O(1)$ worst-case time for lookups and $O(1)$ amortized expected time for insertions and deletions; it uses space proportional to the size of the set stored. Furthermore, lower bounds for the time complexity of a class of deterministic algorithms for the dictionary problem are proved. This class encompasses realistic hashing-based schemes that use linear space. Such algorithms have amortized worst-case time complexity $\Omega (\log n)$ for a sequence of n insertions and lookups; if the worst-case lookup time is restricted to k, then the lower bound becomes $\Omega (k \cdot n^{{1 / k}} )$.
"
1994,Improved Algorithms for Bipartite Network Flow.,"In this paper, network flow algorithms for bipartite networks are studied. A network $G = (V,E)$ is called bipartite if its vertex set V can be partitioned into two subsets $V_1 $ and $V_2 $ such that all edges have one endpoint in $V_1 $ and the other in $V_2 $. Let $n = |V|$, $n_1 = |V_1 |$ , $n_2 = |V_2 |$, $m = |E|$ and assume without loss of generality that $n_1 \leqslant n_2 $. A bipartite network is called unbalanced if $n_1 \ll n_2 $ and balanced otherwise. (This notion is necessarily imprecise.) It is shown that several maximum flow algorithms can be substantially sped up when applied to unbalanced networks. The basic idea in these improvements is a two-edge push rule that allows one to “charge” most computation to vertices in $V_1 $, and hence develop algorithms whose running times depend on $n_1 $ rather than n. For example, it is shown that the two-edge push version of Goldberg and Tarjan’s FIFO preflow-push algorithm runs in $O(n_1 m + n_1^3 )$ time and that the analogous version of Ahuja and Orlin’s excess scaling algorithm runs in $O(n_1 m + n_1^2 \log U)$ time, where U is the largest edge capacity. These ideas are also extended to dynamic tree implementations, parametric maximum flows, and minimum-cost flows.
"
1994,Optimal Parallel Verification of Minimum Spanning Trees in Logarithmic Time.,"Abstract
We have presented algorithms for verification and sensitivity analysis of minimum spanning trees. Both algorithms are optimal; however, the exact bound on the number of processors for deterministic sensitivity analysis is not known. This leaves the obvious open question from the Dixon, Rauch and Tarjan paper: â€œWhat is the best bound on the number of comparisons needed to perform sensitivity analysis?â€ The algorithms given here are for the CREW PRAM model. Can these algorithms be transformed into an EREW algorithm?
A very interesting question now is whether the new result of Klein and Tarjan"
1994,Tractability of parameterized completion problems on chordal and interval graphs: Minimum Fill-in and Physical Mapping.,"Abstract:
We study the parameterized complexity of several NP-Hard graph completion problems: The minimum fill-in problem is to decide if a graph can be triangulated by adding at most k edges. We develop an O(k/sup 5/ mn+f(K)) algorithm for the problem on a graph with n vertices and m edges. In particular, this implies that the problem is fixed parameter tractable (FPT). proper interval graph completion problems, motivated by molecular biology, ask for adding edges in order to obtain a proper interval graph, so that a parameter in that graph does not exceed k. We show that the problem is FPT when k is the number of added edges. For the problem where k is the clique size, we give an O(f(k)n/sup k-1/) algorithm, so it is polynomial for fixed k. On the other hand, we prove its hardness in the parameterized hierarchy, so it is probably not FPT. Those results are obtained even when a set of edges which should not be added is given. That set can be given either explicitly or by a proper vertex coloring which the added edges should respect.< >"
1994,Unstructured Multigrid Strategies on Massively Parallel Computers: A Case for Integrated Design.,"Multigrid techniques have been used successfully in practice to speed up the convergence of computationally intensive, PDE iterative solution schemes. Instead of iterating to termination accuracy on a fine grid, multigrid algorithms move computation among a hierarchy of grids. Adapting structured multigrid techniques to unstructured mesh hierarchies requires a substantial increase in preprocessing tasks such as mesh generation, discretization, and the construction of inter-mesh transfer operators. In addition, the current generation of medium-grained parallel supercomputers requires a set of good domain partitions for efficient parallel execution. We present three practical strategies to efficiently accomplish these preprocessing tasks. These strategies were designed to address large problem sizes by using fast, simple heuristics. We present analytical and experimental work demonstrating the viability of these strategies. Issues and directions are presented for future work toward the goal of efficient implementing 3D unstructured multigrid algorithms on the current generation of supercomputers."
1994,A randomized linear-time algorithm for finding minimum spanning trees.,We present a randomized linear-time algorithm for finding a minimum spanning tree in a connected graph with edge weights. The algorithm is a modification of one proposed by Karger and uses random sampling in combination with a recently discovered linear-time algorithm for verifying a minimum spanning tree. Our computational model is a unit-cost random-access machine with the restriction that the only operations allowed on edge weights are binary comparisons. 
1993,Finding the Minimum-Cost Maximum Flow in a Series-Parallel Network.,"Abstract
We give a fast algorithm for computing a minimum-cost maximum flow in a series-parallel network. On an m-edge network, the algorithm runs in O(m log m) time. The space needed is O(m) if only the cost of the minimum-cost flow is desired, or O(m log m) if the entire flow is needed. This space bound can be reduced to O(m log* m) without increasing the running time. The idea behind the algorithm is to represent a set of augmenting paths by a balanced search tree."
1993,An O(m log n)-Time Algorithm for the Maximal Planar Subgraph Problem.,"Based on a new version of the Hopcroft and Tarjan planarity testing algorithm, this paper develops an $O(m\log n)$-time algorithm to find a maximal planar subgraph.
"
1993,Confluently Persistent Deques via Data Structural Bootstrapping.,"We introduce data-structural bootstrapping, a technique to design data structures recursively, and use it to design confluently persistent deques. Our data structure requires O(log* k) worst-case time and space per deletion, where k is the total number of deque operations, and constant worst-case time and space for other operations. Further, the data structure allows a purely functional implementation, with no side effects. This improves a previous result of Driscoll, Sleator, and Tajan (in ""Proceedings 2nd Annual ACM-SIAM Symposium on Discrete Algorithms, 1991,"" pp. 89-99)."
1992,Maintaining Bridge-Connected and Biconnected Components On-Line.,"We consider the twin problems of maintaining the bridge-connected components and the biconnected components of a dynamic undirected graph. The allowed changes to the graph are vertex and edge insertions. We give an algorithm for each problem. With simple data structures, each algorithm runs inO(n logn +m) time, wheren is the number of vertices andm is the number of operations. We develop a modified version of the dynamic trees of Sleator and Tarjan that is suitable for efficient recursive algorithms, and use it to reduce the running time of the algorithms for both problems toO(m?(m,n)), where ? is a functional inverse of Ackermann's function. This time bound is optimal. All of the algorithms useO(n) space.
"
1992,A Linear-Time Algorithm for Finding an Ambitus.,"Abstract
We devise a linear-time algorithm for finding an ambitus Ã­n an undirected graph. An ambitus is a cycle in a graph containing two distinguished vertices such that certain different groups of bridges (calledB itp-,B itQ-, andB itPQ-bridges) satisfy the property that a bridge in one group does not interlace with any bridge in the other groups. Thus, an ambitus allows the graph to be cut into pieces, where, in each piece, certain graph properties may be investigated independently and recursively, and then the pieces can be pasted together to yield information about these graph properties in the original graph. In order to achieve a good time-complexity for such an algorithm employing the divide-and-conquer paradigm, it is necessary to find an ambitus quickly. We also show that, using ambitus, linear-time algorithms can be devised for abiding-path-finding and nonseparating-induced-cycle-finding problems."
1992,Polygon Triangulation in O (n log log n) Time with Simple Data Structures.,"Abstract
We give a newO(n log logn)-time deterministic algorithm for triangulating simplen-vertex polygons, which avoids the use of complicated data structures. In addition, for polygons whose vertices have integer coordinates of polynomially bounded size, the algorithm can be modified to run inO(n log*n) time. The major new techniques employed are the efficient location of horizontal visibility edges that partition the interior of the polygon into regions of approximately equal size, and a linear-time algorithm for obtaining the horizontal visibility partition of a subchain of a polygonal chain, from the horizontal visibility partition of the entire chain. The latter technique has other interesting applications, including a linear-time algorithm to convert a Steiner triangulation of a polygon into a true triangulation."
1992,Randomized parallel algorithms for trapezoidal diagrams.,"We describe randomized parallel algorithms for building trapezoidal diagrams of line segments in the plane. The algorithms are designed for a CRCW PRAM. For general segments, we give an algorithm requiring optimal O(A+n log n) expected work and optimal O(log n) time, where A is the number of intersecting pairs of segments. If the segments form a simple chain, we give an algorithm requiring optimal O(n) expected work and O(log n log log n log* n) expected time, and a simpler algorithm requiring O(n log* n) expected work. The serial algorithm corresponding to the latter is among the simplest known algorithms requiring O(n log* n) expected operations. For a set of segments forming K chains, we give an algorithm requiring O(A+n log* n+K log n) expected work and O(log n log log n log* n) expected time. The parallel time bounds require the assumption that enough processors are available, with processor allocations every log n steps.
"
1992,Maintenance of a Minimum Spanning Forest in a Dynamic Plane Graph.,"Abstract
We give an efficient algorithm for maintaining a minimum spanning forest of a plane graph subject to on-line modifications. The modifications supported include changes in the edge weights and insertion and deletion of edges and vertices which are consistent with the given embedding. To implement the algorithms, we develop a data structure called an edge-ordered dynamic tree, which is a variant of the dynamic tree data structure of Sleator and Tarjan. Using this data structure, our algorithm runs in O(log n) time per operation and O(n) space. The algorithm can be used to maintain the connected components of a dynamic planar graph in O(logn) time per operation. We also show that any algorithm will need
amortized time per operation, given a set of machine operations that is fairly general."
1992,Finding minimum-cost flows by double scaling.,"Several researchers have recently developed new techniques that give fast algorithms for the minimum-cost flow problem. In this paper we combine several of these techniques to yield an algorithm running in O(nm(log logU) log(nC)) time on networks withn vertices,m edges, maximum arc capacityU, and maximum arc cost magnitudeC. The major techniques used are the capacity-scaling approach of Edmonds and Karp, the excess-scaling approach of Ahuja and Orlin, the cost-scaling approach of Goldberg and Tarjan, and the dynamic tree data structure of Sleator and Tarjan. For nonsparse graphs with large maximum arc capacity, we obtain a similar but slightly better bound. We also obtain a slightly better bound for the (noncapacitated) transportation problem. In addition, we discuss a capacity-bounding approach to the minimum-cost flow problem.
"
1992,Verification and Sensitivity Analysis of Minimum Spanning Trees in Linear Time.,"Komlós has devised a way to use a linear number of binary comparisons to test whether a given spanning tree of a graph with edge costs is a minimum spanning tree. The total computational work required by his method is much larger than linear, however. This paper describes a linear-time algorithm for verifying a minimum spanning tree. This algorithm combines the result of Komlós with a preprocessing and table look-up method for small subproblems and with a previously known almost-linear-time algorithm. Additionally, an optimal deterministic algorithm and a linear-time randomized algorithm for sensitivity analysis of minimum spanning trees are presented.
"
1992,Short Encodings of Evolving Structures.,"A derivation in a transformational system such as a graph grammar may be redundant in the sense that the exact order of the transformations may not affect the final outcome; all that matters is that each transformation, when applied, is applied to the correct substructure. By taking advantage of this redundancy, we can develop an efficient encoding scheme for such derivations. This encoding scheme has a number of diverse applications. It can be used in efficient enumeration of combinatorial objects or for compact representation of program and data structure transformations. It can also be used to derive lower bounds on lengths of derivations. It is shown, for example, that $\Omega ( n \log n )$ applications of the associative and commutative laws are required in the worst case to transform an n-variable expression over a binary associative, commutative operation into some other equivalent expression. Similarly, it is shown that $\Omega ( n\log n )$ “diagonal flips” are required in the worst case to transform one n-vertex numbered triangulated planar graph into some other one. Both of these lower bounds have matching upper bounds. An $O( n\log n )$ upper bound for associative, commutative operations was known previously, whereas here an $O( n\log n )$ upper bound for diagonal flips is obtained.

"
1992,More Efficient Bottom-Up Multi-Pattern Matching in Trees.,"Abstract
Pattern matching in trees is fundamental to a variety of programming language systems. However, progress has been slow in satisfying a pressing need for general-purpose pattern-matching algorithms that are efficient in both time and space. We offer asymptotic improvements in both time and space to Chase's bottom-up algorithm for pattern preprocessing. A preliminary implementation of our algorithm runs ten times faster than Chase's (1987) implementation on the hardest problem instances. Our preprocessing algorithm has the advantage of being on-line with respect to pattern additions and deletions. It also adapts to favorable input instances, and on Hoffmann and O'Donnell's (1982) class of simple patterns, it performs better than their special-purpose algorithm tailored to this class. We show how to modify our algorithm using a new decomposition method to obtain a space/time tradeoff. Finally, we trade a log factor in time for a linear space bottom-up pattern-matching algorithm that handles a wide subclass of Hoffmann and O'Donnell's (1982) simple patterns."
1992,"Data Structural Bootstrapping, Linear Path Compression, and Catenable Heap Ordered Double Ended Queues.","Abstract:
The authors provide an efficient implementation of catenable mindeques. To prove that the resulting data structure achieves constant amortized time per operation, they consider order preserving path compression. They prove a linear bound on deque ordered spine-only path compression, a case of order persevering path compression employed by the data structure.< >"
1992,Computing Minimal Spanning Subgraphs in Linear Time.,"Let P be a property of undirected graphs. We consider the following problem: given a graph G that has property P, find a minimal spanning subgraph of G with property P. We describe two related algorithms for this problem and prove their correctness under some rather weak assumptions about P. We devise a general technique for analyzing the worst-case behavior of these algorithms. By applying the technique to 2-edge-connectivity and biconnectivity, we obtain an &OHgr;(m + n log n) lower bound on the worst-case running time of the algorithms for these two properties, thus settling open questions posed earlier with regard to these properties. We then describe refinements of the basic algorithms that yield the first linear-time algorithms for finding a minimal 2-edge-connected spanning subgraph and a minimal biconnected spanning subgraph of a graph."
1992,A Faster Deterministic Maximum Flow Algorithm.,"We describe a deterministic version of a 1990 Cheriyan, Hagerup, and Mehlhorn randomized algorithm for computing the maximum flow on a directed graph with n nodes and m edges which runs in time O(mn + n2+&egr;, for any constant &egr;. This improves upon Alon's 1989 bound of O(mn + n8/3log n) [A] and gives an O(mn) deterministic algorithm for all m > n1+&egr;. Thus it extends the range of m/n for which an O(mn) algorithm is known, and matches the 1988 algorithm of Goldberg and Tarjan [GT] for smaller values of m/n."
1991,Faster Scaling Algorithms for General Graph-Matching Problems.,"An algorithm for minimum-cost matching on a general graph with integral edge costs is presented. The algorithm runs in time close to the fastest known bound for maximum-cardinality matching. Specifically, let n, m and N denote the number of vertices, number of edges, and largest magnitude of a cost, respectively. The best known time bound for maximum-cardinality matching is. The new algorithm for minimum-cost matching has time bound . A slight modification of the new algorithm finds a maximum-cardinality matching in time. Other applications of the new algorithm are given, including an efficient implementation of Christofides' travelling salesman approximation algorithm and efficient solutions to update problems that require the linear programming duals for matching."
1991,Transitive Compaction in Parallel via Branchings.,"Abstract
We study the following problem: given a strongly connected digraph, find a minimal strongly connected spanning subgraph of it. Our main result is a parallel algorithm for this problem, which runs in polylog parallel time and uses O(n3) processors on a PRAM. Our algorithm is simple and the major tool it uses is computing a minimum-weight branching with zero-one weights. We also present sequential algorithms for the problem that run in time O(m + n Â· log n)."
1991,Efficiency of the Primal Network Simplex Algorithm for the Minimum-Cost Circulation Problem.,"We study the number of pivots required by the primal network simplex algorithm to solve the minimum-cost circulation problem. We propose a pivot selection rule with a bound of n(logn)/2+O(1) on the number of pivots, for an n-vertex network. This is the first known subexponential bound. The network simplex algorithm with this rule can be implemented to run in n(logn)/2+O(1) time. In the special case of planar graphs, we obtain a polynomial bound on the number of pivots and the running time. We also consider the relaxation of the network simplex algorithm in which cost-increasing pivots are allowed as well as cost-decreasing ones. For this algorithm we propose a pivot selection rule with a bound of

O(nm · min{log(nC), m log n})

on the number of pivots, for a network with n vertices, m arcs, and integer arc costs bounded in magnitude by C. The total running time is

O(nm log n · min{(log nC), m log n}).

This bound is within a logarithmic factor of those of the best previously known algorithms for the minimum-cost circulation problem."
1991,Use of dynamic trees in a network simplex algorithm for the maximum flow problem.,"Abstract
Goldfarb and Hao (1990) have proposed a pivot rule for the primal network simplex algorithm that will solve a maximum flow problem on ann-vertex,m-arc network in at mostnm pivots and O(n2m) time. In this paper we describe how to extend the dynamic tree data structure of Sleator and Tarjan (1983, 1985) to reduce the running time of this algorithm to O(nm logn). This bound is less than a logarithmic factor larger than those of the fastest known algorithms for the problem. Our extension of dynamic trees is interesting in its own right and may well have additional applications."
1991,Faster parametric shortest path and minimum-balance algorithms.,"We use Fibonacci heaps to improve a parametric shortest path algorithm of Karp and Orlin, and we combine our algorithm and the method of Schneider and Schneider's minimum?balance algorithm to obtain a faster minimum?balance algorithm.

For a graph with n vertices and m edges, our parametric shortest path algorithm and our minimum?balance algorithm both run in O(nm + n2 log n) time, improved from O(nm log n) for the parametric shortest path algorithm of Karp and Orlin and O(n2m) for the minimum?balance algorithm of Schneider and Schneider.

An important application of the parametric shortest path algorithm is in finding a minimum mean cycle. Experiments on random graphs suggest that the expected time for finding a minimum mean cycle with our algorithm is O(n log n + m)."
1991,Randomized Parallel Algorithms for Trapezoidal Diagrams.,"We describe randomized parallel CREW PRAM algorithms for building trapezoidal diagrams of line segments in the plane. For general segments, we give an algorithm requiring optimal O(A + n log n) expected work and optimal O(log n) time, where A is the number of intersecting pairs of segments. If the segments form a simple chain, we give an algorithm requiring optimal O(n) expected work and O(log n log log n log* n) expected time, and a simpler algorithm requiring O(n log* n) expected work. The serial algorithm corresponding to the latter is the simplest known algorithm requiring O(n log* n) expected operations. For a set of segments forming K chains, we give an algorithm requiring O(A + n log” n -IK log n) expected work and O(log n log log n log” n) expected time. The parallel time bounds require the assumption that enough processors are available, with processor allocations every log n steps."
1991,Fully Persistent Lists with Catenation.,"This paper considers the problem of representing stacks with catenation so that any stack, old or new, is available for access or update operations. This problem arises in the implementation of list-based and functional programming languages. A solution is proposed requiring constant time and space for each stack operation except catenation, which requires O(log log k) time and space. Here k is the number of stack operations done before the catenation. All the resource bounds are amortized over the sequence of operations."
1990,Simplified Linear-Time Jordan Sorting and Polygon Clipping.,"Abstract
Given the intersection points of a Jordan curve with the x-axis in the order in which they occur along the curve, the Jordan sorting problem is to sort them into the order in which they occur along the x-axis. This problem arises in clipping a simple polygon against a rectangle (a â€œwindowâ€) and in efficient algorithms for triangulating a simple polygon. Hoffman, Mehlhorn, Rosenstiehl, and Tarjan proposed an algorithm that solves the Jordan sorting problem in time that is linear in the number of intersection points, but their algorithm requires the use of a sophisticated data structure, the level-linked search tree. We propose a variant of the algorithm of Hoffman et al. that retains the linear-time bound but simplifies both the primary data structure and the operations it must perform."
1990,Faster Algorithms for the Shortest Path Problem.,"Efficient implementations of Dijkstra's shortest path algorithm are investigated. A new data structure, called the radix heap, is proposed for use in this algorithm. On a network with n vertices, m edges, and nonnegative integer arc costs bounded by C, a one-level form of radix heap gives a time bound for Dijkstra's algorithm of O(m + n log C). A two-level form of radix heap gives a bound of O(m + n log C/log log C). A combination of a radix heap and a previously known data structure called a Fibonacci heap gives a bound of O(m + na @@@@log C). The best previously known bounds are O(m + n log n) using Fibonacci heaps alone and O(m log log C) using the priority queue structure of Van Emde Boas et al. [ 17]."
1990,Finding Minimum-Cost Circulations by Successive Approximation.,"We develop a new approach to solving minimum-cost circulation problems. Our approach combines methods for solving the maximum flow problem with successive approximation techniques based on cost scaling. We measure the accuracy of a solution by the amount that the complementary slackness conditions are violated.

We propose a simple minimum-cost circulation algorithm, one version of which runs in O(n3log(nC)) time on an n-vertex network with integer arc costs of absolute value at most C. By incorporating sophisticated data structures into the algorithm, we obtain a time bound of O(nm log(n2/m)log(nC)) on a network with m arcs. A slightly different use of our approach shows that a minimum-cost circulation can be computed by solving a sequence of O(n log(nC)) blocking flow problems. A corollary of this result is an O(n2(log n)log(nC))-time, m-processor parallel minimum-cost circulation algorithm. Our approach also yields strongly polynomial minimum-cost circulation algorithms.

Our results provide evidence that the minimum-cost circulation problem is not much harder than the maximum flow problem. We believe that a suitable implementation of our method will perform extremely well in practice."
1990,More Efficient Bottom-Up Tree Pattern Matching.,"Abstract
Pattern matching in trees is fundamental to a variety of programming language systems. However, progress has been slow in satisfying a pressing need for general purpose pattern matching algorithms that are efficient in both time and space. We offer asymptotic improvements in both time and space to Chase's bottom-up algorithm for pattern preprocessing. Our preprocessing algorithm has the additional advantage of being incremental with respect to pattern additions and deletions. We show how to modify our algorithm using a new decomposition method to obtain a space/time tradeoff. Finally, we trade a log factor in time for a linear space bottom-up pattern matching algorithm that handles a wide subclass of Hoffmann and O'Donnell's Simple Patterns."
1990,Polygon Triangulation in O(n log log n) Time with Simple Data-Structures.,"We give a new &Ogr;(n log log n)-time deterministic linear-time algorithm for triangulating simple n-vertex polygons, which avoids the use of complicated data-structures. In addition, for polygons whose vertices have integer coordinates of polynomially bounded size, the algorithm can be modified to run in &Ogr;(n log* n) time. The major new techniques employed are the efficient location of horizontal visibility edges which partition the interior of the polygon into regions of approximately equal size, and a linear-time algorithm for obtaining the horizontal visibility partition of a subchain of a polygonal chain, from the horizontal visibility partition of the entire chain. This latter technique has other interesting applications, including a linear-time algorithm to convert a Steiner triangulation of a polygon into a true triangulation. This research was partially supported by DIMACS and the following grants: NSERC 583584, NSERC 580485, NSF-STC88-09648, ONR-N00014-87-0467."
1990,Maintenance of a Minimum Spanning Forest in a Dynamic Planar Graph.,"We give efficient algorithms for maintaining a minimum spanning forest of a planar graph subject to on-line modifications. The modifications supported include changes in the edge weights, and insertion and deletion of edges and vertices. To implement the algorithms, we develop a data structure called an edge-ordered dynamic tree, which is a variant of the dynamic tree data structure of Sleator and Tarjan. Using this data structure, our algorithms run in O(log n) time per operation and O(n) space. The algorithms can be used to maintain the connected components of a dynamic planar graph in O(log n) time per operation."
1990,Unique Binary Search Tree Representations and Equality-testing of Sets and Sequences.,"Given an ordered universe U, we study the problem of representing each subset of U by a unique binary search tree so that dictionary operations can be performed efficiently. We exhibit representations that permit the execution of dictionary operations in optimal time when the dictionary is sufficiently sparse or sufficiently dense. We apply unique representations to obtain efficient data structures for maintaining a collection of sets/sequences under queries that test the equality of a pair of objects. In the process, we devise an interesting method for maintaining a dynamic, sparse array."
1989,A Fast Las Vegas Algorithm for Triangulating a Simple Polygon.,"We present a randomized algorithm that triangulates a simple polygon onn vertices inO(n log*n) expected time. The averaging in the analysis of running time is over the possible choices made by the algorithm; the bound holds for any input polygon.
"
1989,A Tight Amortized Bound for Path Reversal.,"Abstract
Path reversal is a form of path compression used in a disjoint set union algorithm and a mutual exclusion algorithm. We derive a tight upper bound on the amortized cost of path reversal."
1989,A Parallel Algorithm for Finding a Blocking Flow in an Acyclic Network.,"Abstract
We propose a simple parallel algorithm for finding a blocking flow in an acyclic network. On an n-vertex, m-arc network, our algorithm runs in O(n log n) time and O(nm) space using an m-processor EREW PRAM. A consequence of our algorithm is an O(n2(log n)log(nC)) time, O(nm) space, m-processor algorithm for the minimum-cost circulation problem, on a network with integer arc capacities of magnitude at most C."
1989,Finding minimum-cost circulations by canceling negative cycles.,"A classical algorithm for finding a minimum-cost circulation consists of repeatedly finding a residual cycle of negative cost and canceling it by pushing enough flow around the cycle to saturate an arc. We show that a judicious choice of cycles for canceling leads to a polynomial bound on the number of iterations in this algorithm. This gives a very simple strongly polynomial algorithm that uses no scaling. A variant of the algorithm that uses dynamic trees runs in &Ogr;(nm(log n)min{log(nC), m log n}) time on a network of n vertices, m arcs, and arc costs of maximum absolute value C. This bound is comparable to those of the fastest previously known algorithms."
1989,Making Data Structures Persistent.,"Abstract
This paper is a study of persistence in data structures. Ordinary data structures are ephemeral in the sense that a change to the structure destroys the old version, leaving only the new version available for use. In contrast, a persistent structure allows access to any version, old or new, at any time. We develop simple, systematic, and efficient techniques for making linked data structures persistent. We use our techniques to devise persistent forms of binary search trees with logarithmic access, insertion, and deletion times and O(1) space bounds for insertion and deletion."
1989,Amortized Analysis of Algorithms for Set Union with Backtracking.,"Mannila and Ukkonen [Lecture Notes in Computer Science 225, Springer-Verlag, New York, 1986, pp. 236–243] have studied a variant of the classical disjoint set union (equivalence) problem in which an extra operation, called de-union, can undo the most recently performed union operation not yet undone. They proposed a way to modify standard set union algorithms to handle de-union operations. In this paper several algorithms are analyzed based on their approach. The most efficient such algorithms have an amortized running time of $O({{\log n} / {\log \log n}})$ per operation, where n is the total number of elements in all the sets. These algorithms use $O(n\log n)$ space, but the space usage can be reduced to $O(n)$ by a simple change. The authors prove that any separable pointer-based algorithm for the problem requires $\Omega ({{\log n} / {\log \log n}})$ time per operation, thus showing that our upper bound on amortized time is tight.




"
1989,A Fast Parametric Maximum Flow Algorithm and Applications.,"The classical maximum flow problem sometimes occurs in settings in which the arc capacities are not fixed but are functions of a single parameter, and the goal is to find the value of the parameter such that the corresponding maximum flow or minimum cut satisfies some side condition. Finding the desired parameter value requires solving a sequence of related maximum flow problems. In this paper it is shown that the recent maximum flow algorithm of Goldberg and Tarjan can be extended to solve an important class of such parametric maximum flow problems, at the cost of only a constant factor in its worst-case time bound. Faster algorithms for a variety of combinatorial optimization problems follow from the result.
"
1989,Improved Time Bounds for the Maximum Flow Problem.,"Recently, Goldberg proposed a new approach to the maximum network flow problem. The approach yields a very simple algorithm running in $O(n^3 )$ time on n-vertex networks. Incorporation of the dynamic tree data structure of Sleator and Tarjan yields a more complicated algorithm with a running time of $O(nm\log (n^2 /m)$ on m-arc networks. Ahuja and Orlin developed a variant of Goldberg’s algorithm that uses scaling and runs in $O(nm + (n^2 \log U)$ time on networks with integer arc capacities bounded by U. In this paper possible improvements to the Ahuja-Orlin algorithm are explored. First, an improved running time of $O(nm + n^2 \log U/\log \log U)$ is obtained by using a nonconstant scaling factor. Second, an even better bound of $O(nm + n^2 (\log U)^{1/2} )$ is obtained by combining the Ahuja-Orlin algorithm with the wave algorithm of Tarjan. Third, it is shown that the use of dynamic trees in the latter algorithm reduces the running time to $O(nm\log (({n / m})(\log U)^{{1 / 2}} + 2))$. This result shows that the combined use of three different techniques, results in speed not obtained by using any of the techniques alone. The above bounds are all for a unit-cost random access machine. Also considered is a semilogarithmic computation model in which the bounds increase by an additive term of $O(m\log _n U)$, which is the time needed to read the input in the model.
"
1989,Faster Scaling Algorithms for Network Problems.,"This paper presents algorithms for the assignment problem, the transportation problem, and the minimum-cost flow problem of operations research. The algorithms find a minimum-cost solution, yet run in time close to the best-known bounds for the corresponding problems without costs. For example, the assignment problem (equivalently, minimum-cost matching in a bipartite graph) can be solved in $O(\sqrt {nm} \log (nN))$ time, where $n,m$, and N denote the number of vertices, number of edges, and largest magnitude of a cost; costs are assumed to be integral. The algorithms work by scaling. As in the work of Goldberg and Tarjan, in each scaled problem an approximate optimum solution is found, rather than an exact optimum.




"
1988,Relaxed Heaps: An Alternative to Fibonacci Heaps with Applications to Parallel Computation.,"The relaxed heap is a priority queue data structure that achieves the same amortized time bounds as the Fibonacci heapâ€”a sequence of m decrease_key and n delete_min operations takes time O(m + n log n). A variant of relaxed heaps achieves similar bounds in the worst caseâ€”O(1) time for decrease_key and O(log n) for delete_min. Relaxed heaps give a processor-efficient parallel implementation of Dijkstra's shortest path algorithm, and hence other algorithms in network optimization. A relaxed heap is a type of binomial queue that allows heap order to be violated."
1988,A Linear-Time Algorithm for Finding a Minimum Spanning Pseudoforest.,"Abstract
A pseudoforest is a graph each of whose connected components is a tree or a tree plus an edge; a spanning pseudoforest of a graph contains the greatest number of edges possible. This paper shows that a minimum cost spanning pseudoforest of a graph with n vertices and m edges can be found in O(m+n) time. This implies that a minimum spanning tree can be found in O(m) time for graphs with girth at least log(i)n for some constant i."
1988,A new approach to the maximum-flow problem.,"All previously known efficient maximum-flow algorithms work by finding augmenting paths, either one path at a time (as in the original Ford and Fulkerson algorithm) or all shortest-length augmenting paths at once (using the layered network approach of Dinic). An alternative method based on the preflow concept of Karzanov is introduced. A preflow is like a flow, except that the total amount flowing into a vertex is allowed to exceed the total amount flowing out. The method maintains a preflow in the original network and pushes local flow excess toward the sink along what are estimated to be shortest paths. The algorithm and its analysis are simple and intuitive, yet the algorithm runs as fast as any other known method on dense graphs, achieving an O(n3) time bound on an n-vertex graph. By incorporating the dynamic tree data structure of Sleator and Tarjan, we obtain a version of the algorithm running in O(nm log(n2/m)) time on an n-vertex, m-edge graph. This is as fast as any known method for any graph density and faster on graphs of moderate density. The algorithm also admits efficient distributed and parallel implementations. A parallel implementation running in O(n2log n) time using n processors and O(m) space is obtained. This time bound matches that of the Shiloach-Vishkin algorithm, which also uses n processors but requires O(n2) space."
1988,Algorithms for Two Bottleneck Optimization Problems.,"Abstract
A bottleneck optimization problem on a graph with edge costs is the problem of finding a subgraph of a certain kind that minimizes the maximum edge cost in the subgraph. The bottleneck objective contrasts with the more common objective of minimizing the sum of edge costs. We propose fast algorithms for two bottleneck optimization problems. For the problem of finding a bottleneck spanning tree in a directed graph of n vertices and m edges, we propose an
algorithm. For the bottleneck maximum cardinality matching problem, we propose an
algorithm."
1988,One-Processor Scheduling with Symmetric Earliness and Tardiness Penalties.,"We consider one-processor scheduling problems having the following form: Tasks T1, T2, …, TN are given, with each Ti having a specified length li and a preferred starting time ai (or, equivalently, a preferred completion time bi). The tasks are to be scheduled nonpreemptively (i.e., a task cannot be split) on a single processor to begin as close to their preferred starting times as possible. We examine two different cost measures for such schedules, the sum of the absolute discrepancies from the preferred starting times and the maximum such discrepancy. For the first of these, we show that the problem of finding minimum cost schedules is NP-complete; however, we give an efficient algorithm that finds minimum cost schedules whenever the tasks either all have the same length or are required to be executed in a given fixed sequence. For the second cost measure, we give an efficient algorithm that finds minimum cost schedules in general, with no constraints on the ordering or lengths of the tasks.
"
1988,An O(n log log n)-Time Algorithm for Triangulating a Simple Polygon.,"Given a simple n-vertex polygon, the triangulation problem is to partition the interior of the polygon into $n - 2$ triangles by adding $n - 3$ nonintersecting diagonals. We propose an $O(n\log \log n)$-time algorithm for this problem, improving on the previously best bound of $O(n\log n)$ and showing that triangulation is not as hard as sorting. Improved algorithms for several other computational geometry problems, including testing whether a polygon is simple, follow from our result.
"
1988,A Fast Las Vegas Algorithm for Triangulating a Simple Polygon.,"We present an algorithm that triangulates a simple polygon on n vertices in &Ogr;(n log* n) expected time. The algorithm uses random sampling on the input, and its running time does not depend on any assumptions about a probability distribution from which the polygon is drawn."
1988,Dynamic Perfect Hashing: Upper and Lower Bounds.,"Abstract:
A randomized algorithm is given for the dictionary problem with O(1) worst-case time for lookup and O(1) amortized expected time for insertion and deletion. An Omega (log n) lower bound is proved for the amortized worst-case time complexity of any deterministic algorithm in a class of algorithms encompassing realistic hashing-based schemes. If the worst-case lookup time is restricted to k, then the lower bound for insertion becomes Omega (kn/sup 1/k/).< >"
1988,Finding Minimum-Cost Circulations by Canceling Negative Cycles.,"A classical algorithm for finding a minimum-cost circulation consists of repeatedly finding a residual cycle of negative cost and canceling it by pushing enough flow around the cycle to saturate an arc. We show that a judicious choice of cycles for canceling leads to a polynomial bound on the number of iterations in this algorithm. This gives a very simple strongly polynomial algorithm that uses no scaling. A variant of the algorithm that uses dynamic trees runs in O(nm(log n) min{log(nC), mlog n}) time on a network of n vertices, m arcs, and arc costs of maximum absolute value C. This bound is comparable to those of the fastest previously known algorithms."
1988,Almost-Optimum Speed-ups of Algorithms for Bipartite Matching and Related Problems.,"We present algorithms for matching and related problems that run on an EREW PRAM with p processors. Given is a bipartite graph G with n vertices, m edges, and integral edge costs at most N in magnitude. We give an algorithm for the assignment problem (minimum cost perfect bipartite matching) that runs in &Ogr;(âˆšnm log (nN)(log(2p))/p) time and &Ogr;(m) space, for p â‰¤ m/(âˆšnlog2n). For p = 1 this improves the best known sequential algorithm, and is within a factor of log (nN) of the best known bound for the problem without costs (maximum cardinality matching). For p > 1 the time is within a factor of log p of optimum speed-up. Extensions include an algorithm for maximum cardinality bipartite matching with slightly better processor bounds, and similar results for bipartite degree-constrained subgraph problems (with and without costs). Our ideas also extend to general graph matching problems."
1987,Linear-Time Algorithms for Visibility and Shortest Path Problems Inside Triangulated Simple Polygons.,"Given a triangulation of a simple polygonP, we present linear-time algorithms for solving a collection of problems concerning shortest paths and visibility withinP. These problems include calculation of the collection of all shortest paths insideP from a given source vertexS to all the other vertices ofP, calculation of the subpolygon ofP consisting of points that are visible from a given segment withinP, preprocessingP for fast ""ray shooting"" queries, and several related problems.
"
1987,Algorithmic Design.,"The quest for efficiency in computational methods yields not only fast algorithms, but also insights that lead to elegant, simple, and general problem-solving methods."
1987,Fibonacci heaps and their uses in improved network optimization algorithms.,"In this paper we develop a new data structure for implementing heaps (priority queues). Our structure, Fibonacci heaps (abbreviated F-heaps), extends the binomial queues proposed by Vuillemin and studied further by Brown. F-heaps support arbitrary deletion from an n-item heap in O(log n) amortized time and all other standard heap operations in O(1) amortized time. Using F-heaps we are able to obtain improved running times for several network optimization algorithms. In particular, we obtain the following worst-case bounds, where n is the number of vertices and m the number of edges in the problem graph:
O(n log n + m) for the single-source shortest path problem with nonnegative edge lengths, improved from O(mlog(m/n+2)n);
O(n2log n + nm) for the all-pairs shortest path problem, improved from O(nm log(m/n+2)n);
O(n2log n + nm) for the assignment problem (weighted bipartite matching), improved from O(nmlog(m/n+2)n);
O(mÎ²(m, n)) for the minimum spanning tree problem, improved from O(mlog log(m/n+2)n); where Î²(m, n) = min {i | log(i)n â‰¤ m/n}. Note that Î²(m, n) â‰¤ log*n if m â‰¥ n.
Of these results, the improved bound for minimum spanning trees is the most striking, although all the results give asymptotic improvements for graphs of appropriate densities."
1987,Three Partition Refinement Algorithms.,"We present improved partition refinement algorithms for three problems: lexicographic sorting, relational coarsest partition, and double lexical ordering. Our double lexical ordering algorithm uses a new, efficient method for unmerging two sorted sets.




"
1987,"Correction to ""A Linear-Time Algorithm for Triangulating Simple Polygons"".","Abstract:
In ""A linear-time algorithm for triangulating a simple polygon"" [Proceedings of the Eighteenth Annual ACM Symposium on Theory of Computing (1986), 380-388. 486], the analysis showing that the authors' triangulation algorithm runs in linear time is incorrect, and indeed the algorithm does not run in linear time in the worst case. So far they have been unable to obtain a linear-time algorithm for the triangulation problem. They have been able to obtain an O(n loglogn)-time algorithm, however. The details are described in ""An O(n loglogn)-Time Algorithm for Triangulating a Simple Polygon,"" SIAM Journal on Computing 17, 1 (February, 1988), to appear."
1987,Solving Minimum-Cost Flow Problems by Successive Approximation.,"We introduce a framework for solving minimum-cost flow problems. Our approach measures the quality of a solution by the amount that the complementary slackness conditions are violated. We show how to extend techniques developed for the maximum flow problem to improve the quality of a solution. This framework allows us to achieve &Ogr;(min(n3, n5/3 m2/3, nm log n) log (nC)) running time."
1986,The Pairing Heap: A New Form of Self-Adjusting Heap.,"Recently, Fredman and Tarjan invented a new, especially efficient form of heap (priority queue) called theFibonacci heap. Although theoretically efficient, Fibonacci heaps are complicated to implement and not as fast in practice as other kinds of heaps. In this paper we describe a new form of heap, called thepairing heap, intended to be competitive with the Fibonacci heap in theory and easy to implement and fast in practice. We provide a partial complexity analysis of pairing heaps. Complete analysis remains an open problem.
"
1986,A Locally Adaptive Data Compression Scheme.,"A data compression scheme that exploits locality of reference, such as occurs when words are used frequently over short intervals and then fall into long periods of disuse, is described. The scheme is based on a simple heuristic for self-organizing sequential search and on variable-length encodings of integers. We prove that it never performs much worse than Huffman coding and can perform substantially better; experiments on real files show that its performance is usually quite close to that of Huffman coding. Our scheme has many implementation advantages: it is simple, allows fast encoding and decoding, and requires only one pass over the data to be compressed (static Huffman coding takes two passes)."
1986,Planar Point Location Using Persistent Search Trees.,"A classical problem in computational geometry is the planar point location problem. This problem calls for preprocessing a polygonal subdivision of the plane defined by n line segments so that, given a sequence of points, the polygon containing each point can be determined quickly on-line. Several ways of solving this problem in O(log n) query time and O(n) space are known, but they are all rather complicated. We propose a simple O(log n)-query-time, O(n)-space solution, using persistent search trees. A persistent search tree differs from an ordinary search tree in that after an insertion or deletion, the old version of the tree can still be accessed. We develop a persistent form of binary search tree that supports insertions and deletions in the present and queries in the past. The time per query or update is O(log m), where m is the total number of updates, and the space needed is O(1) per update. Our planar point location algorithm is an immediate application of this data structure. The structure also provides an alternative to Chazelle's ""hive graph"" structure, which has a variety of applications in geometric retrieval."
1986,Efficient algorithms for finding minimum spanning trees in undirected and directed graphs.,"Recently, Fredman and Tarjan invented a new, especially efficient form of heap (priority queue). Their data structure, theFibonacci heap (or F-heap) supports arbitrary deletion inO(logn) amortized time and other heap operations inO(1) amortized time. In this paper we use F-heaps to obtain fast algorithms for finding minimum spanning trees in undirected and directed graphs. For an undirected graph containingn vertices andm edges, our minimum spanning tree algorithm runs inO(m log? (m, n)) time, improved fromO(m?(m, n)) time, where?(m, n)=min {i|log(i) n ?m/n}. Our minimum spanning tree algorithm for directed graphs runs inO(n logn + m) time, improved fromO(n log n +m log log log(m/n+2) n). Both algorithms can be extended to allow a degree constraint at one vertex.
"
1986,Rectilinear Planar Layouts and Bipolar Orientations of Planar Graphs.,"Abstract
We propose a linear-time algorithm for generating a planar layout of a planar graph. Each vertex is represented by a horizontal line segment and each edge by a vertical line segment. All endpoints of the segments have integer coordinates. The total space occupied by the layout is at mostn by at most 2nâ€“4. Our algorithm, a variant of one by Otten and van Wijk, generally produces a more compact layout than theirs and allows the dual of the graph to be laid out in an interlocking way. The algorithm is based on the concept of abipolar orientation. We discuss relationships among the bipolar orientations of a planar graph."
1986,Sorting Jordan Sequences in Linear Time Using Level-Linked Search Trees.,"For a Jordan curve C in the plane nowhere tangent to the x axis, let x1, x2,â€¦, xn be the abscissas of the intersection points of C with the x axis, listed in the order the points occur on C. We call x1, x2,â€¦, xn a Jordan sequence. In this paper we describe an O(n)-time algorithm for recognizing and sorting Jordan sequences. The problem of sorting such sequences arises in computational geometry and computational geography. Our algorithm is based on a reduction of the recognition and sorting problem to a list-splitting problem. To solve the list-splitting problem we use level-linked search trees."
1986,Deques with Heap Order.,"Abstract
A deque with heap order is a deque (double-ended queue) such that each item has a real-valued key and the operation of finding an item of minimum key is allowed as well as the usual deque operations. By combining a standard deque implementation with an auxiliary heap (priority queue) it is possible to implement a deque with heap order so that the worst-case time per operation is O(log n), where n is the number of items on the deque. This paper describes an implementation of deques with heap order for which the worst-case time per operation is O(1)."
1986,Self-Adjusting Heaps.,"In this paper we explore two themes in data structure design: amortized computational complexity and self adjustment. We are motivated by the following observations. In most applications of data structures, we wish to perform not just a single operation but a sequence of operations, possibly having correlated behavior. By averaging the running time per operation over a worst-case sequence of operations, we can sometimes obtain an overall time bound much smaller than the worst-case time per operation multiplied by the number of operations. We call this kind of averaging amortization.

Standard kinds of data structures, such as the many varieties of balanced trees, are specifically designed so that the worst-case time per operation is small. Such efficiency is achieved by imposing an explicit structural constraint that must be maintained during updates, at a cost of both running time and storage space. However, if amortized running time is the complexity measure of interest, we can guarantee efficiency without maintaining a structural constraint. Instead, during each access or update operation we adjust the data structure in a simple, uniform way. We call such a data structure self adjusting.

In this paper we develop the skew heap, a self-adjusting form of heap related to the leftist heaps of Crane and Knuth. (What we mean by a heap has also been called a “priority queue” or a “mergeable heap”.) Skew heaps use less space than leftist heaps and similar worst-case-efficient data structures and are competitive in running time, both in theory and in practice, with worst-case structures. They are also easier to implement. We derive an information-theoretic lower bound showing that skew heaps have minimum possible amortized running time, to within a constant factor, on any sequence of certain heap operations.

"
1986,Linear Time Algorithms for Visibility and Shortest Path Problems Inside Simple Polygons.,"We present linear time algorithms for solving the following problems involving a simple planar polygon P: (i) Computing the collection of all shortest paths inside P from a given source vertex s to all the other vertices of P; (ii) Computing the subpolygon of P consisting of points that are visible from a segment within P; (iii) Preprocessing P so that for any query ray r emerging from some fixed edge e of P, we can find in logarithmic time the first intersection of r with the boundary of P; (iv) Preprocessing P so that for any query point x in P, we can find in logarithmic time the portion of the edge e that is visible from x; (v) Preprocessing P so that for any query point x inside P and direction u, we can find in logarithmic time the first point on the boundary of P hit by the ray at direction u from x; (vi) Calculating a hierarchical decomposition of P into smaller polygons by recursive polygon cutting, as in [Ch]. (vii) Calculating the (clockwise and counterclockwise) â€œconvex ropesâ€ (in the terminology of [PS]) from a fixed vertex s of P lying on its convex hull, to all other vertices of P. All these algorithms are based on a recent linear time algorithm of Tarjan and Van Wyk for triangulating a simple polygon, but use additional techniques to make all subsequent phases of these algorithms also linear."
1986,Making Data Structures Persistent.,"This paper is a study of persistence in data structures. Ordinary data structures are ephemeral in the sense that a change to the structure destroys the old version, leaving only the new version available for use. In contrast, a persistent structure allows access to any version, old or new, at any time. We develop simple, systematic, and efficient techniques for making linked data structures persistent. We use our techniques to devise persistent forms of binary search trees with logarithmic access, insertion, and deletion times and O(1) space bounds for insertion and deletion."
1986,"Rotation Distance, Triangulations, and Hyperbolic Geometry.",n/a
1986,A New Approach to the Maximum Flow Problem.,"A rotation in a binary tree is a local restructuring that changes the tree into another tree. Rotations are useful in the design of tree-based data structures. The rotation distance between a pair of trees is the minimum number of rotations needed to convert one tree into the other. In this paper we establish a tight bound of In 6 on the maximum rotation distance between two A2-node trees for all large n, using volumetric arguments in hyperbolic 3-space. Our proof also gives a tight bound on the minimum number of tetrahedra needed to dissect a polyhedron in the worst case, and reveals connections"
1986,A Linear-Time Algorithm for Triangulating Simple Polygons.,"This paper presents a linear-time algorithm for the special case of the disjoint set union problem in which the structure of the unions (defined by a “union tree”) is known in advance. The algorithm executes an intermixed sequence of m union and find operations on n elements in O(m+n) time and O(n) space. This is a slight but theoretically significant improvement over the fastest known algorithm for the general problem, which runs in O(m?(m+n, n)+n) time and O(n) space, where a is a functional inverse of Ackermann's function. Used as a subroutine, the algorithm gives similar improvements in the efficiency of algorithms for solving several other problems, including two-processor scheduling, matching on convex graphs, finding nearest common ancestors off-line, testing a flow graph for reducibility, and finding two disjoint directed spanning trees. The algorithm obtains its efficiency by combining the fast algorithm for the general problem with table look-up on small sets, and requires a random access machine for its implementation. The algorithm extends to the case in which single-node additions to the union tree are allowed. The extended algorithm is useful in finding maximum cardinality matchings in nonbipartite graphs."
1985,Amortized Efficiency of List Update and Paging Rules.,"In this article we study the amortized efficiency of the â€œmove-to-frontâ€ and similar rules for dynamically maintaining a linear list. Under the assumption that accessing the ith element from the front of the list takes &thgr;(i) time, we show that move-to-front is within a constant factor of optimum among a wide class of list maintenance rules. Other natural heuristics, such as the transpose and frequency count rules, do not share this property. We generalize our results to show that move-to-front is within a constant factor of optimum as long as the access cost is a convex function. We also study paging, a setting in which the access cost is not convex. The paging rule corresponding to move-to-front is the â€œleast recently usedâ€ (LRU) replacement rule. We analyze the amortized complexity of LRU, showing that its efficiency differs from that of the off-line paging rule (Belady's MIN algorithm) by a factor that depends on the size of fast memory. No on-line paging algorithm has better amortized performance."
1985,Sequential access in play trees takes linear time.,"Abstract
Sleator and Tarjan have invented a form of self-adjusting binary search tree called thesplay tree. On any sufficiently long access sequence, splay trees are as efficient, to within a constant factor, as both dynamically balanced and static optimum search trees. Sleator and Tarjan have made a much stronger conjecture; namely, that on any sufficiently long access sequence and to within a constant factor, splay trees are as efficient asany form of dynamically updated search tree. Thisdynamic optimality conjecture implies as a special case that accessing the items in a splay tree in sequential order takes linear time, i.e.O(1) time per access. In this paper we prove this special case of the conjecture, generalizing an unpublished result of Wegman. Oursequential access theorem not only supports belief in the dynamic optimality conjecture but provides additional insight into the workings of splay trees. As a corollary of our result, we show that splay trees can be used to simulate output-restricted deques (double-ended queues) in linear time. We pose several open problems related to our result."
1985,Decomposition by clique separators.,"Abstract
We consider the problem of decomposing a graph by means of clique separators, by which we mean finding cliques (complete graphs) whose removal disconnects the graph. We give an O(nm)-time algorithm for finding a decomposition of an n-vertex, m-edge graph. We describe how such a decomposition can be used in divide-and-conquer algorithms for various graph problems, such as graph coloring and finding maximum independent sets. We survey classes of graphs for which such divide-and-conquer algorithms are especially useful."
1985,Self-Adjusting Binary Search Trees.,"The splay tree, a self-adjusting form of binary search tree, is developed and analyzed. The binary search tree is a data structure for representing tables and lists so that accessing, inserting, and deleting items is easy. On an n-node splay tree, all the standard search tree operations have an amortized time bound of O(log n) per operation, where by â€œamortized timeâ€ is meant the time per operation averaged over a worst-case sequence of operations. Thus splay trees are as efficient as balanced trees when total running time is the measure of interest. In addition, for sufficiently long access sequences, splay trees are as efficient, to within a constant factor, as static optimum search trees. The efficiency of splay trees comes not from an explicit structural constraint, as with balanced trees, but from applying a simple restructuring heuristic, called splaying, whenever the tree is accessed. Extensions of splaying give simplified forms of two other data structures: lexicographic or multidimensional search trees and link/cut trees."
1985,A Linear-Time Algorithm for a Special Case of Disjoint Set Union.,"Abstract
This paper presents a linear-time algorithm for the special case of the disjoint set union problem in which the structure of the unions (defined by a â€œunion treeâ€) is known in advance. The algorithm executes an intermixed sequence of m union and find operations on n elements in O(m+n) time and O(n) space. This is a slight but theoretically significant improvement over the fastest known algorithm for the general problem, which runs in O(mÎ±(m+n, n)+n) time and O(n) space, where a is a functional inverse of Ackermann's function. Used as a subroutine, the algorithm gives similar improvements in the efficiency of algorithms for solving several other problems, including two-processor scheduling, matching on convex graphs, finding nearest common ancestors off-line, testing a flow graph for reducibility, and finding two disjoint directed spanning trees. The algorithm obtains its efficiency by combining the fast algorithm for the general problem with table look-up on small sets, and requires a random access machine for its implementation. The algorithm extends to the case in which single-node additions to the union tree are allowed. The extended algorithm is useful in finding maximum cardinality matchings in nonbipartite graphs."
1985,A Note on Finding Minimum-Cost Edge-Disjoint Spanning Trees.,"Let G be an undirected graph with n vertices and m edges, such that each edge has a real-valued cost. We consider the problem of finding a set of k edge-disjoint spanning trees in G of minimum total edge cost. This problem can be solved in polynomial time by the matroid greedy algorithm. We present an implementation of this algorithm that runs in O(m log m + k2n2) time. If all edge costs are the same, the algorithm runs in O(k2n2) time. The algorithm can also be extended to find the largest k such that k edge-disjoint spanning trees exist in O(m2) time. We mention several applications of the algorithm.
"
1985,Strongly connected orientations of mixed multigraphs.,"We study the problem of orienting all the undirected edges of a mixed multigraph so as to preserve reachability. Extending work by Robbins and by Boesch and Tindell, we develop a linear?time algorithm to test whether there is an orientation that preserves strong connectivity and to construct such an orientation whenever possible. This algorithm makes no attempt to minimize distances in the resulting directed graph, and indeed the maximum distance, for example, can blow up by a factor proportional to the number of vertices in the graph. Extending work by Chvátal and Thomassen, we then prove that, if a mixed multigraph of radius r has any strongly connected orientation, it must have an orientation of radius at most 42 + Ar. The proof gives a polynomial?time algorithm for constructing such an orientation.
"
1985,Biased Search Trees.,"We consider the problem of storing items from a totally ordered set in a search tree so that the access time for a given item depends on a known estimate of the access frequency of the item. We describe two related classes of biased search trees whose average access time is within a constant factor of the minimum and that are easy to update under insertions, deletions and more radical update operations. We present and analyze efficient update algorithms for biased search trees. We list several applications of such trees.
"
1985,An Efficient Parallel Biconnectivity Algorithm.,"In this paper we propose a new algorithm for finding the blocks (biconnected components) of an undirected graph. A serial implementation runs in $O(n + m)$ time and space on a graph of n vertices and m edges. A parallel implementation runs in $O(\log n)$ time and $O(n + m)$ space using $O(n + m)$ processors on a concurrent-read, concurrent-write parallel RAM. An alternative implementation runs in $O(n^2 /p)$ time and $O(n^2 )$ space using any number $p \leqq n^2 /\log ^2 n$ of processors, on a concurrent-read, exclusive-write parallel RAM. The last algorithm has optimal speedup, assuming an adjacency matrix representation of the input. A general algorithmic technique that simplifies and improves computation of various functions on trees is introduced. This technique typically requires $O(\log n)$ time using processors and $O(n)$ space on an exclusive-read exclusive-write parallel RAM.
"
1985,A Linear Time Solution to the Single Function Coarsest Partition Problem.,"Abstract
The problem of finding the coarsest partition of a set S with respect to another partition of S one or more functions on S has several applications, one of which is the state minimization of finite state automata. In 1971, Hopcroft presented an algorithm to solve the many function coarsest partition problem for sets of n elements in O(n log n) time and O(n) space. In 1974, Aho, Hopcroft and Ullman presented an O(n log n) algorithm that solves the special case of this problem for only one function. Both these algorithms use a negative strategy that repeatedly refines the original partition until a solution is found. We present a new algorithm to solve the single function coarsest partition problem in O(n) time and space using a different, constructive approach. Our algorithm can be applied to the automated manufacturing of woven fabric."
1985,Sorting Jordan sequences in linear time.,"For a Jordan curve C in the plane, let x_{1},x_{2},...,x_{n} be the abscissas of the intersection points of C with the x-axis, listed in the order the points occur on C. We call x_{1},x_{2},...,x_{n} a Jordan sequence. In this paper we describe an O(n)-time algorithm for recognizing and sorting Jordan sequences. The problem of sorting such sequences arises in computational geometry and computational geography. Our algorithm is based on a reduction of the recognition and sorting problem to a list-splitting problem. To solve the list-splitting problem we use level linked search trees. "
1984,Worst-case Analysis of Set Union Algorithms.,"This paper analyzes the asymptotic worst-case running time of a number of variants of the well-known method of path compression for maintaining a collection of disjoint sets under union. We show that two one-pass methods proposed by van Leeuwen and van der Weide are asymptotically optimal, whereas several other methods, including one proposed by Rem and advocated by Dijkstra, are slower than the best methods."
1984,Efficient Algorithms for a Family of Matroid Intersection Problems.,"Abstract
Consider a matroid where each element has a real-valued cost and a color, red or green; a base is sought that contains q red elements and has smallest possible cost. An algorithm for the problem on general matroids is presented, along with a number of variations. Its efficiency is demonstrated by implementations on specific matroids. In all cases but one, the running time matches the best-known algorithm for the problem without the red element constraint: On graphic matroids, a smallest spanning tree with q red edges can be found in time O(n log n) more than what is needed to find a minimum spanning tree. A special case is finding a smallest spanning tree with a degree constraint; here the time is only O(m + n) more than that needed to find one minimum spanning tree. On transversal and matching matroids, the time is the same as the best-known algorithms for a minimum cost base. This also holds for transversal matroids for convex graphs, which model a scheduling problem on unit-length jobs with release times and deadlines. On partition matroids, a linear-time algorithm is presented. Finally an algorithm related to our general approach finds a smallest spanning tree on a directed graph, where the given root has a degree constraint. Again the time matches the best-known algorithm for the problem without the red element (i.e., degree) constraint."
1984,"Gauss Codes, Planar Hamiltonian Graphs, and Stack-Sortable Permutations.","Abstract
In this paper the following three recognition problems are considered: (1) Test whether a given sequence is the Gauss code of a planar self-intersecting curve; (2) test whether a given graph with a known Hamiltonian cycle is planar; and (3) test whether a given permutation can be sorted using two stacks in parallel. These three problems are closely related: A simple linear-time algorithm that solves all three is described. The heart of the algorithm is a data structure, previously used in general planarity testing, called a pile of twin stacks."
1984,A Separator Theorem for Graphs of Bounded Genus.,"Abstract
Many divide-and-conquer algorithms on graphs are based on finding a small set of vertices or edges whose removal divides the graph roughly in half. Most graphs do not have the necessary small separators, but some useful classes do. One such class is planar graphs: If an n-vertex graph can be drawn on the plane, then it can be bisected by removal of O(sqrt(n)) vertices (R. J. Lipton and R. E. Tarjan, SIAM J. Appl. Math.36 (1979), 177â€“189). The main result of the paper is that if a graph can be drawn on a surface of genus g, then it can be bisected by removal of O(sqrt(gn)) vertices. This bound is best possible to within a constant factor. An algorithm is given for finding the separator that takes time linear in the number of edges in the graph, given an embedding of the graph in its genus surface. Some extensions and applications of these results are discussed."
1984,A quick method for finding shortest pairs of disjoint paths.,"Let G be a directed graph containing n vertices, one of which is a distinguished source s, and m edges, each with a non?negative cost. We consider the problem of finding, for each possible sink vertex v, a pair of edge?disjoint paths from s to v of minimum total edge cost. Suurballe has given an O(n2 logn)?time algorithm for this problem. We give an implementation of Suurballe's algorithm that runs in O(m log(1+ m/n)n) time and O(m) space. Our algorithm builds an implicit representation of the n pairs of paths; given this representation, the time necessary to explicitly construct the pair of paths for any given sink is O(1) per edge on the paths.
"
1984,Fast Algorithms for Finding Nearest Common Ancestors.,"We consider the following problem: Given a collection of rooted trees, answer on-line queries of the form, “What is the nearest common ancester of vertices x and y?” We show that any pointer machine that solves this problem requires $\Omega (\log \log n)$ time per query in the worst case, where n is the total number of vertices in the trees. On the other hand, we present an algorithm for a random access machine with uniform cost measure (and a bound of $\Omega (\log n)$ on the number of bits per word) that requires $O(1)$ time per query and $O(n)$ preprocessing time, assuming that the collection of trees is static. For a version of the problem in which the trees can change between queries, we obtain an almost-linear-time (and linear-space) algorithm.

"
1984,"Simple Linear-Time Algorithms to Test Chordality of Graphs, Test Acyclicity of Hypergraphs, and Selectively Reduce Acyclic Hypergraphs.","Chordal graphs arise naturally in the study of Gaussian elimination on sparse symmetric matrices; acyclic hypergraphs arise in the study of relational data bases. Rose, Tarjan and Lueker [SIAM J. Comput., 5 (1976), pp. 266–283] have given a linear-time algorithm to test whether a graph is chordal, which Yannakakis has modified to test whether a hypergraph is acyclic. Here we develop a simplified linear-time test for graph chordality and hypergraph acyclicity. The test uses a new kind of graph (and hypergraph) search, which we call maximum cardinality search A variant of the method gives a way to selectively reduce acyclic hypergraphs, which is needed for evaluating queries in acyclic relational data bases."
1984,Finding Biconnected Components and Computing Tree Functions in Logarithmic Parallel Time (Extended Summary).,"Abstract:
We propose a new algorithm for finding the blocks (biconnected components) of an undirected graph. A serial implementation runs in 0[n+m] time and space on a graph of n vertices and m edges. A parallel implmentation runs in 0[log n] time and 0[n+m] space using 0[n+m] processors on a concurrent-read, concurrent-write parallel RAM. An alternative implementation runs in 0[n/sup 2/p] time and 0[n/sup 2/] space using any number p â©½ n/sup 2/log/sup 2/-n of processors, on a concurrent-read, exclusive-write parallel RAM. The latter algorithm has optimal speedup, assuming an adjacency matrix representation of the input. A general algorithmic technique which simplifies and improve computation of various functions on tress is introduced. This technique typically requires 0(log n) time using 0(n) space on an exclusive-read exclusive-write parallel RAM."
1984,Fibonacci Heaps and Their Uses in Improved Network Optimization Algorithms.,"Abstract:
In this paper we develop a new data structure for implementing heaps (priority queues). Our structure, Fibonacci heaps (abbreviated F-heaps), extends the binomial queues proposed by Vuillemin and studied further by Brown. F-heaps support arbitrary deletion from an n-item heap in 0(log n) amortized time and all other standard heap operations in 0(1) amortized time. Using F-heaps we are able to obtain improved running times for several network optimization algorithms."
1984,A Linear Time Algorithm to Solve the Single Function Coarsest Partition Problem.,"Abstract
The problem of finding the coarsest partition of a set S with respect to another partition of S and one or more functions on S has several applications, one of which is the state minimization of finite state automata. In 1971 Hopcroft presented an algorithm to solve the many function coarsest partition problem for sets of n elements in O(n log n) time and O(n) space. Aho, Hopcroft, and Ullman later presented an algorithm that solves the special case of this problem for only one function. Both these algorithms use a negative strategy that repeatedly refines the original partition until a solution is found. We present a new algorithm to solve the single function coarsest partition problem in O(n) time and space using a different, constructive approach."
1984,Scaling and Related Techniques for Geometry Problems.,"Three techniques in computational geometry are explored: Scaling solves a problem by viewing it at increasing levels of numerical precision; activation is a restricted type of update operation, useful in sweep algorithms; the Cartesian tree is a data structure for problems involving maximums and minimums. These techniques solve the minimum spanning tree problem in Rk1 and Rk@@@@ in O(n(lg n)rlg lg n) time and O(n) space, where for Rk@@@@ and k â‰¥ 3, r = k-2; for Rk1, r = 1, 2, 4 for k = 3, 4, 5 and r = k for k > 5. Other problems solved include Rk1and Rk all nearest neighbors, post office and maximum spanning tree; Rk maxima, Rk rectangle searching problems, and Zkp all nearest neighbors (1 â‰¤ p â‰¤ @@@@)."
1984,Amortized Efficiency of List Update Rules.,"In this paper we study the amortized complexity of two well-known algorithms used in system software, These are the ""move-to-front"" rule for maintaining an unsorted linear list representing a set, and the ""least-recently-used"" replacement rule for reducing page faults in a two-level paged memory. These algorithms have been subjected to much analysis, most of it average case. By studying the amortized complexity of these algorithms we are able to gain additional insight into their behavior. "
1983,Updating a Balanced Search Tree in O(1) Rotations.,"Abstract
OliviÃ© has recently introduced the class of â€˜half-balancedâ€™ binary search trees, which have O(log n) access time but require only a constant number of single rotations for rebalancing after an insertion or a deletion. In this paper we show that a well-known class of balanced binary trees, the â€˜symmetric binary B-treesâ€™ of Bayer, have the same properties. This is not surprising, for Bayer's class and OliviÃ©s class contain exactly the same binary trees."
1983,An Improved Algorithm for Hierarchical Clustering Using Strong Components.,"Abstract
In 1982 the author presented an O(m(log n)2) time algorithm for hierarchically decomposing a directed n-vertex, m-edge graph with weighted edges into strong components. Such an algorithm is useful in cluster analysis of data with an asymmetric similarity measure. The present paper gives a simpler algorithm with the faster running time of O(m log n)."
1983,A Data Structure for Dynamic Trees.,"Abstract
A data structure is proposed to maintain a collection of vertex-disjoint trees under a sequence of two kinds of operations: a link operation that combines two trees into one by adding an edge, and a cut operation that divides one tree into two by deleting an edge. Each operation requires O(log n) time. Using this data structure, new fast algorithms are obtained for the following problems:
1.
(1) Computing nearest common ancestors.
2.
(2) Solving various network flow problems including finding maximum flows, blocking flows, and acyclic flows.
3.
(3) Computing certain kinds of constrained minimum spanning trees.
4.
(4) Implementing the network simplex algorithm for minimum-cost flows.
The most significant application is (2); an O(mn log n)-time algorithm is obtained to find a maximum flow in a network of n vertices and m edges, beating by a factor of log n the fastest algorithm previously known for sparse graphs."
1983,Space-Efficient Implementations of Graph Search Methods.,"Several space-efficient implementations of the two most common and useful kinds of graph search, namely, breadth-first search and depth-first search, are discussed. A straightforward implementation of each method requires n bits and n + O(1) pointers of auxiliary storage, where n is the number of vertices in the graph. We devise methods that need only 2n + m bits, of which m are read-only, where m is the number of edges in the graph. We save space by folding the the queue or stack required by the search into the graph representation; two of our methods for depth-first search are variants of the Deutsch-Schorr-Waite list marking algorithm. Our algorithms are expressed in a version of Dijkstra's guarded command language."
1983,Self-Adjusting Binary Trees.,"We use the idea of self-adjusting trees to create new, simple data structures for priority queues (which we call heaps) and search trees. Unlike other efficient implementations of these data structures, self-adjusting trees have no balance condition. Instead, whenever the tree is accessed, certain adjustments take place. (In the case of heaps, the adjustment is a sequence of exchanges of children, in the case of search trees the adjustment is a sequence of rotations.) Self-adjusting trees are efficient in an amortized sense: any particular operation may be slow but any sequence of operations must be fast. Self-adjusting trees have two advantages over the corresponding balanced trees in both applications. First, they are simpler to implement because there are fewer cases in the algorithms. Second, they are more storage-efficient because no balance information needs to be stored. Furthermore, a self-adjusting search tree has the remarkable property that its running time (for any sufficiently long sequence of search operations) is within a constant factor of the running time for the same set of searches on any fixed binary tree. It follows that a self-adjusting tree is (up to a constant factor) as fast as the optimal fixed tree for a particular probability distribution of search requests, even though the distribution is unknown."
1983,A Linear-Time Algorithm for a Special Case of Disjoint Set Union.,"This paper presents a linear-time algorithm for the special case of the disjoint set union problem in which the structure of the unions (defined by a â€œunion treeâ€) is known in advance. The algorithm executes an intermixed sequence of m union and find operations on n elements in 0(m+n) time and 0(n) space. This is a slight but theoretically significant improvement over the fastest known algorithm for the general problem, which runs in 0(m&agr;(m+n, n)+n) time and 0(n) space, where &agr; is a functional inverse of Ackermann's function. Used as a subroutine, the algorithm gives similar improvements in the efficiency of algorithms for solving a number of other problems, including two-processor scheduling, the off-line min problem, matching on convex graphs, finding nearest common ancestors off-line, testing a flow graph for reducibility, and finding two disjoint directed spanning trees. The algorithm obtains its efficiency by combining a fast algorithm for the general problem with table look-up on small sets, and requires a random access machine for its implementation. The algorithm extends to the case in which single-node additions to the union tree are allowed. The extended algorithm is useful in finding maximum cardinality matchings on nonbipartite graphs."
1982,A Hierarchical Clustering Algorithm Using Strong Components.,n/a
1982,Sensitivity Analysis of Minimum Spanning Trees and Shortest Path Trees.,n/a
1982,Asymptotically tight bounds on time-space trade-offs in a pebble game.,n/a
1982,Symbolic Program Analysis in Almost-Linear Time.,"This paper describes an algorithm to construct, for each expression in a given program text, a symbolic expression whose value is equal to the value of the text expression for all executions of the program. We call such a mapping from text expressions to symbolic expressions a cover. Covers are useful in such program optimization techniques as constant propagation and code motion. The particular cover constructed by our methods is in general weaker than the covers obtainable by the methods of [Ki], [FKU], [RL], [R2] but our method has the advantage of being very efficient. It requires $O(m\alpha (m,n) + l)$ operations if extended bit vector operations have unit cost, where n is the number of vertices in the control flow graph of the program, m is the number of edges, l is the length of the program text, and $\alpha $ is related to a functional inverse of Ackermann’s function [T2]. Our method does not require that the program be well-structured nor that the flow graph be reducible.
"
1982,The Recognition of Series Parallel Digraphs.,"We present a linear-time algorithm to recognize the class of vertex series-parallel (VSP) digraphs. Our method is based on the relationship between VSP digraphs and the class of edge series-parallel multidigraphs. As a byproduct of our analysis, we obtain efficient methods to compute the transitive closure and transitive reduction of VSP digraphs, and to test isomorphism of minimal VSP digraphs.


"
1981,A Unified Approach to Path Problems.,"We describe a general method for solving path problems on directed graphs. Such path problems include finding shortest paths, solving sparse systems of linear equations, and carrying out global flow analysis of computer programs. Our method consists of two steps. First, we construct a collection of regular expressions representing sets of paths in the graph. This can be done by using any standard algorithm, such as Gaussian or Gauss-Jordan elimination. Next, we apply a natural mapping from regular expressions into the given problem domain. We exhibit the mappings required to find shortest paths, solve sparse systems of linear equations, and carry out global flow analysis. Our results provide a general-purpose algorithm for solving any path problem, and show that the problem of constructing path expressions is in some sense the most general path problem."
1981,Fast Algorithms for Solving Path Problems.,"Let G = (V,E) be a directed graph with a distinguished source vertex s. The single-source path expression problem is to find, for each vertex v, a regular expression P(s,v) which represents the set of all paths in G from s to v. A solution to this problem can be used to solve shortest path problems, solve sparse systems of linear equations, and carry out global flow analysis. We describe a method to compute path expressions by dividing G into components, computing path expressions on the components by Gaussian elimination, and combining the solutions. This method requires O(m $\alpha$(m,n)) time on a reducible flow graph, where n is the number of vertices in G, m is the number of edges in G, and $\alpha$ is a functional inverse of Ackermann''s function. The method makes use of an algorithm for evaluating functions defined on paths in trees. A simplified version of the algorithm, which runs in O(m log n) time on reducible flow graphs, is quite easy to implement and efficient in practice"
1981,Scheduling Unit-Time Tasks with Arbitrary Release Times and Deadlines.,"The basic problem considered is that of scheduling n unit-time tasks, with arbitrary release times and deadlines, so as to minimize the maximum task completion time. Previous work has shown that this problem can be solved rather easily when all release times are integers. We are concerned with the general case in which noninteger release times are allowed, a generalization that considerably increases the difficulty of the problem even for only a single processor. Our results are for the one-processor case, where we provide an $O(n\log n)$ algorithm based on the concept of “forbidden regions”.
"
1981,On a Greedy Heuristic for Complete Matching.,"Finding a minimum weighted complete matching on a set of vertices in which the distances satisfy the triangle inequality is of general interest and of particular importance when drawing graphs on a mechanical plotter. The “greedy” heuristic of repeatedly matching the two closest unmatched points can be implemented in worst-case time $O(n^2 \log n)$, a reasonable savings compared to the general minimum weighted matching algorithm which requires time proportional to $n^3 $ to find the minimum cost matching in a weighted graph. We show that, for an even number n of vertices whose distances satisfy the triangle inequality, the ratio of the cost of the matching produced by this greedy heuristic to the cost of the minimal matching is at most ${}_3^4 n^{\lg _2^3 } - 1$, $\lg _2^3 \approx 0.58496$, and there are examples that achieve this bound. We conclude that this greedy heuristic, although desirable because of its simplicity, would be a poor choice for this problem.


"
1981,A Data Structure for Dynamic Trees.,"We propose a data structure to maintain a collection of vertex-disjoint trees under a sequence of two kinds of operations: a link operation that combines two trees into one by adding an edge, and a cut operation that divides one tree into two by deleting an edge. Our data structure requires O(log n) time per operation when the time is amortized over a sequence of operations. Using our data structure, we obtain new fast algorithms for the following problems: (1) Computing deepest common ancestors. (2) Solving various network flow problems including finding maximum flows, blocking flows, and acyclic flows. (3) Computing certain kinds of constrained minimum spanning trees. (4) Implementing the network simplex algorithm for the transshipment problem. Our most significant application is (2); we obtain an O(mn log n)-time algorithm to find a maximum flow in a network of n vertices and m edges, beating by a factor of log n the fastest algorithm previously known for sparse graphs."
1980,The Space Complexity of Pebble Games on Trees.,n/a
1980,Variations on the Common Subexpression Problem.,"Let G be a directed graph such that for each vertex v in G, the successors of v are ordered Let C be any equivalence relation on the vertices of G. The congruence closure C* of C is the finest equivalence relation containing C and such that any two vertices having corresponding successors equivalent under C* are themselves equivalent under C* Efficient algorithms are described for computing congruence closures in the general case and in the following two special cases. 0) G under C* is acyclic, and (it) G is acychc and C identifies a single pair of vertices. The use of these algorithms to test expression eqmvalence (a problem central to program verification) and to test losslessness of joins in relational databases is described"
1980,Linear Expected-Time Algorithms for Connectivity Problems.,"Abstract
This paper describes fast average-time algorithms for four graph connectivity problems. Algorithms that run in O(n) average time on n-vertex graphs are developed for finding connected components, strong components, and blocks. An O(m)-time algorithm to find a minimum spanning forest in an m-graph is also presented. The analysis of these algorithms uses the random graph model of ErdÃ¶s and Renyi. All the algorithms are optimum to within a constant factor."
1980,The Pebbling Problem is Complete in Polynomial Space.,"In this paper we study a pebbling problem that models the storage requirements of various kinds of computation. Sethi has shown this problem to be $NP$-hard and Lingas has shown a generalization to be P-space complete. We prove the original problem P-space complete by using a modification of Lingas’s proof. The pebbling problem is an example of a P-space complete problem not exhibiting any obvious quantifier alternation.
"
1980,Design and Analysis of a Data Structure for Representing Sorted Lists.,"In this paper we explore the use of 2-3 trees to represent sorted lists. We analyze the worst-case cost of sequences of insertions and deletions in 2-3 trees under each of the following three assumptions: (i) only insertions are performed; (ii) only deletions are performed; (iii) deletions occur only at the small end of the list and insertions occur only away from the small end. Our analysis leads to a data structure for representing sorted lists when the access pattern exhibits a (perhaps time-varying) locality of reference. This structure has many of the properties of the representation proposed by Guibas, McCreight, Plass and Roberts [A new representation for linear lists, Proc. Ninth Annual Symposium on Theory of Computing, Boulder, CO, 1977, pp. 49–60], but it is substantially simpler and may be practical for lists of moderate size.
"
1980,Applications of a Planar Separator Theorem.,"Any n-vertex planar graph has the property that it can be divided into components of roughly equal size by removing only $O(\sqrt n )$ vertices. This separator theorem, in combination with a divide-and-conquer strategy, leads to many new complexity results for planar graph problems. This paper describes some of these results.



"
1980,Performance Bounds for Level-Oriented Two-Dimensional Packing Algorithms.,"We analyze several “level-oriented” algorithms for packing rectangles into a unit-width, infinite-height bin so as to minimize the total height of the packing. For the three algorithms we discuss, we show that the ratio of the height obtained by the algorithm to the optimal height is asymptotically bounded, respectively, by 2, 1.7, and 1.5. The latter two improve substantially over the performance bounds for previously proposed algorithms. In addition, we give more refined bounds for special cases in which the widths of the given rectangles are restricted and in which only squares are to be packed.




"
1980,Biased 2-3 Trees.,"Abstract:
We describe a new data structure for maintaining collections of weighted items. The access time for an item of weight w in a collection of total weight W is proportional to log(W/w) in the worst case (which is optimal in a certain sense), and several other useful operations can be made to work just, as fast. The data structure is simpler than previous proposals, but the running time must be amortized over a sequence of operations to achieve the time bounds."
1980,Prime Subprogram Parsing of a Program.,A parsing method based on the triconnected decomposition of a biconnected graph is presented. The parsing algorithm runs in linear time and handles a large class of flow graphs. The applications of this algorithm to flow analysis and to the automatic structuring of programs are discussed.
1980,Linear Expected-Time Algorithms for Connectivity Problems (Extended Abstract).,"Researchers in recent years have developed many graph algorithms that are fast in the worst case, but little work has been done on graph algorithms that are fast on the average. (Exceptions include the work of Angluin and Valiant [1], Karp [7], and Schnorr [9].) In this paper we analyze the expected running time of four algorithms for solving graph connectivity problems. Our goal is to exhibit algorithms whose expected time is within a constant factor of optimum and to shed light on the properties of random graphs. In Section 2 we develop and analyze a simple algorithm that finds the connected components of an undirected graph with n vertices in O(n) expected time. In Sections 3 and 4 we describe algorithms for finding the strong components of a directed graph and the blocks of an undirected graph in O(n) expected time. The time required for these three problems is Î©(m) in the worst case, where m is the number of edges in the graph, since all edges must be examined; but our results show that only O(n) edges must be examined on the average.*@@@@ In Section 5 we present an algorithm for finding a minimum weight spanning forest in an undirected graph with edge weights in O(m) expected time."
1979,Storing a Sparse Table.,"The problem of storing and searching large sparse tables is ubiquitous in computer science. The standard technique for storing such tables is hashing, but hashing has poor worst-case performance. We propose a good worst-case method for storing a static table of n entries, each an integer between 0 and N - 1. The method requires O(n) words of storage and allows O(logn N) access time. Although our method is a little complicated to use in practice, our analysis shows why a simpler algorithm used for compressing LR parsing tables works so well."
1979,A Linear-Time Algorithm for Testing the Truth of Certain Quantified Boolean Formulas.,n/a
1979,A Fast Merging Algorithm.,"An algorithm that merges sorted lists represented as height-balanced binary trees is given. If the lists have lengths m and n then the merging procedure runs in steps, which is the same order as the lower bound on all comparison-based algorithms for this problem."
1979,Applications of Path Compression on Balanced Trees.,"We devise a method for computing functions defined on paths in trees. The method is based on tree manipulation techniques first used for efficiently representing equivalence relations. It has an almost-linear running time. We apply the method to give O(m $\alpha$(m,n)) algorithms for two problems. A. Verifying a minimum spanning tree in an undirected graph (best previous bound: O(m log log n) ). B. Finding dominators in a directed graph (best previous bound: O(n log n + m) ). Here n is the number of vertices and m the number of edges in the problem graph, and $\alpha$(m,n) is a very slowly growing function which is related to a functional inverse of Ackermann''s function. The method is also useful for solving, in O(m $\alpha$(m,n)) time, certain kinds of pathfinding problems on reducible graphs. Such problems occur in global flow analysis of computer programs and in other contexts. A companion paper will discuss this application."
1979,A Class of Algorithms which Require Nonlinear Time to Maintain Disjoint Sets.,"Abstract
This paper describes a machine model intended to be useful in deriving realistic complexity bounds for tasks requiring list processing. As an example of the use of the model, the paper defines a class of algorithms which compute unions of disjoint sets on-line, and proves that any such algorithm requires nonlinear time in the worst case. All set union algorithms known to the author are instances of the model and are thus subject to the derived bound. One of the known algorithms achieves the bound to within a constant factor."
1979,A Fast Algorithm for Finding Dominators in a Flowgraph.,"A fast algorithm for finding dominators in a flowgraph is presented. The algorithm uses depth-first search and an efficient method of computing functions defined on paths in trees. A simple implementation of the algorithm runs in O(m log n) time, where m is the number of edges and n is the number of vertices in the problem graph. A more sophisticated implementation runs in O(m&agr;(m, n)) time, where &agr;(m, n) is a functional inverse of Ackermann's function. Both versions of the algorithm were implemented in Algol W, a Stanford University version of Algol, and tested on an IBM 370/168. The programs were compared with an implementation by Purdom and Moore of a straightforward O(mn)-time algorithm, and with a bit vector algorithm described by Aho and Ullman. The fast algorithm beat the straightforward algorithm and the bit vector algorithm on all but the smallest graphs tested."
1979,Efficient Algorithms for Simple Matroid Intersection Problems.,"Abstract:
Given a matroid, where each element has a realvalued cost and is colored red or green; we seek a minimum cost base with exactly q red elements. This is a simple case of the matroid intersection problem. A general algorithm is presented. Its efficiency is illustrated in the special case of finding a minimum spanning tree with q red edges; the time is O(m log log n + n Î± (n,n) log n). Efficient algorithms are also given for job scheduling matroids and partition matroids. An algorithm is given for finding a minimum spanning tree where a vertex r has prespecified degree; it shows this problem is equivalent to finding a minimum spanning tree, without the degree constraint. An algorithm is given for finding a minimum spanning tree on a directed graph, where the given root r has prespecified degree; the time is O(m log n), the same as for the problem without the degree constraint."
1979,The recognition of Series Parallel digraphs.,We present an algorithm that recognizes the class of General Series Parallel digraphs and runs in time proportional to the size of its input. To perform this recognition task it is necessary to compute the transitive reduction and transitive closure of any General Series Parallel digraph. Our analysis is based on the relationship between General Series Parallel digraphs and a class of well known models of electrical networks.
1979,The Pebbling Problem is Complete in Polynomial Space.,We examine a pebbling problem which has been used to study the storage requirements of various models of computation. Sethi has shown this problem to be NP-hard and Lingas has shown a generalization to be P-space complete. We prove the original problem P-space complete by employing a modification of Lingas's proof. The pebbling problem is one of the few examples of a P-space complete problem not exhibiting any obvious quantifier alternation.
1979,Upper and Lower Bounds on Time-Space Tradeoffs.,"This paper derives asymptotically tight bounds on the time-space tradeoffs for pebbling three different classes of directed acyclic graphs. Let N be the size of the graph, S the number of available pebbles, and T the time necessary for pebbling the graph. (a) A time space tradeoff of the form ST &equil; &thgr;(N2) is proved for a special class of permutation graphs which implement the bit reversal permutation. (b) A time-space tradeoff of the form T &equil; S &thgr;(N/S)&thgr;(N/S) is proved for a class of graphs constructed by stacking superconcentrators in series. (c) A time-space tradeoff of the form T &equil; S.22&thgr;(N/S)is proved for pebbling general directed acyclic graphs."
1978,Time-Space Trade-Offs in a Pebble Game.,"Summary
A certain pebble game on graphs has been studied in various contexts as a model for the time and space requirements of computations [1,2,3,8]. In this note it is shown that there exists a family of directed acyclic graphs Gn and constants c1, c2, c3 such that
(1)
Gn has n nodes and each node in Gn has indegree at most 2.
 (2)
Each graph Gn can be pebbled with c1âˆšn pebbles in n moves.
 (3)
Each graph Gn can also be pebbled with c2âˆšn pebbles, c2<c1, but every strategy which achieves this has at least 2 c 3âˆšn moves.
 "
1978,Triangulating a Simple Polygon.,n/a
1978,A Linear-Time Algorithm for Finding All Feedback Vertices.,n/a
1978,A Representation for Linear Lists with Movable Fingers.,"This paper describes a data structure which is useful for representing linear lists when the pattern of accesses to a list exhibits a (perhaps time-varying) locality of reference. The structure has many of the properties of the representation proposed by Guibas, McCreight, Plass, and Roberts [4], but is substantially simpler and may be practical for lists of moderate size. The analysis of our structure includes a general treatment of the worst-case node splitting caused by consecutive insertions into a 2-3 tree."
1977,Space Bounds for a Game on Graphs.,"We study a one-person game played by placing pebbles, according to certain rules, on the vertices of a directed graph. In [3] it was shown that for each graph withn vertices and maximum in-degreed, there is a pebbling strategy which requires at mostc(d) n/logn pebbles. Here we show that this bound is tight to within a constant factor. We also analyze a variety of pebbling algorithms, including one which achieves the 0(n/logn) bound.
"
1977,Finding optimum branchings.,"Chu and Liu, Edmonds, and Bock have independently devised an efficient algorithm to find an optimum branching in a directed graph. We give an implementation of the algorithm which runs in 0(m logn) time if the problem graph has n vertices and m edges. A modification for dense graphs gives a running time of 0(n2). We also show that the unmodified algorithm runs in 0(n(log n)2 +m) time on an average graph, assuming a uniform probability distribution.
"
1977,Finding a Maximum Independent Set.,"We present an algorithm which finds a maximum independent set in an n-vertex graph in $O(2^{n/3})$ time. The algorithm can thus handle graphs roughly three times as large as could be analyzed using a naive algorithm.

"
1977,Application of a Planar Separator Theorem.,"Abstract:
Any n-vertex planar graph has the property that it can be divided into components of roughly equal size by removing only O(âˆšn) vertices. This separator theorem, in combination with a divide-and-conquer strategy, leads to many new complexity results for planar graph problems. This paper describes some of these results."
1977,Time-Space Trade-Offs in a Pebble Game.,"Abstract
A certain pebble game on graphs has been studied in various contexts as a model for time and space requirements of computations [1,2,3,7]. In this note it is shown that there exists a family of directed acyclic graphs Gn and constants c1,c2,c3 such that
1)
Gn has n nodes and each node in Gn has indegree at most 2.
 2)
Each graph Gn can be pebbled with c_1 \sqrt n
pebbles in n moves.
 3)
Each graph Gn can also be pebbled with c_2 \sqrt n
pebbles, c2 < c1,
 but every strategy which achieves this has at least 2^{c_3 \sqrt n }
moves."
1977,Reference Machines Require Non-linear Time to Maintain Disjoint Sets.,"This paper describes a machine model intended to be useful in deriving realistic complexity bounds for tasks requiring list processing. As an example of the use of the model, the paper shows that any such machine requires non-linear time in the worst case to compute unions of disjoint sets on-line. All set union algorithms known to the author are instances of the model and are thus subject to the derived bound. One of the known algorithms achieves the bound to within a constant factor."
1976,Edge-Disjoint Spanning Trees and Depth-First Search.,"This paper presents an algorithm for finding two edge-disjoint spanning trees rooted at a fixed vertex of a directed graph. The algorithm uses depthfirst search and an efficient method for computing disjoint set unions. It requires O (e?(e, n)) time and O(e) space to analyze a graph with n vertices and e edges, where ? (e, n) is a very slowly growing function related to a functional inverse of Ackermann's function."
1976,Lower bounds on the lengths of node sequences in directed graphs.,"A strong node sequence for a directed graph G=(N,A) is a sequence of nodes containing every cycle-free path of G as a subsequence. A weak node sequence for G is a sequence of nodes containing every basic path in G as a subsequence, where a basic path n1, n2, â€¦, nk is a path from n1 to nk such that no proper subsequence is a path from n1 to nk. (Every strong node sequence for G is a weak node sequence for G.) Kennedy has developed a global program data flow analysis method using node sequences. Kwiatowski and Kleitman have shown that any strong node sequence for the complete graph on n nodes must have length at least n2âˆ’O(n7/4+Î±), for arbitrary positive Îµ. Every graph on n nodes has a strong sequence of length n2â€“2n+4, so this bound is tight to within O(n7/4+Î±). However, the complete graph on n nodes has a weak node sequence of length n nodes (all with in-degree and out-degree bounded by two) such that any weak node sequence for G has length at least 1/2 log2 nâˆ’O(n log log n). Aho and Ullman have shown that every reducible flow graph has a strong node sequence of length O(n log2 n); thus our bound is tight to within a constant factor for reducible graphs. We also show that for infinitely many n, there is a (non-reducible) flow graph H with n nodes (all with in-degree and out-degree bounded by two), such that any weak node sequence for H has length at least cn2, where c is a positive constant. This bound, too, is tight to within a constant factor."
1976,A Combinatorial Problem Which Is Complete in Polynomial Space.,"This paper considers a generalization, called the Shannon switching game on vertices, of a familiar board game called Hex. It is shown that determining who wins such a game if each player plays perfectly is very hard; in fact, if this game problem is solvable in polynomial time, then any problem solvable in polynomial space is solvable in polynomial time. This result suggests that the theory of combinational games is difficult."
1976,Intersection graphs of curves in the plane.,"Abstract
Let V be a set of curves in the plane. The corresponding intersection graph has V as the set of vertices, and two vertices are connected by an edge if and only if the two corresponding curves intersect in the plane.
It is shown that the set of intersection graphs of curves in the plane is a proper subset of the set of all undirected graphs. Furthermore, the set of intersection graphs of straight line-segments is a proper subset of the set of the intersection graphs of curves in the plane. Finally, it is shown that for every k â‰¥ 3, the problem of determining whether an intersection graph of straight line-segments is k-colorable is NP-complete."
1976,b-Matchings in Trees.,"We develop linear-time algorithms to find maximum weighted and unweighted degree-constrained subgraphs (b-matchings) of a tree. We use a generalization of an algorithm for finding a maximum 2-matching in a tree.
"
1976,Algorithmic Aspects of Vertex Elimination on Graphs.,"We consider a graph-theoretic elimination process which is related to performing Gaussian elimination on sparse symmetric positive definite systems of linear equations. We give a new linear-time algorithm to calculate the fill-in produced by any elimination ordering, and we give two new related algorithms for finding orderings with special properties. One algorithm, based on breadth-first search, finds a perfect elimination ordering, if any exists, in $O(n + e)$ time, if the problem graph has n vertices and e edges. An extension of this algorithm finds a minimal (but not necessarily minimum) ordering in $O(ne)$ time. We conjecture that the problem of finding a minimum ordering is NP-complete
"
1976,Augmentation Problems.,"This paper considers problems in which the object is to add a minimum-weight set of edges to a graph so as to satisfy a given connectivity condition. Simple characterizations of the minimum number of edges necessary to make a directed graph strongly connected and to make an undirected graph bridge-connected or biconnected are given. Efficient algorithms for finding such minimum sets of edges are discussed. It is shown that the weighted versions of these problems are NP-complete.
"
1976,The Planar Hamiltonian Circuit Problem is NP-Complete.,"We consider the problem of determining whether a planar, cubic, triply-connected graph G has a Hamiltonian circuit. We show that this problem is NP-complete. Hence the Hamiltonian circuit problem for this class of graphs, or any larger class containing all such graphs, is probably computationally intractable.



"
1976,Finding Minimum Spanning Trees.,"This paper studies methods for finding minimum spanning trees in graphs. Results include 1. several algorithms with $O(m\log \log n)$ worst-case running times, where n is the number vertices and m is the number of edges in the problem graph; 2. an $O(m)$ worst-case algorithm for dense graphs (those for which m is $\Omega (n^{1 + \varepsilon } )$ for some positive constant $\varepsilon $); 3. an $O(n)$ worst-case algorithm for planar graphs; 4. relationships with other problems which might lead general lower bound for the complexity of the minimum spanning tree problem.
"
1976,Computing an  st  -Numbering.,"Abstract
Lempel, Even and Cederbaum proved the following result: Given any edge {st} in a biconnected graph G with n vertices, the vertices of G can be numbered from 1 to n so that vertex s receives number 1, vertex t receives number n, and any vertex except s and t is adjacent both to a lower-numbered and to a higher-numbered vertex (we call such a numbering an st-numbering for G). They used this result in an efficient algorithm for planarity-testing. Here we provide a linear-time algorithm for computing an st-numbering for any biconnected graph. This algorithm can be combined with some new results by Booth and Lueker to provide a linear-time implementation of the Lempel-Even-Cederbaum planarity-testing algorithm."
1976,Space Bounds for a Game of Graphs.,"We study a one-person game played by placing pebbles, according to certain rules, on the vertices of a directed graph. In [3] it was shown that for each graph with n vertices and maximum in-degree d , there is a pebbling strategy which requires at most c(d) n/log n pebbles. Here we show that this bound is tight to within a constant factor. We also analyze a variety of pebbling algorithms, including one which achieves the 0(n/log n) bound."
1975,Optimal Chain Partitions of Trees.,n/a
1975,Efficiency of a Good But Not Linear Set Union Algorithm.,"Two types of instructions for manipulating a family of disjoint sets which partition a universe of n elements are considered. FIND(x) computes the name of the unique set containing element x UNION(A,B,C) combines sets A and B into a new set named C. A known algorithm for implementing sequences of these instructions is examined. It is shown that if t(m, n) is the maximum time required by a sequence of FINDs and n - 1 intermixed UNIONs, then for some positive constants where is related to a functional inverse of Ackermann's function and is very slow growing."
1975,Network Flow and Testing Graph Connectivity.,"An algorithm of Dinic for finding the maximum flow in a network is described. It is then shown that if the vertex capacities are all equal to one, the algorithm requires at most $O(|V|^{1/2} \cdot |E|)$ time, and if the edge capacities are all equal to one, the algorithm requires at most $O(|V|^{2/3} \cdot |E|)$ time. Also, these bounds are tight for Dinic’s algorithm.

These results are used to test the vertex connectivity of a graph in $O(|V|^{1/2} \cdot |E|^2 )$ time and the edge connectivity in $O(|V|^{5/3} \cdot |E|)$ time.
"
1975,a Combinatorial Problem which is Complete in Polynomial Space.,"We consider a generalization, which we call the Shannon switching game on vertices, of a familiar board game called HEX. We show that determining who wins such a game if each player plays perfectly is very hard; in fact, it is as hard as carrying out any polynomial-space-bounded computation. This result suggests that the theory of combinatorial games is difficult."
1975,Algorithmic Aspects of Vertex Elimination.,"We consider a graph-theoretic elimination process which is related to performing Gaussian elimination on sparse symmetric and unsymmetric systems of linear equations. We discuss good algorithms for finding elimination orderings, showing that a generalization of breadth-first search, called lexicographic search, can be used to find perfect orderings in 0(n+e) time and minimal orderings in 0(ne) time, if the problem graph is undirected and has n vertices and e edges. We also give efficient (though slower) algorithms for generating such orderings on directed graphs. We claim that the minimum ordering problem for directed graphs is NP-complete, and conjecture that it is also NP-complete for undirected graphs. We include a brief discussion of the relation of elimination to transitive closure and discuss some unresolved, more general, issues."
1974,A Note on Finding the Bridges of a Graph.,n/a
1974,A New Algorithm for Finding Weak Components.,n/a
1974,A Good Algorithm for Edge-Disjoint Branching.,n/a
1974,Efficient Planarity Testing.,"This paper describes an efficient algorithm to determine whether an arbitrary graph G can be embedded in the plane. The algorithm may be viewed as an iterative version of a method originally proposed by Auslander and Parter and correctly formulated by Goldstein. The algorithm used depth-first search and has O(V) time and space bounds, where V is the number of vertices in G. An ALGOL implementation of the algorithm succesfully tested graphs with as many as 900 vertices in less than 12 seconds."
1974,Testing Flow Graph Reducibility.,"Many problems in program optimization have been solved by applying a technique called interval analysis to the flow graph of the program. A flow graph which is susceptible to this type of analysis is called reducible. This paper describes an algorithm for testing whether a flow graph is reducible. The algorithm uses depth-first search to reveal the structure of the flow graph and a good method for computing disjoint set unions to determine reducibility from the search information. When the algorithm is implemented on a random access computer, it requires O(E log*E) time to analyze a graph with E edges, where log*x=min{iâ€–log(i)xâ‰¤1}. The time bound compares favorably with the O(E log E) bound of a previously known algorithm."
1974,Finding Dominators in Directed Graphs.,"This paper describes an algorithm for finding dominators in an arbitrary directed graph. The algorithm uses depth-first search and efficient algorithms for computing disjoint set unions and manipulating priority queues to achieve a time bound of $O(V\log V + E)$ if V is the number of vertices and E is the number of edges in the graph. This bound compares favorably with the $O(V(V + E))$ time bound of previously known algorithms for finding dominators in arbitrary directed graphs, and with the $O(V + E\log E)$ time bound of a known algorithm for finding dominators in reducible graphs. If $E \geqq V\log V$, the new algorithm requires $O(E)$ time and is optimal to within a constant factor.
"
1974,Testing Graph Connectivity.,"An algorithm proposed by Dinic for finding maximum flows in networks and by Hopcroft and Karp for finding maximum bipartite matchings is applied to graph connectivity problems. It is shown that the algorithm requires 0(V1/2E) time to find a maximum set of node-disjoint paths in a graph, and 0(V2/3E) time to find a maximum set of edge disjoint paths. These bounds are tight. Thus the node connectivity of a graph may be tested in 0(V5/2E) time, and the edge connectivity of a graph may be tested in 0(V5/3E) time."
1973,Efficient Algorithms for Graph Manipulation [H] (Algorithm 447).,"Efficient algorithms are presented for partitioning a graph into connected components, biconnected components and simple paths. The algorithm for partitioning of a graph into simple paths of iterative and each iteration produces a new path between two vertices already on paths. (The start vertex can be specified dynamically.) If V is the number of vertices and E is the number of edges, each algorithm requires time and space proportional to max (V, E) when executed on a random access computer."
1973,A V log V Algorithm for Isomorphism of Triconnected Planar Graphs.,An algorithm for determining whether two triconnected planar graphs are isomorphic is presented. The asymptotic growth rate of the algorithm is bounded by a constant times |V| log |V| where |V| is the number of vertices in the graphs.
1973,Time Bounds for Selection.,"The number of comparisons required to select the i-th smallest of n numbers is shown to be at most a linear function of n by analysis of a new selection algorithmâ€”PICK. Specifically, no more than 5.4305 n comparisons are ever required. This bound is improved for extreme values of i, and a new lower bound on the requisite number of comparisons is also proved."
1973,Dividing a Graph into Triconnected Components.,"An algorithm for dividing a graph into triconnected components is presented. When implemented on a random access computer, the algorithm requires $O(V + E)$ time and space to analyze a graph with V vertices and E edges. The algorithm is both theoretically optimal to within a constant factor and efficient in practice.


"
1973,Enumeration of the Elementary Circuits of a Directed Graph.,"An algorithm to enumerate all the elementary circuits of a directed graph is presented. The algorithm is based on a backtracking procedure of Tiernan, but uses a lookahead and labeling technique to avoid unnecessary work. It has a time bound of $O((V \cdot E)(C + 1))$ when applied to a graph with V vertices, E edges, and C elementary circuits.



"
1973,Testing Flow Graph Reducibility.,"Many problems in program optimization have been solved by applying a technique called interval analysis to the flow graph of the program. A flow graph which is susceptible to this type of analysis is called reducible. This paper describes an algorithm for testing whether a flow graph is reducible. The algorithm uses depth-first search to reveal the structure of the flow graph and a good method for computing disjoint set unions to determine reducibility from the search information. When the algorithm is implemented on a random access computer, it requires O(E log* E) time to analyze a graph with E edges, where log* x = min{i/logixâ‰¤1}. The time bound compares favorably with the O(E log E) bound of a previously known algorithm."
1972,Determining Whether a Groupoid is a Group.,n/a
1972,Sorting Using Networks of Queues and Stacks.,
1972,Depth-First Search and Linear Graph Algorithms.,"The value of depth-first search or “backtracking” as a technique for solving problems is illustrated by two examples. An improved version of an algorithm for finding the strongly connected components of a directed graph and at algorithm for finding the biconnected components of an undirect graph are presented. The space and time requirements of both algorithms are bounded by $k_1 V + k_2 E + k_3 $ for some constants $k_1 ,k_2 $, and $k_3 $, where V is the number of vertices and E is the number of edges of the graph being examined.
"
1972,Isomorphism of Planar Graphs.,"Abstract
An algorithm is presented for determining whether or not two planar graphs are isomorphic. The algorithm requires O(V log V) time, if V is the number of vertices in each graph."
1972,Linear Time Bounds for Median Computations.,"New upper and lower bounds are presented for the maximum number of comparisons, f(i,n), required to select the i-th largest of n numbers. An upper bound is found, by an analysis of a new selection algorithm, to be a linear function of n: f(i,n) â‰¤ 103n/18 < 5.73n, for 1 â‰¤ i â‰¤ n. A lower bound is shown deductively to be: f(i,n) â‰¥ n+min(i,nâˆ’i+l) + [log2(n)] âˆ’ 4, for 2 â‰¤ i â‰¤ nâˆ’1, or, for the case of computing medians: f([n/2],n) â‰¥ 3n/2 âˆ’ 3"
1971,A VÂ² Algorithm for Determining Isomorphism of Planar Graphs.,n/a
1971,Depth-First Search and Linear Graph Algorithms (Working Paper).,"Abstract:
The value of depth-first search or ""backtracking"" as a technique for solving graph problems is illustrated by two examples. An algorithm for finding the biconnected components of an undirected graph and an improved version of an algorithm for finding the strongly connected components of a directed graph are presented. The space and time requirements of both algorithms are bounded by k1V + k2E + k3 for some constants k1, k2, and k3, where V is the number of vertices and E is the number of edges of the graph being examined."
1971,Planarity Testing in V log V Steps: Extended Abstract.,n/a
