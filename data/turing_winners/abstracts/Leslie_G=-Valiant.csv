2018,Some observations on holographic algorithms.,"We define the notion of diversity for families of finite functions and express the limitations of a simple class of holographic algorithms, called elementary algorithms, in terms of limitations on diversity. We show that this class of elementary algorithms is too weak to solve the Boolean circuit value problem, or Boolean satisfiability, or the permanent. The lower bound argument is a natural but apparently novel combination of counting and algebraic dependence arguments that is viable in the holographic framework. We go on to describe polynomial time holographic algorithms that go beyond the elementarity restriction in the two respects that they use exponential size fields, and multiple oracle calls in the form of polynomial interpolation. These new algorithms, which use bases of three components, compute the parity of the following quantities for degree three planar undirected graphs: the number of 3-colorings up to permutation of colors, the number of connected vertex covers, and the number of induced forests or feedback vertex sets. In each case, the parity can also be computed for any one slice of the problem, in particular for colorings where the first color is used a certain number of times, or where the connected vertex cover, feedback set or induced forest has a certain number of nodes.
"
2017,Capacity of Neural Networks for Lifelong Learning of Composable Tasks.,"Abstract:
We investigate neural circuits in the exacting setting that (i) the acquisition of a piece of knowledge can occur from a single interaction, (ii) the result of each such interaction is a rapidly evaluatable subcircuit, (iii) hundreds of thousands of such subcircuits can be acquired in sequence without substantially degrading the earlier ones, and (iv) recall can be in the form of a rapid evaluation of a composition of subcircuits that have been so acquired at arbitrary different earlier times.We develop a complexity theory, in terms of asymptotically matching upper and lower bounds, on the capacity of a neural network for executing, in this setting, the following action, which we call {\it association}: Each action sets up a subcircuit so that the excitation of a chosen set of neurons A will in future cause the excitation of another chosen set B.% As model of computation we consider the neuroidal model, a fully distributed model in which the quantitative resources n, the neuron numbers, d, the number of other neurons each neuron is connected to, and k, the inverse of the maximum synaptic strength, are all accounted for.A succession of experiences, possibly over a lifetime, results in the realization of a complex set of subcircuits. The composability requirement constrains the model to ensure that, for each association as realized by a subcircuit, the excitation in the triggering set of neurons A is quantitatively similar to that in the triggered set B, and also that the unintended excitation in the rest of the system is negligible. These requirements ensure that chains of associations can be triggeredWe first analyze what we call the Basic Mechanism, which uses only direct connections between neurons in the triggering set A and the target set B. We consider random networks of n neurons with expected number d of connections to and from each. We show that in the composable context capacity growth is limited by d2, a severe limitation if the network is sparse, as it is...
(View more)"
2013,The Complexity of Symmetric Boolean Parity Holant Problems.,"For certain subclasses of NP, $\oplus$P, or #P characterized by local constraints, it is known that if there exist any problems within that subclass that are not polynomial time computable, then all the problems in the subclass are NP-complete, $\oplus$P-complete, or #P-complete. Such dichotomy results have been proved for characterizations such as constraint satisfaction problems and directed and undirected graph homomorphism problems, often with additional restrictions. Here we give a dichotomy result for the more expressive framework of Holant problems. For example, these additionally allow for the expression of matching problems, which have had pivotal roles in the development of complexity theory. As our main result we prove the dichotomy theorem that, for the class $\oplus$P, every set of symmetric Holant signatures of any arities that is not polynomial time computable is $\oplus$P-complete. The result exploits some special properties of the class $\oplus$P and characterizes four distinct tractable subclasses within $\oplus$P. It leaves open the corresponding questions for NP, $\#$P, and $\#_k$P for $k\neq 2$.
"
2012,The Hippocampus as a Stable Memory Allocator for Cortex.,"It is suggested here that mammalian hippocampus serves as an allocator of neurons in cortex for memorizing new items. A construction of a shallow feedforward network with biologically plausible parameters is given that possesses the characteristics needed for such an allocator. In particular, the construction is stabilizing in that for inputs within a range of activity levels spanning more than an order of magnitude, the output will have activity levels differing as little as 1%. It is also noise tolerant in that pairs of input patterns that differ little will generate output patterns that differ little. Further, pairs of inputs that differ by much will be mapped to outputs that also differ sufficiently that they can be treated by cortex as distinct.
"
2012,An Algorithmic View of the Universe.,"In the years since Alan Turing, and following his lead, computer scientists advanced their understanding of computational phenomena by developing a very specialized, original and penetrating way of rigorous thinking. Now it turns out that this ""algorithmic"" way of thinking can be applied productively to the study of important phenomena outside computation proper (examples: the cell, the brain, the market, the universe, indeed mathematical truth itself). This development is an exquisite unintended consequence of the fact that there is latent computation underlying each of these phenomena, or the ways in which science studies them."
2011,A bridging model for multi-core computing.,"Abstract
Writing software for one parallel system is a feasible though arduous task. Reusing the substantial intellectual effort so expended for programming a second system has proved much more challenging. In sequential computing algorithms textbooks and portable software are resources that enable software systems to be written that are efficiently portable across changing hardware platforms. These resources are currently lacking in the area of multi-core architectures, where a programmer seeking high performance has no comparable opportunity to build on the intellectual efforts of others. In order to address this problem we propose a bridging model aimed at capturing the most basic resource parameters of multi-core architectures. We suggest that the considerable intellectual effort needed for designing efficient algorithms for such architectures may be most fruitfully expended in designing portable algorithms, once and for all, for such a bridging model. Portable algorithms would contain efficient designs for all reasonable combinations of the basic resource parameters and input sizes, and would form the basis for implementation or compilation for particular machines. Our Multi-BSP model is a multi-level model that has explicit parameters for processor numbers, memory/cache sizes, communication costs, and synchronization costs. The lowest level corresponds to shared memory or the PRAM, acknowledging the relevance of that model for whatever limitations on memory and processor numbers it may be efficacious to emulate it. We propose parameter-aware portable algorithms that run efficiently on all relevant architectures with any number of levels and any combination of parameters. For these algorithms we define a parameter-free notion of optimality. We show that for several fundamental problems, including standard matrix multiplication, the Fast Fourier Transform, and comparison sorting, there exist optimal portable algorithms in that sense, for all combinations of machine parameters. Thus some algorithmic generality and elegance can be found in this many parameter setting."
2011,The Complexity of Symmetric Boolean Parity Holant Problems - (Extended Abstract).,"Abstract
For certain subclasses of NP, ‚äïP or #P characterized by local constraints, it is known that if there exist any problems that are not polynomial time computable within that subclass, then those problems are NP-, ‚äïP- or #P-complete. Such dichotomy results have been proved for characterizations such as Constraint Satisfaction Problems, and directed and undirected Graph Homomorphism Problems, often with additional restrictions. Here we give a dichotomy result for the more expressive framework of Holant Problems. These additionally allow for the expression of matching problems, which have had pivotal roles in complexity theory. As our main result we prove the dichotomy theorem that, for the class ‚äïP, every set of boolean symmetric Holant signatures of any arities that is not polynomial time computable is ‚äïP-complete. The result exploits some special properties of the class ‚äïP and characterizes four distinct tractable subclasses within ‚äïP. It leaves open the corresponding questions for NP, #P and # k P for k‚Äâ‚â†‚Äâ2."
2010,Evolution with Drifting Targets.,"We consider the question of the stability of evolutionary algorithms to gradual changes, or drift, in the target concept. We define an algorithm to be resistant to drift if, for some inverse polynomial drift rate in the target function, it converges to accuracy 1 ? ? with polynomial resources, and then stays within that accuracy indefinitely, except with probability ? at any one time. We show that every evolution algorithm, in the sense of Valiant [20], can be converted using the Correlational Query technique of Feldman [9], into such a drift resistant algorithm. For certain evolutionary algorithms, such as for Boolean conjunctions, we give bounds on the rates of drift that they can resist. We develop some new evolution algorithms that are resistant to significant drift. In particular, we give an algorithm for evolving linear separators over the spherically symmetric distribution that is resistant to a drift rate of O(?/n), and another algorithm over the more general product normal distributions that resists a smaller drift rate. The above translation result can be also interpreted as one on the robustness of the notion of evolvability itself under changes of definition. As a second result in that direction we show that every evolution algorithm can be converted to a quasi-monotonic one that can evolve from any starting point without the performance ever dipping significantly below that of the starting point. This permits the somewhat unnatural feature of arbitrary performance degradations to be removed from several known robustness translations."
2010,Some Observations on Holographic Algorithms.,"Abstract
We define the notion of diversity for families of finite functions, and express the limitations of a simple class of holographic algorithms in terms of limitations on diversity. We go on to describe polynomial time holographic algorithms for computing the parity of the following quantities for degree three planar undirected graphs: the number of 3-colorings up to permutation of colors, the number of connected vertex covers, and the number of induced forests or feedback vertex sets. In each case the parity can be computed for any slice of the problem, in particular for colorings where the first color is used a certain number of times, or where the connected vertex cover, feedback set or induced forest has a certain number of nodes. These holographic algorithms use bases of three components, rather than two."
2009,Evolvability.,"Living organisms function in accordance with complex mechanisms that operate in different ways depending on conditions. Darwin's theory of evolution suggests that such mechanisms evolved through variation guided by natural selection. However, there has existed no theory that would explain quantitatively which mechanisms can so evolve in realistic population sizes within realistic time periods, and which are too complex. In this article, we suggest such a theory. We treat Darwinian evolution as a form of computational learning from examples in which the course of learning is influenced only by the aggregate fitness of the hypotheses on the examples, and not otherwise by specific examples. We formulate a notion of evolvability that distinguishes function classes that are evolvable with polynomially bounded resources from those that are not. We show that in a single stage of evolution monotone Boolean conjunctions and disjunctions are evolvable over the uniform distribution, while Boolean parity functions are not. We suggest that the mechanism that underlies biological evolution overall is ‚Äúevolvable target pursuit‚Äù, which consists of a series of evolutionary stages, each one inexorably pursuing an evolvable target in the technical sense suggested above, each such target being rendered evolvable by the serendipitous combination of the environment and the outcomes of previous evolutionary stages."
2009,Experience-Induced Neural Circuits That Achieve High Capacity.,"Over a lifetime, cortex performs a vast number of different cognitive actions, mostly dependent on experience. Previously it has not been known how such capabilities can be reconciled, even in principle, with the known resource constraints on cortex, such as low connectivity and low average synaptic strength. Here we describe neural circuits and associated algorithms that respect the brain's most basic resource constraints and support the execution of high numbers of cognitive actions when presented with natural inputs. Our circuits simultaneously support a suite of four basic kinds of task, each requiring some circuit modification: hierarchical memory formation, pairwise association, supervised memorization, and inductive learning of threshold functions. The capacity of our circuits is established by experiments in which sequences of several thousand such actions are simulated by computer and the circuits created tested for subsequent efficacy. Our underlying theory is apparently the only biologically plausible systems-level theory of learning and memory in cortex for which such a demonstration has been performed, and we argue that no general theory of information processing in the brain can be considered viable without such a demonstration.
"
2009,Neural Computations That Support Long Mixed Sequences of Knowledge Acquisition Tasks.,"Abstract
In this talk we shall first give a brief review of a quantitative approach to understanding neural computation [4-6]. We target so-called random access tasks, defined as those in which one instance of a task execution may need to access arbitrary combinations of items in memory. Such tasks are communication intensive, and therefore the known severe constraints on connectivity in the brain can inform their analysis."
2008,Holographic Algorithms.,"Complexity theory is built fundamentally on the notion of efficient reduction among computational problems. Classical reductions involve gadgets that map solution fragments of one problem to solution fragments of another in one-to-one, or possibly one-to-many, fashion. In this paper we propose a new kind of reduction that allows for gadgets with many-to-many correspondences, in which the individual correspondences among the solution fragments can no longer be identified. Their objective may be viewed as that of generating interference patterns among these solution fragments so as to conserve their sum. We show that such holographic reductions provide a method of translating a combinatorial problem to finite systems of polynomial equations with integer coefficients such that the number of solutions of the combinatorial problem can be counted in polynomial time if one of the systems has a solution over the complex numbers. We derive polynomial time algorithms in this way for a number of problems for which only exponential time algorithms were known before. General questions about complexity classes can also be formulated. If the method is applied to a #P-complete problem, then polynomial systems can be obtained, the solvability of which would imply P$^{{\tiny{#P}}}$ = NC2.
"
2008,The Learning Power of Evolution.,n/a
2008,A Bridging Model for Multi-core Computing.,"Abstract
We propose a bridging model aimed at capturing the most basic resource parameters of multi-core architectures. We suggest that the considerable intellectual effort needed for designing efficient algorithms for such architectures may be most fruitfully pursued as an effort in designing portable algorithms for such a bridging model. Portable algorithms would contain efficient designs for all reasonable ranges of the basic resource parameters and input sizes, and would form the basis for implementation or compilation for particular machines."
2008,Knowledge Infusion: In Pursuit of Robustness in Artificial Intelligence.,"Endowing computers with the ability to apply commonsense knowledge with human-level performance is a primary challenge for computer science, comparable in importance to past great challenges in other fields of science such as the sequencing of the human genome. The right approach to this problem is still under debate. Here we shall discuss and attempt to justify one approach, that of {\it knowledge infusion}. This approach is based on the view that the fundamental objective that needs to be achieved is {\it robustness} in the following sense: a framework is needed in which a computer system can represent pieces of knowledge about the world, each piece having some uncertainty, and the interactions among the pieces having even more uncertainty, such that the system can nevertheless reason from these pieces so that the uncertainties in its conclusions are at least controlled. In knowledge infusion rules are learned from the world in a principled way so that subsequent reasoning using these rules will also be principled, and subject only to errors that can be bounded in terms of the inverse of the effort invested in the learning process."
2008,A First Experimental Demonstration of Massive Knowledge Infusion.,"A central goal of Artificial Intelligence is to create systems that embody commonsense knowledge in a reliable enough form that it can be used for reasoning in novel situations. Knowledge Infusion is an approach to this problem in which the commonsense knowledge is acquired by learning. In this paper we report on experiments on a corpus of a half million sentences of natural language text that test whether commonsense knowledge can be usefully acquired through this approach. We examine the task of predicting a deleted word from the remainder of a sentence for some 268 target words. As baseline we consider how well this task can be performed using learned rules based on the words within a fixed distance of the target word and their parts of speech. This captures an approach that has been previously demonstrated to be highly successful for a variety of natural language tasks. We then go on to learn from the corpus rules that embody commonsense knowledge, additional to the knowledge used in the baseline case. We show that chaining learned commonsense rules together leads to measurable improvements in prediction performance on our task as compared with the baseline. This is apparently the first experimental demonstration that commonsense knowledge can be learned from natural inputs on a massive scale reliably enough that chaining the learned rules is efficacious for reasoning.
"
2007,Evolvability.,"Abstract
Living organisms function according to complex mechanisms that operate in different ways depending on conditions. Evolutionary theory suggests that such mechanisms evolved as result of a random search guided by selection. However, there has existed no theory that would explain quantitatively which mechanisms can so evolve in realistic population sizes within realistic time periods, and which are too complex. In this paper we suggest such a theory. Evolution is treated as a form of computational learning from examples in which the course of learning is influenced only by the fitness of the hypotheses on the examples, and not otherwise by the specific examples. We formulate a notion of evolvability that quantifies the evolvability of different classes of functions. It is shown that in any one phase of evolution where selection is for one beneficial behavior, monotone Boolean conjunctions and disjunctions are demonstrably evolvable over the uniform distribution, while Boolean parity functions are demonstrably not. The framework also allows a wider range of issues in evolution to be quantified. We suggest that the overall mechanism that underlies biological evolution is evolvable target pursuit, which consists of a series of evolutionary stages, each one pursuing an evolvable target in our technical sense, each target being rendered evolvable by the serendipitous combination of the environment and the outcome of previous evolutionary stages."
2006,A Quantitative Theory of Neural Computation.,"We show how a general quantitative theory of neural computation can be used to explain two recent experimental findings in neuroscience. The first of these findings is that in human medial temporal lobe there exist neurons that correspond to identifiable concepts, such as a particular actress. Further, even when such concepts are preselected by the experimenter, such neurons can be found with paradoxical ease, after examining relatively few neurons. We offer a quantitative computational explanation of this phenomenon, where apparently none existed before. Second, for the locust olfactory system estimates of the four parameters of neuron numbers, synapse numbers, synapse strengths, and the numbers of neurons that represent an odor are now available. We show here that these numbers are related as predicted by the general theory. More generally, we identify two useful regimes for neural computation with distinct ranges of these quantitative parameters.
"
2006,Knowledge Infusion.,"The question of how machines can be endowed with the ability to acquire and robustly manipulate commonsense knowledge is a fundamental scientific problem. Here we formulate an approach to this problem that we call knowledge infusion. We argue that robust logic offers an appropriate semantics for this endeavor because it supports provably efficient algorithms for a basic set of necessary learning and reasoning tasks. We observe that multiple concepts can be learned simultaneously from a common data set in a data efficient manner. We also point out that the preparation of appropriate teaching materials for training systems constructed according to these principles raises new challenges.
"
2006,Accidental Algorithms.,"Abstract:
We provide evidence for the proposition that the computational complexity of individual problems, and of whole complexity classes, hinge on the existence of certain solvable polynomial systems that are unlikely to be encountered other than by systematic explorations for them. We consider a minimalist version of Cook's 3CNF problem, namely that of monotone planar 3 CNF formulae where each variable occurs twice. We show that counting the number of solutions of these modulo 2 is oplusP-complete (hence NP-hard) but counting them modulo 7 is polynomial time computable (sic). We also show a similar dichotomy for a vertex cover problem. To derive completeness results we use a new holographic technique for proving completeness results in oplusP for problems that are in P. For example, we can show in this way that oplus2CNF, the parity problem for 2CNF, is oplusP-complete. To derive efficient algorithms we use computer algebra systems to find appropriate holographic gates. In order to explore the limits of holographic techniques we define the notion of an elementary matchgrid algorithm to capture a natural but restricted use of them. We show that for the NP-complete general 3 CNF problem no such elementary matchgrid algorithm can exist. We observe, however, that it remains open for many natural #P-complete problems whether such elementary matchgrid algorithms exist, and for the general CNF problem whether non-elementary matchgrid algorithms exist"
2005,Memorization and Association on a Realistic Neural Model.,"A central open question of computational neuroscience is to identify the data structures and algorithms that are used in mammalian cortex to support successive acts of the basic cognitive tasks of memorization and association. This letter addresses the simultaneous challenges of realizing these two distinct tasks with the same data structure, and doing so while respecting the following four basic quantitative parameters of cortex: the neuron number, the synapse number, the synapse strengths, and the switching times. Previous work has not succeeded in reconciling these opposing constraints, the low values of synapse strengths that are typically observed experimentally having contributed a particular obstacle. In this article, we describe a computational scheme that supports both memory formation and association and is feasible on networks of model neurons that respect the widely observed values of the four quantitative parameters. Our scheme allows for both disjoint and shared representations. The algorithms are simple, and in one version both memorization and association require just one step of vicinal or neighborly influence. The issues of interference among the different circuits that are established, of robustness to noise, and of the stability of the hierarchical memorization process are addressed. A calculus therefore is implied for analyzing the capabilities of particular neural systems and subsystems, in terms of their basic numerical parameters.
"
2005,Completeness for Parity Problems.,"Abstract
In this talk we shall review recent work on holographic algorithms and circuits. This work can be interpreted as offering formulations of the question of whether computations within such complexity classes as NP, ‚äïP, BQP, or #P, can be efficiently computed classically using linear algebra. The central part of the theory is the consideration of gadgets that map simple combinatorial constraints into gates, assemblies of which can be evaluated efficiently using linear algebra. The combinatorial constraints that appear most fruitful to investigate are the simplest ones that correspond to problems complete in these complexity classes. With this motivation we shall in this note consider the parity class ‚äïP for which our understanding of complete problems is particularly limited. For example, among the numerous search problems for which the existence of solutions can be determined in P and the counting problem is known to be #P-complete, the #P-completeness proof does not generally translate to a ‚äïP-completeness proof. We observe that in one case it does, and enumerate several natural problems for which the complexity of parity is currently unresolved. We go on to consider two examples of NP-complete problems for which ‚äïP-completeness can be proved but is not immediate: Hamiltonian circuits for planar degree three graphs, and satisfiability of read-twice Boolean formulae."
2005,Holographic Circuits.,"Abstract
Holographic circuits are defined here to be circuits in which information is represented as linear superpositions. Holographic circuits when suitably formulated can be emulated on classical computers in polynomial time. The questions we investigate are those of characterizing the complexity classes of computations that can be expressed by holographic circuits."
2004,Holographic Algorithms (Extended Abstract).,"Abstract:
We introduce a new notion of efficient reduction among computational problems. Classical reductions involve gadgets that map local solutions of one problem to local solutions of another in one-to-one, or possibly many-to-one or one-to-many, fashion. Our proposed reductions allow for gadgets with many-to-many correspondences. Their objective is to preserve the sum of the local solutions. Such reductions provide a method of translating a combinatorial problem to a family of finite systems of polynomial equations with integer coefficients such that the number of solutions of the combinatorial problem can be counted in polynomial time if some system in the family has a solution over the complex numbers. We can derive polynomial time algorithms in this way for ten problems for which only exponential time algorithms were known before. General questions about complexity classes are also formulated. If the method is applied to a #P-complete problem then we obtain families of polynomial systems such that the solvability of any one member would imply P/sup #P/ = NC2."
2003,Three problems in computer science.,n/a
2002,Quantum Circuits That Can Be Simulated Classically in Polynomial Time.,"A model of quantum computation based on unitary matrix operations was introduced by Feynman and Deutsch. It has been asked whether the power of this model exceeds that of classical Turing machines. We show here that a significant class of these quantum computations can be simulated classically in polynomial time. In particular we show that two-bit operations characterized by 4 ◊ 4 matrices in which the sixteen entries obey a set of five polynomial relations can be composed according to certain rules to yield a class of circuits that can be simulated classically in polynomial time. This contrasts with the known universality of two-bit operations and demonstrates that efficient quantum computation of restricted classes is reconcilable with the Polynomial Time Turing Hypothesis. Therefore, it is possible that, The techniques introduced bring the quantum computational model within the realm of algebraic complexity theory. In a manner consistent with one view of quantum physics, the wave function is simulated deterministically, and randomization arises only in the course of making measurements. The results generalize the quantum model in that they do not require the matrices to be unitary. In a different direction these techniques also yield deterministic polynomial time algorithms for the decision and parity problems for certain classes of read-twice Boolean formulae. All our results are based on the use of gates that are defined in terms of their graph matching properties.
"
2002,Expressiveness of matchgates.,"Abstract
Matchgates have been used to simulate classically in polynomial time assemblies of quantum gates. In this paper it is shown that every matchgate for 2-input 2-output functions has to obey a certain set of five polynomial identities. It is also shown that no matchgate can realize a nontrivial control gate for any number of inputs. On the other hand, it is proved that classical Boolean formulae can be expressed as matchcircuits of polynomial size."
2001,Quantum computers that can be simulated classically in polynomial time.,"A model of quantum computation based on unitary matrix operations was introduced by Feynman and Deutsch. It has been asked whether the power of this model exceeds that of classical Turing machines. We show here that a significant class of these quantum computations can be simulated classically in polynomial time. In particular we show that two-bit operations characterized by 4 \times 4 matrices in which the sixteen entries obey a set of five polynomial relations can be composed according to certain rules to yield a class of circuits that can be simulated classically in polynomial time. This contrasts with the known universality of two-bit operations, and demonstrates that efficient quantum computation of restricted classes is reconcilable with the Polynomial Time Turing Hypothesis. In other words it is possible that quantum phenomena can be used in a scalable fashion to make computers but that they do not have superpolynomial speedups compared to Turing machines for any problem. The techniques introduced bring the quantum computational model within the realm of algebraic complexity theory. In a manner consistent will one view of quantum physics, the wave function is simulated deterministically, and randomization arises only in the course of making measurements. The results generalize the quantum model in that they do not require the matrices to be unitary. In a different direction these techniques also yield deterministic polynomial time algorithms for the decision and parity problems for certain classes of read-twice Boolean formulae. All our results are based on the use of gates that are defined in terms of their graph matching properties."
2000,Robust logics.,"Abstract
Suppose that we wish to learn from examples and counter-examples a criterion for recognizing whether an assembly of wooden blocks constitutes an arch. Suppose also that we have preprogrammed recognizers for various relationships, e.g., on-top-of
, above
, etc. and believe that some possibly complex expression in terms of these base relationships should suffice to approximate the desired notion of an arch. How can we formulate such a relational learning problem so as to exploit the benefits that are demonstrably available in propositional learning, such as attribute-efficient learning by linear separators, and error-resilient learning?
We believe that learning in a general setting that allows for multiple objects and relations in this way is a fundamental key to resolving the following dilemma that arises in the design of intelligent systems: Mathematical logic is an attractive language of description because it has clear semantics and sound proof procedures. However, as a basis for large programmed systems it leads to brittleness because, in practice, consistent usage of the various predicate names throughout a system cannot be guaranteed, except in application areas such as mathematics where the viability of the axiomatic method has been demonstrated independently.
In this paper we develop the following approach to circumventing this dilemma. We suggest that brittleness can be overcome by using a new kind of logic in which each statement is learnable. By allowing the system to learn rules empirically from the environment, relative to any particular programs it may have for recognizing some base predicates, we enable the system to acquire a set of statements approximately consistent with each other and with the world, without the need for a globally knowledgeable and consistent programmer.
We illustrate this approach by describing a simple logic that has a sound and efficient proof procedure for reasoning about instances, and that is rendered robust by having the rules learnable. The complexity and accuracy of both learning and deduction are provably polynomial bounded."
2000,A neuroidal architecture for cognitive computation.,"An architecture is described for designing systems that acquire and ma nipulate large amounts of unsystematized, or so-called commonsense, knowledge. Its aim is to exploit to the full those aspects of computational learning that are known to offer powerful solutions in the acquisition and maintenance of robust knowledge bases. The architecture makes explicit the requirements on the basic computational tasks that are to be performed and is designed to make this computationally tractable even for very large databases. The main claims are that (i) the basic learning and deduction tasks are provably tractable and (ii) tractable learning offers viable approaches to a range of issues that have been previously identified as problematic for artificial intelligence systems that are programmed. Among the issues that learning offers to resolve are robustness to inconsistencies, robustness to incomplete information and resolving among alternatives. Attribute-efficient learning algorithms, which allow learning from few examples in large dimensional systems, are fundamental to the approach. Underpinning the overall architecture is a new principled approach to manipulating relations in learning systems. This approach, of independently quantified arguments, allows propositional learning algorithms to be applied systematically to learning relational concepts in polynomial time and in modular fashion."
1999,Projection Learning.,"Abstract
A method of combining learning algorithms is described that preserves attribute-efficiency. It yields learning algorithms that require a number of examples that is polynomial in the number of relevant variables and logarithmic in the number of irrelevant ones. The algorithms are simple to implement and realizable on networks with a number of nodes linear in the total number of variables. They include generalizations of Littlestone's Winnow algorithm, and are, therefore, good candidates for experimentation on domains having very large numbers of attributes but where nonlinear hypotheses are sought."
1999,Relational Learning for NLP using Linear Threshold Elements.,"We describe a coherent view of learning and reasoning with relational representations in the context of natural language processing. In particular, we discuss the Neuroidal Architecture, Inductive Logic Programming and the SNoW system explaining the relationships among these, and thereby offer an explanation of the theoretical basis of the SNoW system. We suggest that extensions of this system along the lines suggested by the theory may provide new levels of scalability and functionality."
1999,Robust Logics.,"Suppose that we wish to learn from examples and counter-examples a criterion for recognizing whether an assembly of wooden blocks constitutes an arch. Suppose also that we have preprogrammed recognizers for various relationships, e.g., on-top-of, above, etc. and believe that some possibly complex expression in terms of these base relationships should suffice to approximate the desired notion of an arch. How can we formulate such a relational learning problem so as to exploit the benefits that are demonstrably available in propositional learning, such as attribute-efficient learning by linear separators, and error-resilient learning?

We believe that learning in a general setting that allows for multiple objects and relations in this way is a fundamental key to resolving the following dilemma that arises in the design of intelligent systems: Mathematical logic is an attractive language of description because it has clear semantics and sound proof procedures. However, as a basis for large programmed systems it leads to brittleness because, in practice, consistent usage of the various predicate names throughout a system cannot be guaranteed, except in application areas such as mathematics where the viability of the axiomatic method has been demonstrated independently.

In this paper we develop the following approach to circumventing this dilemma. We suggest that brittleness can be overcome by using a new kind of logic in which each statement is learnable. By allowing the system to learn rules empirically from the environment, relative to any particular programs it may have for recognizing some base predicates, we enable the system to acquire a set of statements approximately consistent with each other and with the world, without the need for a globally knowledgeable and consistent programmer.

We illustrate this approach by describing a simple logic that has a sound and efficient proof procedure for reasoning about instances, and that is rendered robust by having the rules learnable. The complexity and accuracy of both learning and deduction are provably polynomial bounded."
1998,Projection Learning.,"A method of combining learning algorithms is described that preserves attribute-efficiency. It yields learning algorithms that require a number of examples that is polynomial in the number of relevant variables and logarithmic in the number of irrelevant ones. The algorithms are simple to implement and realizable on networks with a number of nodes linear in the total number of variables. They include generalizations of Littlestone's Winnow algorithm, and are, therefore, good candidates for experimentation on domains having very large numbers of attributes but where nonlinear hypotheses are sought."
1998,A Neuroidal Architecture for Cognitive Computation.,"Abstract
An architecture is described for designing systems that acquire and manipulate large amounts of unsystematized, or so-called commonsense, knowledge. Its aim is to exploit to the full those aspects of computational learning that are known to offer powerful solutions in the acquisition and maintenance of robust knowledge bases. The architecture makes explicit the requirements on the basic computational tasks that are to be performed and is designed to make these computationally tractable even for very large databases. The main claims are that (i) the basic learning tasks are tractable and (ii) tractable learning offers viable approaches to a range of issues that have been previously identified as problematic for artificial intelligence systems that are entirely programmed. In particular, attribute efficiency holds a central place in the definition of the learning tasks, as does also the capability to handle relational information efficiently. Among the issues that learning offers to resolve are robustness to inconsistencies, robustness to incomplete information and resolving among alternatives."
1996,Managing Complexity in Neurodial Circuits.,n/a
1995,Rationality.,n/a
1995,Cognitive Computation (Extended Abstract).,"Abstract:
Cognitive computation is discussed as a discipline that links together neurobiology, cognitive psychology and artificial intelligence."
1995,Bulk synchronous parallel computing-a paradigm for transportable software.,"Abstract:
A necessary condition for the establishment, on a substantial basis, of a parallel software industry would appear to be the availability of technology for generating transportable software, i.e. architecture independent software which delivers scalable performance for a wide variety of applications on a wide range of multiprocessor computers. This paper describes H-BSP-a general purpose parallel computing environment for developing transportable algorithms. H-BSP is based on the Bulk Synchronous Parallel Model (BSP), in which a computation involves a number of supersteps, each having several parallel computational threads that synchronize at the end of the superstep. The BSP Model deals explicitly with the notion of communication among computational threads and introduces parameters g and L that quantify the ratio of communication throughput to computation throughput, and the synchronization period, respectively. These two parameters, together with the number of processors and the problem size, are used to quantify the performance and, therefore, the transportability of given classes of algorithms across machines having different values for these parameters. This paper describes the role of unbundled compiler technology in facilitating the development of such a parallel computer environment.< >"
1994,Cryptographic Limitations on Learning Boolean Formulae and Finite Automata.,"In this paper, we prove the intractability of learning several classes of Boolean functions in the distribution-free model (also called the Probably Approximately Correct or PAC model) of learning from examples. These results are representation independent, in that they hold regardless of the syntactic form in which the learner chooses to represent its hypotheses. Our methods reduce the problems of cracking a number of well-known public-key cryptosystems to the learning problems. We prove that a polynomial-time learning algorithm for Boolean formulae, deterministic finite automata or constant-depth threshold circuits would have dramatic consequences for cryptography and number theory. In particular, such an algorithm could be used to break the RSA cryptosystem, factor Blum integers (composite numbers equivalent to 3 modulo 4), and detect quadratic residues. The results hold even if the learning algorithm is only required to obtain a slight advantage in prediction over random guessing. The techniques used demonstrate an interesting duality between learning and cryptography. We also apply our results to obtain strong intractability results for approximating a generalization of graph coloring."
1994,Learning Boolean Formulas.,"Efficient distribution-free learning of Boolean formulas from positive and negative examples is considered. It is shown that classes of formulas that are efficiently learnable from only positive examples or only negative examples have certain closure properties. A new substitution technique is used to show that in the distribution-free case learning DNF (disjunctive normal form formulas) is no harder than learning monotone DNF. We prove that monomials cannot be efficiently learned from negative examples alone, even if the negative examples are uniformly distributed. It is also shown that, if the examples are drawn from uniform distributions, then the class of DNF in which each variable occurs at most once is efficiently weakly learnable (i.e., individual examples are correctly classified with a probability larger than 1/2 + 1/p, where p is a polynomial in the relevant parameters of the learning problem). We then show an equivalence between the notion of weak learning and the notion of group learning, where a group of examples of polynomial size, either all positive or all negative, must be correctly classified with high probability."
1994,Direct Bulk-Synchronous Parallel Algorithms.,"Abstract
We describe a methodology for constructing parallel algorithms that are transportable among parallel computers having different numbers of processors, different bandwidths of interprocessor communication, and different periodicity of global synchronization. We do this for the bulk-synchronous parallel (BSP) model, which abstracts the characteristics of a parallel machine into three numerical parameters p, g, and L, corresponding to processors, bandwidth, and periodicity, respectively. The model differentiates memory that is local to a processor from that which is not, but, for the sake of universality, does not differentiate network proximity. The advantages of this model in supporting shared memory or PRAM style programming have been treated elsewhere. Here we emphasize the viability of an alternative direct style of programming where, for the sake of efficiency, the programmer retains controls of memory allocation. We show that optimality to within a multiplicative factor close to one can be achieved for the problems of Gauss-Jordan elimination and sorting, by transportable algorithms that can be applied for a wide range of values of the parameters p, g, and L. While these algorithms are fairly simple themselves, descriptions of their behavior in terms of these parameters are somewhat complicated. The main reward of quantifying these complications, whether derived by analysis, as we do here, or by simulations, is that it allows software to be written once and for all that can be transported efficiently among parallel machines that fit the model. By way of contrast to these direct algorithms, we also give some simulation results for PRAMs on the BSP that identify the level of slack at which corresponding efficiencies can be approached by shared memory simulations, provided that the bandwidth parameter g is good enough."
1994,A Computational Model for Cognition (Abstract).,"A model of computation that is designed to bridge the gap in biological systems between the neural level of computation and the level of cognitive functioning. A neuroid is a model neuron that consists essentially of a classical threshold element that is augmented with states. The availability of states makes the model more flexibly programmable than more traditional neural networks, and allows a variety of useful timing mechanisms to be exploited flexibly in the computations. The paper presents two viewpoints from which neuroidal systems can be studied."
1993,Why BSP Computers?,"Abstract:
The author gives a summary of some of the arguments favoring the adoption of the bulk-synchronous parallel (BSP) model as a standard for parallel computing. First, he argues that for parallel computing to become a major industry, agreement has to be reached on a standard model at a level intermediate between the language and architecture levels. He goes on to list the factors that make the BSP model attractive as a standard at this intermediate or bridging level. Finally, he provides some reasons for favoring it over the shared memory or PRAM model which is an alternative candidate for this role.< >"
1993,Cryptographic Limitations on Learning Boolean Formulae and Finite Automata.,n/a
1992,A Combining Mechanism for Parallel Computers.,"Abstract
In a multiprocessor computer communication among the components may be based either on a simple router, which delivers messages point-to-point like a mail service, or on a more elaborate combining network that, in return for a greater investment in hardware, can combine messages to the same address prior to delivery. This paper describes a mechanism for recirculating messages in a simple router so that the added functionality of a combining network, for arbitrary access patterns, can be achieved by it with reasonable efficiency. The method brings together the messages with the same destination address in more than one stage, and at a set of components that is determined by a hash function and decreases in number at each stage."
1992,Direct Bulk-Synchronous Parallel Algorithms.,"Abstract
We describe a methodology for constructing parallel algorithms that are transportable among parallel computers having different numbers of processors, different bandwidths of interprocessor communication and different periodicity of global synchronisation. We do this for the bulk-synchronous parallel (BSP) model, which abstracts the characteristics of a parallel machine into three numerical parameters p, g, and L, corresponding to processors, bandwidth, and periodicity respectively. The model differentiates memory that is local to a processor from that which is not, but, for the sake of universality, does not differentiate network proximity. The advantages of this model in supporting shared memory or PRAM style programming have been treated elsewhere. Here we emphasise the viability of an alternative direct style of programming where, for the sake of efficiency the programmer retains control of memory allocation. We show that optimality to within a multiplicative factor close to one can be achieved for the problems of Gauss-Jordan elimination and sorting, by transportable algorithms that can be applied for a wide range of values of the parameters p, g, and L. We also give some simulation results for PRAMs on the BSP to identify the level of slack at which corresponding efficiencies can be approached by shared memory simulations, provided the bandwidth parameter g is good enough."
1990,A Bridging Model for Parallel Computation.,"The success of the von Neumann model of sequential computation is attributable to the fact that it is an efficient bridge between software and hardware: high-level languages can be efficiently compiled on to this model; yet it can be effeciently implemented in hardware. The author argues that an analogous bridge between software and hardware in required for parallel computation if that is to become as widely used. This article introduces the bulk-synchronous parallel (BSP) model as a candidate for this role, and gives results quantifying its efficiency both in implementing high-level language features and algorithms, as well as in being implemented in hardware."
1989,A General Lower Bound on the Number of Examples Needed for Learning.,"Abstract
We prove a lower bound of Œ©((1/…õ)ln(1/Œ¥)+VCdim(C)/…õ) on the number of random examples required for distribution-free learning of a concept class C, where VCdim(C) is the Vapnik-Chervonenkis dimension and …õ and Œ¥ are the accuracy and confidence parameters. This improves the previous best lower bound of Œ©((1/…õ)ln(1/Œ¥)+VCdim(C)) and comes close to the known general upper bound of O((1/…õ)ln(1/Œ¥)+(VCdim(C)/…õ)ln(1/…õ)) for consistent algorithms. We show that for many interesting concept classes, including kCNF and kDNF, our bound is actually tight to within a constant factor."
1989,Cryptographic Limitations on Learning Boolean Formulae and Finite Automata.,n/a
1988,Computational limitations on learning from examples.,"The computational complexity of learning Boolean concepts from examples is investigated. It is shown for various classes of concept representations that these cannot be learned feasibly in a distribution-free sense unless R = NP. These classes include (a) disjunctions of two monomials, (b) Boolean threshold functions, and (c) Boolean formulas in which each variable occurs at most once. Relationships between learning of heuristics and finding approximate solutions to NP-hard optimization problems are given."
1988,Functionality in Neural Nets.,"We investigate the functional capabilities of sparse networks of computing elements in accumulating knowledge through successive learning experiences . As experiences, we consider various combinations of episodic and concept learning, in supervised or unsupervised mode, of conjunctions and of disjunctions. For these we exhibit algorithms for learning in well defined senses. Each concept or episode is expressible in terms of concepts or episodes already known, and is thus learned hierarchically, without disturbing previous knowledge. Minimal assumptions are made about the computing elements, which are assumed to be classical threshold elements with states. Also we adhere to severe resource constraints. Each new concept or episode requires storage linear in the relevant parameters, and the algorithms take very few steps. We hypothesise that in our context functionality is limited more by the communication bottlenecks in the networks than by the computing capabilities of the elements and hence that this approach may prove useful in understanding biological systems even in the absence of accurate neurophysiological models."
1988,Functionality in Neural Nets.,"We investigate the functional capabilities of sparse networks of computing elements in accumulating knowledge through successive learning experiences . As experiences, we consider various combinations of episodic and concept learning, in supervised or unsupervised mode, of conjunctions and of disjunctions. For these we exhibit algorithms for learning in well defined senses. Each concept or episode is expressible in terms of concepts or episodes already known, and is thus learned hierarchically, without disturbing previous knowledge. Minimal assumptions are made about the computing elements, which are assumed to be classical threshold elements with states. Also we adhere to severe resource constraints. Each new concept or episode requires storage linear in the relevant parameters, and the algorithms take very few steps. We hypothesise that in our context functionality is limited more by the communication bottlenecks in the networks than by the computing capabilities of the elements and hence that this approach may prove useful in understanding biological systems even in the absence of accurate neurophysiological models.
"
1988,A General Lower Bound on the Number of Examples Needed for Learning.,"Abstract We prove a lower bound of ? ((1/?)ln(1/?)+VCdim( C )/?) on the number of random examples required for distribution-free learning of a concept class C , where VCdim( C ) is the Vapnik-Chervonenkis dimension and ? and ? are the accuracy and confidence parameters. This improves the previous best lower bound of ? ((1/?)ln(1/?)+VCdim( C )) and comes close to the known general upper bound of O ((1/?)ln(1/?)+(VCdim( C )/?)ln(1/?)) for consistent algorithms. We show that for many interesting concept classes, including k CNF and k DNF, our bound is actually tight to within a constant factor"
1987,A logarithmic time sort for linear size networks.,"A randomized algorithm that sorts on an N node network with constant valence in O(log N) time is given. More particularly, the algorithm sorts N items on an N-node cube-connected cycles graph, and, for some constant k, for all large enough &agr;, it terminates within k&agr; log N time with probability at least 1 - N-&agr;."
1987,Recent Developments in the Theory of Learning (Abstract).,n/a
1987,On the Learnability of Boolean Formulae.,"The authors study the computational feasibility of learning boolean expressions from examples. Their goals are to prove results and develop general techniques that shed light on the boundary between the classes of expressions that are learnable in polynomial time and those that are apparently not. They employ the distribution-free model of learning. Their results fall into three categories: closure properties of learnable classes, negative results, and distribution-specific positive results."
1986,Negation is Powerless for Boolean Slice Functions.,"It is shown that for any slice function of n variables the monotone circuit complexity exceeds the circuit complexity over a universal basis by at most a multiplicative constant factor and an additive term of order $O(n(\log n)^2 )$.
"
1986,Random Generation of Combinatorial Structures from a Uniform Distribution.,"Abstract
The class of problems involving the random generation of combinatorial structures from a uniform distribution is considered. Uniform generation problems are, in computational difficulty, intermediate between classical existence and counting problems. It is shown that exactly uniform generation of ‚Äòefficiently verifiable‚Äô combinatorial structures is reducible to approximate counting (and hence, is within the third level of the polynomial hierarchy). Natural combinatorial problems are presented which exhibit complexity gaps between their existence and generation, and between their generation and counting versions. It is further shown that for self-reducible problems, almost uniform generation and randomized approximate counting are inter-reducible, and hence, of similar complexity."
1986,NP is as Easy as Detecting Unique Solutions.,"Abstract
For every known NP-complete problem, the number of solutions of its instances varies over a large range, from zero to exponentially many. It is therefore natural to ask if the inherent intractability of NP-complete problem is caused by this wide variation. We give a negative answer to this question using the notion of randomized polynomial time reducibility. We show that the problems of distinguishing between instances of SAT having zero or one solution, or of finding solutions to instances of SAT having a unique solution, are as hard as SAT, under randomized reductions. Several corollaries about the difficulty of specific problems follow. For example, computing the parity of the number of solutions of a SAT formula is shown to be NP-hard, and deciding if a SAT formula has a unique solution is shown to be Dp-hard, under randomized reduction. Central to the study of cryptography is the question as to whether there exist NP-problems whose instances have solutions that are unique but are hard to find. Our result can be interpreted as strengthening the belief that such problems exist."
1985,A Complexity Theory Based on Boolean Algebra.,"A projection of a Boolean function is a function obtained by substituting for each of its variables a variable, the negation of a variable, or a constant. Reducibilities among computational problems under this relation of projection are considered. It is shown that much of what is of everyday relevance in Turing-machine-based complexity theory can be replicated easily and naturally in this elementary framework. Finer distinctions about the computational relationships among natural problems can be made than in previous formulations and some negative results are proved."
1985,Learning Disjunction of Conjunctions.,"The question of whether concepts expressible as disjunctions of conjunctions can be learned from examples in polynomial time is investigated. Positive results are shown for significant subclasses that allow not only propositional predicates but also some relations. The algorithms are extended so as to be provably tolerant to a certain quantifiable error rate in the examples data. It is further shown that under certain restrictions on these subclasses the learning algorithms are well suited to implementation on neural networks of threshold elements. The possible importance of disjunctions of conjunctions as a knowledge representation stems from the observation that on the one hand humans appear to like using it, and, on the other, that there is circumstantial evidence that significantly larger classes may not be learnable in polynomial time. An NP-completeness result corroborating the later is also presented."
1985,NP Is as Easy as Detecting Unique Solutions.,"For all known NP-complete problems the number of solutions in instances having solutions may vary over an exponentially large range. Furthermore, most of the well-known ones, such as satisfiability, are parsimoniously interreducible, and these can have any number of solutions between zero and an exponentially large number. It is natural to ask whether the inherent intractability of NP-complete problems is caused by this wide variation. In this paper we give a negative answer to this using randomized reductions. We show that the problems of distinguishing between instances of SAT having zero or one solution, or finding solutions to instances of SAT having unique solutions, are as hard as SAT itself. Several corollaries about the difficulty of specific problems follow. For example if the parity of the number of solutions of SAT can be computed in RP then NP = RP. Some further problems can be shown to be hard for NP or DP via randomized reductions."
1984,A Theory of the Learnable.,"Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learnt using it in a reasonable (polynomial) number of steps. We find that inherent algorithmic complexity appears to set serious limits to the range of concepts that can be so learnt. The methodology and results suggest concrete principles for designing realistic learning systems."
1984,Short Monotone Formulae for the Majority Function.,"Abstract
It is shown that the monotone formula-size complexity of the monotone symmetric functions on n variables can be bounded above by a function of order O(n5.3)."
1984,A Theory of the Learnable.,"Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learnt using it in a reasonable (polynomial) number of steps. We find that inherent algorithmic complexity appears to set serious limits to the range of concepts that can be so learnt. The methodology and results suggest concrete principles for designing realistic learning systems."
1983,Fast Parallel Computation of Polynomials Using Few Processors.,"It is shown that any multivariate polynomial of degree d that can be computed sequentially in C steps can be computed in parallel in $O((\log d)(\log C + \log d))$ steps using only $(Cd)^{O(1)} $ processors.
"
1983,Optimality of a Two-Phase Strategy for Routing in Interconnection Networks.,"Abstract:
It is shown that for d-way shuffle graphs all oblivious algorithms for realizing permutations in logarithmic time send packets along routes twice as long as the diameter of the graph. This confirms the optimality of the strategy that sends packets to random nodes in a first phase and to the correct destinations in the second. For the shuffle-exchange graph the corresponding route length is shown to be strictly longer than for the 2-way shuffle."
1983,Size Bounds for Superconcentrators.,"Abstract
We prove that any N-superconcentrator of indegree two has at least 4N - o(N) nodes. From this lower bounds of 4N - o(N) follow on the number of additions required to compute the Discrete Fourier Transform of prime order and cyclic convolution. Using small examples we illustrate how small superconcentrators can suggest fast algorithms for instances of these problems.
For superconcentrators with no degree restrictions we prove a lower bound of 5N - o(N) edges. Also, we give a recursive construction with 3N log2 N edges that improves on the best bounds previously known for values of N up to several thousand."
1983,A Logarithmic Time Sort for Linear Size Networks.,We give a randomized algorithm that sorts on an N node network with constant valence in 0(log N) time. More particularly the algorithm sorts N items on an N node cube-connected cycles graph and for some constant k for all large enough &agr; it terminates within k&agr; log N time with probability at least 1‚àíN‚àí&agr;.
1983,Exponential Lower Bounds for Restricted Monotone Circuits.,"In this paper we consider monotone Boolean circuits with three alternations, in the order ‚Äúor‚Äù, ‚Äúand‚Äù, ‚Äúor.‚Äù Whenever the number of alternations is limited to a fixed constant the formula and circuit size measures are polynomially related to each other. We shall therefore refer to this measure interchangeably as Œ£œÄŒ£-formula size or Œ£œÄŒ£-circuit size. We shall prove that any such circuit or formula for detecting the existence of cliques in an N-node graph has at least 2Œ©(NŒµ) gates for some Œµ > 0 independent of N."
1982,A Scheme for Fast Parallel Communication.,"Consider $N = 2^n $ nodes connected by wires to make an n-dimensional binary cube. Suppose that initially the nodes contain one packet each addressed to distinct nodes of the cube. We show that there is a distributed randomized algorithm that can route every packet to its destination without two packets passing down the same wire at any one time, and finishes within time $O(\log N)$ with overwhelming probability for all such routing requests. Each packet carries with it $O(\log N)$ bits of bookkeeping information. No other communication among the nodes takes place.

The algorithm offers the only scheme known for realizing arbitrary permutations in a sparse N node network in $O(\log N)$ time and has evident applications in the design of general purpose parallel computers.

"
1981,A Fast Parallel Algorithm for Routing in Permutation Networks.,"Abstract:
An algorithm is given for routing in permutation networks-that is, for computing the switch settings that implement a given permutation. The algorithm takes serial time O ( n (log N ) 2 ) (for one processor with random access to a memory of O ( n ) words) or parallel time O ((log n ) 3 ) (for n synchronous processors with conflict-free random access to a common memory of O ( n ) words). These time bounds may be reduced by a further logarithmic factor when all of the switch sizes are integral powers of two."
1981,Universality Considerations in VLSI Circuits.,"Abstract:
The problem of embedding the interconnection pattern of a circuit into a two-dimensional surface of minimal area is discussed. Since even for some natural patterns graphs containing m connections may require Œ©( m 2 ) area, in order to achieve compact embeddings restricted classes of graphs have to be considered. For example, arbitrary trees (of bounded degree) can be embedded in linear area without edges crossing over. Planar graphs can be embedded efficiently only if crossovers are allowed in the embedding."
1981,A Complexity Theory Based on Boolean Algebra.,n/a
1981,Fast Parallel Computation of Polynomials Using Few Processes.,"Abstract
It is shown that any multivariate polynomial that can be computed sequentially in C steps and has degree d can be computed in parallel in 0((log d) (log C + log d)) steps using only (Cd)0(1) processors."
1981,Universal Schemes for Parallel Communication.,"In this paper we isolate a combinatorial problem that, we believe, lies at the heart of this question and provide some encouragingly positive solutions to it. We show that there exists an N-processor realistic computer that can simulate arbitrary idealistic N-processor parallel computations with only a factor of O(log N) loss of runtime efficiency. The main innovation is an O(log N) time randomized routing algorithm. Previous approaches were based on sorting or permutation networks, and implied loss factors of order at least (log N)2."
1980,Computing Multivariate Polynomials in Parallel.,n/a
1980,Negation can be Exponentially Powerful.,n/a
1979,Fast Probabilistic Algorithms for Hamiltonian Circuits and Matchings.,"Abstract
We describe and analyse three simple efficient algorithms with good probabilistic behaviour; two algorithms with run times of O(n(log n)2) which almost certainly find directed (undirected) Hamiltonian circuits in random graphs of at least cn log n edges, and an algorithm with a run time of O(n log n) which almost certainly finds a perfect matching in a random graph of at least cn log n edges. Auxiliary propositions regarding conversion between input distributions and the ‚Äúde-randomization‚Äù of randomized algorithms are proved. A new model, the random access computer (RAC), is introduced specifically to treat run times in low-level complexity."
1979,The Complexity of Enumeration and Reliability Problems.,"The class of $# P$-complete problems is a class of computationally eqivalent counting problems (defined by the author in a previous paper) that are at least as difficult as the $NP$-complete problems. Here we show, for a large number of natural counting problems for which there was no previous indication of intractability, that they belong to this class. The technique used is that of polynomial time reduction with oracles via translations that are of algebraic or arithmetic nature.

"
1979,The Complexity of Computing the Permanent.,"Abstract
It is shown that the permanent function of (0, 1)-matrices is a complete problem for the class of counting problems associated with nondeterministic polynomial time computations. Related counting problems are also considered. The reductions used are characterized by their nontrivial use of arithmetic."
1979,Negation Can Be Exponentially Powerful.,"Among the most remarkable algorithms in algebra are Strassen's algorithm for the multiplication of matrices and the Fast Fourier Transform method for the convolution of vectors. For both of these problems the definition suggests an obvious algorithm that uses just the monotone operations + and √ó. Schnorr [18] has shown that these algorithms, which use &thgr;(n3) and &THgr;(n2) operations respectively, are essentially optimal among algorithms that use only these monotone operations. By using subtraction as an additional operation and exploiting cancellations of computed terms in a very intricate way Strassen showed that a faster algorithm requiring only O(n2.81) operations is possible. The FFT method for convolution achieves O(nlog n) complexity in a similar fashion. The question arises as to whether we can expect even greater gains in computational efficiency by such judicious use of cancellations. In this paper we give a positive answer to this, by exhibiting a problem for which an exponential speedup can be attained using {+,‚àí,√ó} rather than just {+,√ó} as operations. The problem in question is the multivariate polynomial associated with perfect matchings in planar graphs. For this a fast algorithm is implicit in the Pfaffian technique of Fisher and Kasteleyn [6,8]. The main result we provide here is the exponential lower bound in the monotone case."
1979,Completeness Classes in Algebra.,"In the theory of recursive functions and computational complexity it has been demonstrated repeatedly that the natural problems tend to cluster together in ‚Äúcompleteness classes‚Äù. These are families of problems that (A) are computationally interreducible and (B) are the hardest members of some computationally defined class. The aim of this paper is to demonstrate that for both algebraic and combinatorial problems this phenomenon exists in a form that is purely algebraic in both of the respects (A) and (B). Such computational consequences as NP-completeness are particular manifestations of something more fundamental. The core of the paper is self-contained, consisting as it does essentially of the two notions of ‚Äúp-definability‚Äù and the five algebraic relations that are proved as theorems. In the remainder our aim is to elucidate the computational consequences of these basic results. Hence in the auxiliary propositions and discussion for convenience we do assume familiarity with algebraic and Boolean complexity theory."
1979,Negative Results on Counting.,n/a
1978,The Complexity of Combinatorial Computations: An Introduction.,"Abstract
The search for mechanizable procedures or algorithms for solving problems has been an integral part of mathematics from the beginning. Concern about the efficiency of the algorithms discovered was rarely mentioned explicitly, however, until the last few decades. Nevertheless, we must presume that this concern was often present, since such algorithms as Gaussian elimination for solving linear equations, Newton‚Äôs iteration for algebraic equations, and Euclid‚Äôs algorithm for greatest common divisors, appear very efficient even now after centuries of further investigation."
1977,On Time Versus Space.,"It is shown that every deterministic multitape Turing machine of time complexity t(n) can be simulated by a deterministic Turing machine of tape complexity t(n)/logt(n). Consequently, for tape constructable t(n), the class of languages recognizable by multitape Turing machines of time complexity t(n) is strictly contained in the class of languages recognized by Turing machines of tape complexity t(n). In particular the context-sensitive languages cannot be recognized in linear time by deterministic multitape Turing machines."
1977,Graph-Theoretic Arguments in Low-Level Complexity.,"Abstract
We have surveyed one approach to understanding complexity issues for certain easily computable natural functions. Shifting graphs have been seen to account accurately and in a unified way for the superlinear complexity of several problems for various restricted models of computation. To attack ""unrestricted"" models (in the present context combinational circuits or straight-line arithmetic programs,) a first attempt, through superconcentrators, fails to provide any lower bounds although it does give counter-examples to alternative approaches. The notion of rigidity, however, does offer for the first time a reduction of relevant computational questions to noncomputional properties. The ""reduction"" consists of the conjunction of Corollary 6.3 and Theorem 6.4 which show that ""for most sets of linear forms over the reals the stated algebraic and combinatorial reasons account for the fact that they cannot be computed in linear time and depth O(log n) simultaneously."" We have outlined some problem areas which our preliminary results raise, and feel that further progress on most of these is humanly feasible. We would be interested in alternative approaches also.
Problem 6 Propose reductions of relevant complexity issues to noncomputational properties, that are more promising or tractable than the ones above."
1977,Fast Probabilistic Algorithms for Hamiltonian Circuits and Matchings.,"The main purpose of this paper is to give techniques for analysing the probabilistic performance of certain kinds of algorithms, and hence to suggest some fast algorithms with provably desirable probabilistic behaviour. The particular problems we consider are: finding Hamiltonian circuits in directed graphs (DHC), finding Hamiltonian circuits in undirected graphs (UHC), and finding perfect matchings in undirected graphs (PM). We show that for each problem there is an algorithm that is extremely fast (0(n(log n)2) for DHC and UHC, and 0(nlog n) for PM), and which with probability tending to one finds a solution in randomly chosen graphs of sufficient density. These results contrast with the known NP-completeness of the first two problems [2,12] and the best worst-case upper bound known of 0(n2.5) for the last [9]."
1976,A Note on the Succinctness of Descriptions of Deterministic Languages.,It is shown that the relative succinctness that may be achieved by describing deterministic context-free languages by general unambiguous grammars rather than by deterministic pushdown automata is not bounded by any recursive function.
1976,Relative Complexity of Checking and Evaluating.,n/a
1976,Shifting Graphs and Their Applications.,"Graphs that in a certain precise sense are rich in sets of vertex-disjoint paths are studied. Bounds are obtained on the minimum number of edges in such graphs, and these are used to deduce nonlinear lower bounds on the computational complexity of shifting, merging, and matching problems."
1976,Graph-Theoretic Properties in computational Complexity.,"The extent to which a set of related graph-theoretic properties can be used to accont for the superlinear complexity of computational problems is explored. While a previously widely held positive conjecture is refuted, it is also shown that certain limited lower bounds can be proved by means of such properties."
1976,Circuit Size is Nonlinear in Depth.,"Abstract
Two fundamental complexity measures for a Boolean function f are its circuit depth d(f) and its circuit size c(f). It is shown that
for all f."
1976,The Equivalence Problem for D0L Systems and its Decidability for Binary Alphabets.,n/a
1976,Universal Circuits (Preliminary Report).,"We show that there is a combinational (acyclic) Boolean circuit of complexity 0(slog s), that can be made to compute any Boolean function of complexity s by setting its specially designated set of control inputs to appropriate fixed values. We investigate the construction of such ‚Äúuniversal circuits‚Äù further so as to exhibit directions in which refinements of the asymptotic multiplicative constant factor in the complexity bound can be found. In this pursuit useful detailed guidance is provided by available lower bound arguments. In the final section we discuss some other problems in computational complexity that can be related directly to the graph-theoretic ideas behind our constructions. For motivation we start by illustrating some of the applications of universal circuits themselves."
1975,Regularity and Related Problems for Deterministic Pushdown Automata.,"It is shown that to decide whether the language accepted by an arbitrary pushdown deterministic pushdown automaton is LL(k) or whether it is accepted by some one-counter or finite-turn pushdown machine, must be at least as difficult as to decide whether it is regular. The regularity problem itself is analyzed in detail, and Stearns' decision procedure for this is improved by one level of exponentiation. Upper bounds, close to known lower bounds, are obtained for the succintness with which a pushdown automaton, and various restrictions of it, can express equivalent finite-state machines."
1975,General Context-Free Recognition in Less than Cubic Time.,An algorithm for general context-free recognition is given that requires less than n3 time asymptotically for input strings of length n.
1975,Deterministic One-Counter Automata.,The equivalence problem for deterministic one-counter automata is shown to bedecidable. A corollary for schema theory is that equivalence is decidable for Ianov schemas with an auxiliary counter.
1975,Parallelism in Comparison Problems.,"The worst-case time complexity of algorithms for multiprocessor computers with binary comparisons as the basic operations is investigated. It is shown that for the problems of finding the maximum, sorting, and merging a pair of sorted lists, if n, the size of the input set, is not less than k, the number of processors, speedups of at least $O(k/\log \log k)$ can be achieved with respect to comparison operations. The algorithm for finding the maximum is shown to be optimal for all values of k and n.



"
1975,On Time versus Space and Related Problems.,"It is shown that every deterministic multitape Turing machine of time complexity t(n) can be simulated by a deterministic Turing machine of tape complexity t(n)/logt(n). Consequently, for tape constructable t(n), the class of languages recognizable by mul"
1975,On Non-linear Lower Bounds in Computational Complexity.,"The purpose of this paper is to explore the possibility that purely graph-theoretic reasons may account for the superlinear complexity of wide classes of computational problems. The results are therefore of two kinds: reductions to graph theoretic conjectures on the one hand, and graph theoretic results on the other. We show that the graph of any algorithm for any one of a number of arithmetic problems (e.g. polynomial multiplication, discrete Fourier transforms, matrix multiplication) must have properties closely related to concentration networks."
1974,The Equivalence Problem for Deterministic Finite-Turn Pushdown Automata.,It is proved that there is an effective procedure for deciding whether two deterministic finite-turn pushdown automata are equivalent.
1974,The Decidability of Equivalence for Deterministic Finite-Turn Pushdown Automata.,"A deterministic pushdown automaton (dpda) is described as finite-turn if there is a bound on the number of times the direction of the stack movement can change in the set of all derivations from the starting configuration. The purpose of this paper is to show that there exists a procedure for deciding whether two such finite-turn machines recognize the same language. By virtue of a direct correspondence between a restricted class of one-turn dpda and deterministic two-tape acceptors (Valiant (1973)), our proof also provides a solution to the equivalence problem for the latter, alternative to that of Bird (1973). Since some of the ideas we introduce are not related exclusively to the finite-turn property, or even pushdown machines, it is hoped that our methods can be adapted for constructing equivalence tests for other classes of deterministic automata. Our main technique can be regarded as a generalization in several directions of one introduced by Rosenkrantz and Stearns (1970). They consider a class of pushdown automata for which a natural valuation can be placed on each stack segment, and deduce that for any input word, two equivalent machines in that class must have closely related stack movements. They show how, under such circumstances, for any two machines a single pushdown automaton can be constructed to simulate them both, and used to decide their equivalence. What we shall show is that even for a class with no such stack valuation known, and in which two equivalent machines can have totally dissimilar stack movements, pushdown automata, now nondeterministic, can be found to perform the required simulations."
1973,Deterministic one-counter automata.,"Abstract
The class of deterministic one-counter automata is a natural extension of the class of finite-state machines. We have shown that in contrast with the inclusion and nullity of intersection problems, which become undecidable under this generalisation the equivalence problem remains decidable.
We have established that these automata have certain periodic structural properties, and have derived an upper bound for the associated period that is asymptotically achievable. The resulting bound on the time complexity of the derived decision procedure is exponential in about the square root of the number of states of the tested machines. Whether a polynomial time test exists, remains open."
