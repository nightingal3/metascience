2020,The Role of Latency and Task Complexity in Predicting Visual Search Behavior.,"Abstract:
Latency in a visualization system is widely believed to affect user behavior in measurable ways, such as requiring the user to wait for the visualization system to respond, leading to interruption of the analytic flow. While this effect is frequently observed and widely accepted, precisely how latency affects different analysis scenarios is less well understood. In this paper, we examine the role of latency in the context of visual search, an essential task in data foraging and exploration using visualization. We conduct a series of studies on Amazon Mechanical Turk and find that under certain conditions, latency is a statistically significant predictor of visual search behavior, which is consistent with previous studies. However, our results also suggest that task type, task complexity, and other factors can modulate the effect of latency, in some cases rendering latency statistically insignificant in predicting user behavior. This suggests a more nuanced view of the role of latency than previously reported. Building on these results and the findings of prior studies, we propose design guidelines for measuring and interpreting the effects of latency when evaluating performance on visual search tasks."
2019,Kyrix: Interactive Pan/Zoom Visualizations at Scale.,"Pan and zoom are basic yet powerful interaction techniques for exploring large datasets. However, existing zoomable UI toolkits such as Pad++ and ZVTM do not provide the backend database support and data?driven primitives that are necessary for creating large?scale visualizations. This limitation in existing general?purpose toolkits has led to many purpose?built solutions (e.g. Google Maps and ForeCache) that address the issue of scalability but cannot be easily extended to support visualizations beyond their intended data types and usage scenarios. In this paper, we introduce Kyrix to ease the process of creating general and large?scale web?based pan/zoom visualizations. Kyrix is an integrated system that provides the developer with a concise and expressive declarative language along with a backend support for performance optimization of large?scale data. To evaluate the scalability of Kyrix, we conducted a set of benchmarked experiments and show that Kyrix can support high interactivity (with an average latency of 100 ms or below) on pan/zoom visualizations of 100 million data points. We further demonstrate the accessibility of Kyrix through an observational study with 8 developers. Results indicate that developers can quickly learn Kyrix's underlying declarative model to create scalable pan/zoom visualizations. Finally, we provide a gallery of visualizations and show that Kyrix is expressive and flexible in that it can support the developer in creating a wide range of customized visualizations across different application domains and data types.
"
2019,Rethinking Database High Availability with RDMA Networks.,"Highly available database systems rely on data replication to tolerate machine failures. Both classes of existing replication algorithms, active-passive and active-active, were designed in a time when network was the dominant performance bottleneck. In essence, these techniques aim to minimize network communication between replicas at the cost of incurring more processing redundancy; a trade-off that suitably fitted the conventional wisdom of distributed database design. However, the emergence of next-generation networks with high throughput and low latency calls for revisiting these assumptions. In this paper, we first make the case that in modern RDMAenabled networks, the bottleneck has shifted to CPUs, and therefore the existing network-optimized replication techniques are no longer optimal. We present Active-Memory Replication, a new high availability scheme that efficiently leverages RDMA to completely eliminate the processing redundancy in replication. Using ActiveMemory, all replicas dedicate their processing power to executing new transactions, as opposed to performing redundant computation. Active-Memory maintains high availability and correctness in the presence of failures through an efficient RDMA-based undologging scheme. Our evaluation against active-passive and activeactive schemes shows that Active-Memory is up to a factor of 2 faster than the second-best protocol on RDMA-based networks."
2019,Data Civilizer 2.0: A Holistic Framework for Data Preparation and Analytics.,"Data scientists spend over 80% of their time (1) parameter-tuning machine learning models and (2) iterating between data cleaning and machine learning model execution. While there are existing efforts to support the first requirement, there is currently no integrated workflow system that couples data cleaning and machine learning development. The previous version of Data Civilizer was geared towards data cleaning and discovery using a set of pre-defined tools. In this paper, we introduce Data Civilizer 2.0, an end-to-end workflow system satisfying both requirements. In addition, this system also supports a sophisticated data debugger and a workflow visualization system. In this demo, we will show how we used Data Civilizer 2.0 to help scientists at the Massachusetts General Hospital build their cleaning and machine learning pipeline on their 30TB brain activity dataset."
2019,Choosing A Cloud DBMS: Architectures and Tradeoffs.,"As analytic (OLAP) applications move to the cloud, DBMSs have shifted from employing a pure shared-nothing design with locally attached storage to a hybrid design that combines the use of shared-storage (e.g., AWS S3) with the use of shared-nothing query execution mechanisms. This paper sheds light on the resulting tradeoffs, which have not been properly identified in previous work. To this end, it evaluates the TPC-H benchmark across a variety of DBMS offerings running in a cloud environment (AWS) on fast 10Gb+ networks, specifically database-as-a-service offerings (Redshift, Athena), query engines (Presto, Hive), and a traditional cloud agnostic OLAP database (Vertica). While these comparisons cannot be apples-to-apples in all cases due to cloud configuration restrictions, we nonetheless identify patterns and design choices that are advantageous. These include prioritizing low-cost object stores like S3 for data storage, using system agnostic yet still performant columnar formats like ORC that allow easy switching to other systems for different workloads, and making features that benefit subsequent runs like query precompilation and caching remote data to faster storage optional rather than required because they disadvantage ad hoc queries."
2019,Smile: A System to Support Machine Learning on EEG Data at Scale.,"In order to reduce the possibility of neural injury from seizures and sidestep the need for a neurologist to spend hours on manually reviewing the EEG recording, it is critical to automatically detect and classify ìinterictal-ictal continuumî (IIC) patterns from EEG data. However, the existing IIC classification techniques are shown to be not accurate and robust enough for clinical use because of the lack of high quality labels of EEG segments as training data. Obtaining high-quality labeled data is traditionally a manual process by trained clinicians that can be tedious, time-consuming, and errorprone. In this work, we propose Smile, an industrial scale system that provides an end-to-end solution to the IIC pattern classification problem. The core components of Smile include a visualizationbased time series labeling module and a deep-learning based active learning module. The labeling module enables the users to explore and label 350 million EEG segments (30TB) at interactive speed. The multiple coordinated views allow the users to examine the EEG signals from both time domain and frequency domain simultaneously. The active learning module first trains a deep neural network that automatically extracts both the local features with respect to each segment itself and the long term dynamics of the EEG signals to classify IIC patterns. Then leveraging the output of the deep learning model, the EEG segments that can best improve the model are selected and prompted to clinicians to label. This process is iterated until the clinicians and the models show high degree of agreement. Our initial experimental results show that our Smile system allows the clinicians to label the EEG segments at will with a response time below 500 ms. The accuracy of the model is progressively improved as more and more high quality labels are acquired over time."
2019,Kyrix: Interactive Visual Data Exploration at Scale.,"Scalable interactive visual data exploration is crucial in many domains due to increasingly large datasets generated at rapid rates. Details-on-demand provides a useful interaction paradigm for exploring large datasets, where users start at an overview, find regions of interest, zoom in to see detailed views, zoom out and then repeat. This paradigm is the primary user interaction mode of widely-used systems such as Google Maps, Aperture Tiles and ForeCache. These earlier systems, however, are highly customized with hardcoded visual representations and optimizations. A more general framework is needed to facilitate the development of visual data exploration systems at scale. In this paper, we present Kyrix, an end-to-end system for developing scalable details-on-demand data exploration applications. Kyrix provides developers with a declarative model for easy specification of general visualizations. Behind the scenes, Kyrix utilizes a suite of performance optimization techniques to achieve a response time within 500ms for various user interactions. We also report results from a performance study which shows that a novel dynamic fetching scheme adopted by Kyrix outperforms tile-based fetching used in earlier systems."
2019,Unsupervised String Transformation Learning for Entity Consolidation.,"Abstract:
Data integration has been a long-standing challenge in data management with many applications. A key step in data integration is entity consolidation. It takes a collection of clusters of duplicate records as input and produces a single ""golden record"" for each cluster, which contains the canonical value for each attribute. Truth discovery and data fusion methods as well as Master Data Management (MDM) systems can be used for entity consolidation. However, to achieve better results, the variant values (i.e., values that are logically the same with different formats) in the clusters need to be consolidated before applying these methods. For this purpose, we propose a data-driven method to standardize the variant values based on two observations: (1) the variant values usually can be transformed to the same representation (e.g., ""Mary Lee"" and ""Lee, Mary"") and (2) the same transformation often appears repeatedly across different clusters (e.g., transpose the first and last name). Our approach first uses an unsupervised method to generate groups of value pairs that can be transformed in the same way. Then the groups are presented to a human for verification and the approved ones are used to standardize the data. In a real-world dataset with 17,497 records, our method achieved 75% recall and 99.5% precision in standardizing variant values by asking a human 100 yes/no questions, which completely outperformed a state of the art data wrangling tool."
2019,How I Learned to Stop Worrying and Love Re-optimization.,"Abstract:
Cost-based query optimizers remain one of the most important components of database management systems for analytic workloads. Though modern optimizers select plans close to optimal performance in the common case, a small number of queries are an order of magnitude slower than they could be. In this paper we investigate why this is still the case, despite decades of improvements to cost models, plan enumeration, and cardinality estimation. We demonstrate why we believe that a re-optimization mechanism is likely the most cost-effective way to improve end-to-end query performance. We find that even a simple re-optimization scheme can improve the latency of many poorly performing queries. We demonstrate that re-optimization improves the end-to-end latency of the top 20 longest running queries in the Join Order Benchmark by 27%, realizing most of the benefit of perfect cardinality estimation."
2019,Towards an End-to-End Human-Centric Data Cleaning Framework.,"Data Cleaning refers to the process of detecting and fixing errors in the data. Human involvement is instrumental at several stages of this process such as providing rules or validating computed repairs. There is a plethora of data cleaning algorithms addressing a wide range of data errors (e.g., detecting duplicates, violations of integrity constraints, and missing values). Many of these algorithms involve a human in the loop, however, this latter is usually coupled to the underlying cleaning algorithms. In a real data cleaning pipeline, several data cleaning operations are performed using different tools. A high-level reasoning on these tools, when combined to repair the data, has the potential to unlock useful use cases to involve humans in the cleaning process. Additionally, we believe there is an opportunity to benefit from recent advances in active learning methods to minimize the effort humans have to spend to verify data items produced by tools or humans. There is currently no end-to-end data cleaning framework that systematically involves humans in the cleaning pipeline regardless of the underlying cleaning algorithms. In this paper, we present opportunities that this framework could offer, and highlight key challenges that need to be addressed to realize this vision. We present a design vision and discuss scenarios that motivate the need for this framework to judiciously assist humans in the cleaning process.
"
2019,Raha: A Configuration-Free Error Detection System.,"Detecting erroneous values is a key step in data cleaning. Error detection algorithms usually require a user to provide input configurations in the form of rules or statistical parameters. However, providing a complete, yet correct, set of configurations for each new dataset is not trivial, as the user has to know about both the dataset and the error detection algorithms upfront. In this paper, we present Raha, a new configuration-free error detection system. By generating a limited number of configurations for error detection algorithms that cover various types of data errors, we can generate an expressive feature vector for each tuple value. Leveraging these feature vectors, we propose a novel sampling and classification scheme that effectively chooses the most representative values for training. Furthermore, our system can exploit historical data to filter out irrelevant error detection algorithms and configurations. In our experiments, Raha outperforms the state-of-the-art error detection techniques with no more than 20 labeled tuples on each dataset."
2019,ANMAT: Automatic Knowledge Discovery and Error Detection through Pattern Functional Dependencies.,"Knowledge discovery is critical to successful data analytics. We propose a new type of meta-knowledge, namely pattern functional dependencies (PFDs), that combine patterns (or regex-like rules) and integrity constraints (ICs) to model the dependencies (or meta-knowledge) between partial values (or patterns) across different attributes in a table. PFDs go beyond the classical functional dependencies and their extensions. For instance, in an employee table, ID ""F-9-107'', ""F'' determines the finance department. Moreover, a key application of PFDs is to use them to identify erroneous data; tuples that violate some PFDs. In this demonstration, attendees will experience the following features: (i) PFD discovery -- automatically discover PFDs from (dirty) data in different domains; and (ii) Error detection with PFDs -- we will show errors that are detected by PFDs but cannot be captured by existing approaches.
"
2019,SchengenDB: A Data Protection Database Proposal.,"Abstract
GDPR in Europe and similar regulations, such as the California CCPA, require new levels of privacy support for consumers. Most challenging to IT departments is the ‚Äúright to be forgotten‚Äù. Hence, an enterprise must ensure that ALL information about a specific consumer be deleted from enterprise storage, when requested. Since enterprises are internally heavily ‚Äúsiloed‚Äù, sharing of information is usually accomplished by copying data between systems. This makes finding and deleting all copies of data on a particular consumer difficult.
GDPR also requires the notion of purposes, which is an access control model orthogonal to the one customarily in SQL. Herein, we sketch an implementation of purposes and show how it fits within a conventional access control framework.
We then propose two solutions to supporting GDPR in a DBMS. When a ‚Äúgreen field‚Äù environment is present, we propose a solution which directly supports the process of ensuring GDPR compliance at enterprise-scale. Specifically, it is designed to store every fact about a consumer exactly once. Therefore, the right to be forgotten is readily supported by deleting that fact. On the other hand, when dealing with legacy systems in the enterprise, we propose a second solution which tracks all copies of personal information, so they can be deleted on request. Of course, this solution entails additional overhead in the DBMS.
Once data leaves the DBMS, it is in some application. We propose ‚Äúsandboxing‚Äù applications in a novel way that will prevent them from leaking data to the outside world when inappropriate. Lastly, we discuss the challenges associated with auditing and logging of data. This paper sketches the design of the above GDPR compliant facilities, which we collectively term SchengenDB."
2019,WIP - SKOD: A Framework for Situational Knowledge on Demand.,"Abstract
Extracting relevant patterns from heterogeneous data streams poses significant computational and analytical challenges. Further, identifying such patterns and pushing analogous content to interested parties according to mission needs in real-time is a difficult problem. This paper presents the design of SKOD, a novel Situational Knowledge Query Engine that continuously builds a multi-modal relational knowledge base using SQL queries; SKOD pushes dynamic content to relevant users through triggers based on modeling of users‚Äô interests. SKOD is a scalable, real-time, on-demand situational knowledge extraction and dissemination framework that processes streams of multi-modal data utilizing publish/subscribe stream engines. The initial prototype of SKOD uses deep neural networks and natural language processing techniques to extract and model relevant objects from video streams and topics, entities and events from unstructured text resources such as Twitter and news articles. Through its extensible architecture, SKOD aims to provide a high-performance, generic framework for situational knowledge on demand, supporting effective information retrieval for evolving missions."
2018,Data Integration: The Current Status and the Way Forward.,n/a
2018,Beagle: Automated Extraction and Interpretation of Visualizations from the Web.,"""How common is interactive visualization on the web?"" ""What is the most popular visualization design?"" ""How prevalent are pie charts really?"" These questions intimate the role of interactive visualization in the real (online) world. In this paper, we present our approach (and findings) to answering these questions. First, we introduce Beagle, which mines the web for SVG-based visualizations and automatically classifies them by type (i.e., bar, pie, etc.). With Beagle, we extract over 41,000 visualizations across five different tools and repositories, and classify them with 85% accuracy, across 24 visualization types. Given this visualization collection, we study usage across tools. We find that most visualizations fall under four types: bar charts, line charts, scatter charts, and geographic maps. Though controversial, pie charts are relatively rare for the visualization tools that were studied. Our findings also suggest that the total visualization types supported by a given tool could factor into its ease of use. However this effect appears to be mitigated by providing a variety of diverse expert visualization examples to users.
"
2018,TabulaROSA: Tabular Operating System Architecture for Massively Parallel Heterogeneous Compute Engines.,"Abstract:
The rise in computing hardware choices is driving a reevaluation of operating systems. The traditional role of an operating system controlling the execution of its own hardware is evolving toward a model whereby the controlling processor is distinct from the compute engines that are performing most of the computations. In this context, an operating system can be viewed as software that brokers and tracks the resources of the compute engines and is akin to a database management system. To explore the idea of using a database in an operating system role, this work defines key operating system functions in terms of rigorous mathematical semantics (associative array algebra) that are directly translatable into database operations. These operations possess a number of mathematical properties that are ideal for parallel operating systems by guaranteeing correctness over a wide range of parallel operations. The resulting operating system equations provide a mathematical specification for a Tabular Operating System Architecture (TabulaROSA) that can be implemented on any platform. Simulations of forking in TabularROSA are performed using an associative array implementation and compared to Linux on a 32,000+ core supercomputer. Using over 262,000 forkers managing over 68,000,000,000 processes, the simulations show that TabulaROSA has the potential to perform operating system functions on a massively parallel scale. The TabulaROSA simulations show 20x higher performance as compared to Linux while managing 2000x more processes in fully searchable tables."
2018,My Top Ten Fears about the DBMS Field.,"Abstract:
In this paper, I present my top ten fears about the future of the DBMS field, with apologies to David Letterman. There are three ‚Äùbig fears‚Äù, which I discuss first. Five additional fears are a result of the ‚Äùbig three‚Äù. I then conclude with ‚Äùthe big enchilada‚Äù, which is a pair of fears. In each case, I indicate what I think is the best way to deal with the current situation."
2018,Seeping Semantics: Linking Datasets Using Word Embeddings for Data Discovery.,"Abstract:
Employees that spend more time finding relevant data than analyzing it suffer from a data discovery problem. The large volume of data in enterprises, and sometimes the lack of knowledge of the schemas aggravates this problem. Similar to how we navigate the Web, we propose to identify semantic links that assist analysts in their discovery tasks. These links relate tables to each other, to facilitate navigating the schemas. They also relate data to external data sources, such as ontologies and dictionaries, to help explain the schema meaning. We materialize the links in an enterprise knowledge graph, where they become available to analysts. The main challenge is how to find pairs of objects that are semantically related. We propose SEMPROP, a DAG of different components that find links based on syntactic and semantic similarities. SEMPROP is commanded by a semantic matcher which leverages word embeddings to find objects that are semantically related. We introduce coherent group, a technique to combine word embeddings that works better than other state of the art combination alternatives. We implement SEMPROP as part of Aurum, a data discovery system we are building, and conduct user studies, real deployments and a quantitative evaluation to understand the benefits of links for data discovery tasks, as well as the benefits of SEMPROP and coherent groups to find those links."
2018,Aurum: A Data Discovery System.,"Abstract:
Organizations face a data discovery problem when their analysts spend more time looking for relevant data than analyzing it. This problem has become commonplace in modern organizations as: i) data is stored across multiple storage systems, from databases to data lakes, to the cloud; ii) data scientists do not operate within the limits of well-defined schemas or a small number of data sources-instead, to answer complex questions they must access data spread across thousands of data sources. To address this problem, we capture relationships between datasets in an enterprise knowledge graph (EKG), which helps users to navigate among disparate sources. The contribution of this paper is AURUM, a system to build, maintain and query the EKG. To build the EKG, we introduce a Two-step process which scales to large datasets and requires only one-pass over the data, avoiding overloading the source systems. To maintain the EKG without re-reading all data every time, we introduce a resource-efficient sampling signature (RESS) method which works by only using a small sample of the data. Finally, to query the EKG, we introduce a collection of composable primitives, thus allowing users to define many different types of discovery queries. We describe our experience using AURUM in three corporate scenarios and do a performance evaluation of each component."
2018,Building Data Civilizer Pipelines with an Advanced Workflow Engine.,"Abstract:
In order for an enterprise to gain insight into its internal business and the changing outside environment, it is essential to provide the relevant data for in-depth analysis. Enterprise data is usually scattered across departments and geographic regions and is often inconsistent. Data scientists spend the majority of their time finding, preparing, integrating, and cleaning relevant data sets. Data Civilizer is an end-to-end data preparation system. In this paper, we present the complete system, focusing on our new workflow engine, a superior system for entity matching and consolidation, and new cleaning tools. Our workflow engine allows data scientists to author, execute and retrofit data preparation pipelines of different data discovery and cleaning services. Our end-to-end demo scenario is based on data from the MIT data warehouse and e-commerce data sets."
2018,P-Store: An Elastic Database System with Predictive Provisioning.,"OLTP database systems are a critical part of the operation of many enterprises. Such systems are often configured statically with sufficient capacity for peak load. For many OLTP applications, however, the maximum load is an order of magnitude larger than the minimum, and load varies in a repeating daily pattern. It is thus prudent to allocate computing resources dynamically to match demand. One can allocate resources reactively after a load increase is detected, but this places additional burden on the already-overloaded system to reconfigure. A predictive allocation, in advance of load increases, is clearly preferable.

We present P-Store, the first elastic OLTP DBMS to use prediction, and apply it to the workload of B2W Digital (B2W), a large online retailer. Our study shows that P-Store outperforms a reactive system on B2W's workload by causing 72% fewer latency violations, and achieves performance comparable to static allocation for peak demand while using 50% fewer servers."
2018,FastDAWG: Improving Data Migration in the BigDAWG Polystore System.,"Abstract
The problem of data integration has been around for decades, yet a satisfactory solution has not yet emerged. A new type of system called a polystore has surfaced to partially address the integration problem. Based on experience with our own polystore called BigDAWG, we identify three major roadblocks to an acceptable commercial solution. We offer a new architecture inspired by these three problems that trades some generality for usability. This architecture also exploits modern hardware (i.e., high-speed networks and RDMA) to gain performance. The paper concludes with some promising experimental results."
2017,What to do about database decay.,n/a
2017,An Evaluation of Distributed Concurrency Control.,"Increasing transaction volumes have led to a resurgence of interest in distributed transaction processing. In particular, partitioning data across several servers can improve throughput by allowing servers to process transactions in parallel. But executing transactions across servers limits the scalability and performance of these systems.

In this paper, we quantify the effects of distribution on concurrency control protocols in a distributed environment. We evaluate six classic and modern protocols in an in-memory distributed database evaluation framework called Deneva, providing an apples-to-apples comparison between each. Our results expose severe limitations of distributed transaction processing engines. Moreover, in our analysis, we identify several protocol-specific scalability bottlenecks. We conclude that to achieve truly scalable operation, distributed concurrency control solutions must seek a tighter coupling with either novel network hardware (in the local area) or applications (via data modeling and semantically-aware execution), or both."
2017,SilkMoth: An Efficient Method for Finding Related Sets with Maximum Matching Constraints.,"Determining if two sets are related - that is, if they have similar values or if one set contains the other -- is an important problem with many applications in data cleaning, data integration, and information retrieval. For example, set relatedness can be a useful tool to discover whether columns from two different databases are joinable; if enough of the values in the columns match, it may make sense to join them. A common metric is to measure the relatedness of two sets by treating the elements as vertices of a bipartite graph and calculating the score of the maximum matching pairing between elements. Compared to other metrics which require exact matchings between elements, this metric uses a similarity function to compare elements between the two sets, making it robust to small dissimilarities in elements and more useful for real-world, dirty data. Unfortunately, the metric suffers from expensive computational cost, taking O(n3) time, where n is the number of elements in the sets, for each set-to-set comparison. Thus for applications that try to search for all pairings of related sets in a brute-force manner, the runtime becomes unacceptably large.

To address this challenge, we developed SilkMoth, a system capable of rapidly discovering related set pairs in collections of sets. Internally, SilkMoth creates a signature for each set, with the property that any other set which is related must match the signature. SilkMoth then uses these signatures to prune the search space, so only sets that match the signatures are left as candidates. Finally, SilkMoth applies the maximum matching metric on remaining candidates to verify which of these candidates are truly related sets. An important property of SilkMoth is that it is guaranteed to output exactly the same related set pairings as the brute-force method, unlike approximate techniques. Thus, a contribution of this paper is the characterization of the space of signatures which enable this property. We show that selecting the optimal signature in this space is NP-complete, and based on insights from the characterization of the space, we propose two novel filters which help to prune the candidates further before verification. In addition, we introduce a simple optimization to the calculation of the maximum matching metric itself based on the triangle inequality. Compared to related approaches, SilkMoth is much more general, handling a larger space of similarity functions and relatedness metrics, and is an order of magnitude more efficient on real datasets."
2017,Exploring big volume sensor data with Vroom.,"State of the art sensors within a single autonomous vehicle (AV) can produce video and LIDAR data at rates greater than 30 GB/hour. Unsurprisingly, even small AV research teams can accumulate tens of terabytes of sensor data from multiple trips and multiple vehicles. AV practitioners would like to extract information about specific locations or specific situations for further study, but are often unable to. Queries over AV sensor data are different from generic analytics or spatial queries because they demand reasoning about fields of view as well as heavy computation to extract features from scenes. In this article and demo we present Vroom, a system for ad-hoc queries over AV sensor databases. Vroom combines domain specific properties of AV datasets with selective indexing and multi-query optimization to address challenges posed by AV sensor data."
2017,Approximate String Joins with Abbreviations.,"String joins have wide applications in data integration and cleaning. The inconsistency of data caused by data errors, term variations and missing values has led to the need for approximate string joins (ASJ). In this paper, we study ASJ with abbreviations, which are a frequent type of term variation. Although prior works have studied ASJ given a user-inputted dictionary of synonym rules, they have three common limitations. First, they suffer from low precision in the presence of abbreviations having multiple full forms. Second, their join algorithms are not scalable due to the exponential time complexity. Third, the dictionary may not exist since abbreviations are highly domain-dependent.

We propose an end-to-end workflow to address these limitations. There are three main components in the workflow: (1) a new similarity measure taking abbreviations into account that can handle abbreviations having multiple full forms, (2) an efficient join algorithm following the filter-verification framework and (3) an unsupervised approach to learn a dictionary of abbreviation rules from input strings. We evaluate our workflow on four real-world datasets and show that our workflow outputs accurate join results, scales well as input size grows and greatly outperforms state-of-the-art approaches in both accuracy and efficiency."
2017,The Data Civilizer System.,"In many organizations, it is often challenging for users to find relevant data for specific tasks, since the data is usually scattered across the enterprise and often inconsistent. In fact, data scientists routinely report that the majority of their effort is spent finding, cleaning, integrating, and accessing data of interest to a task at hand. In order to decrease the ìgrunt workî needed to facilitate the analysis of data ìin the wildî, we present DATA CIVILIZER, an end-to-end big data management system. DATA CIVILIZER has a linkage graph computation module to build a linkage graph for the data and a data discovery module which utilizes the linkage graph to help identify data that is relevant to user tasks. It also uses the linkage graph to discover possible join paths that can then be used in a query. For the actual query execution, we use a polystore DBMS, which federates query processing across disparate systems. In addition, DATA CIVILIZER integrates data cleaning operations into query processing. Because different users need to invoke the above tasks in different orders, DATA CIVILIZER embeds a workflow engine which enables the arbitrary composition of different modules, as well as the handling of data updates. We have deployed our preliminary DATA CIVILIZER system in two institutions, MIT and Merck and describe initial positive experiences that show the system shortens the time and effort required to find, prepare, and analyze data."
2017,Position statement: The case for a visualization performance benchmark.,"Abstract:
Visualizations are an invaluable tool in the data analysis process, as they enable scientists to explore and interpret billions of datapoints quickly, and with just a few rendered images. However, many visualization systems are unable to keep up with the unprecedented accumulation of data through remote sensors, field sensors, medical and personal devices, social networks, and more. This is due to certain assumptions that many of these tools rely on, such as the assumption that these systems can store entire datasets directly in main memory. With so many datasets massive datasets available, ranging from the NASA MODIS satellite imagery dataset[3] to the Internet Movie Database [4] to Twitter streams [1], this assumption no longer matches reality."
2017,BigDAWG version 0.1.,"Abstract:
A polystore system is a database management system composed of integrated heterogeneous database engines and multiple programming languages. By matching data to the storage engine best suited to its needs, complex analytics run faster and flexible storage choices helps improve data organization. BigDAWG (Big Data Working Group) is our prototype implementation of a polystore system. In this paper, we describe the current BigDAWG software release which supports PostgreSQL, Accumulo and SciDB. We describe the overall architecture, API and initial results of applying BigDAWG to the MIMIC II medical dataset."
2017,Database engine integration and performance analysis of the BigDAWG polystore system.,"Abstract:
The BigDAWG polystore database system aims to address workloads dealing with large, heterogeneous datasets. The need for such a system is motivated by an increase in Big Data applications dealing with disparate types of data, from large scale analytics to realtime data streams to text-based records, each suited for different storage engines. These applications often perform cross-engine queries on correlated data, resulting in complex query planning, data migration, and execution. One such application is a medical application built by the Intel Science and Technology Center (ISTC) on data collected from an intensive care unit (ICU). We present work done to add support for two commonly used database engines, Vertica and MySQL, to the BigDAWG system, as well as results and analysis from performance evaluation of the system using the TPC-H benchmark."
2017,A Demo of the Data Civilizer System.,"Finding relevant data for a specific task from the numerous data sources available in any organization is a daunting task. This is not only because of the number of possible data sources where the data of interest resides, but also due to the data being scattered all over the enterprise and being typically dirty and inconsistent. In practice, data scientists are routinely reporting that the majority (more than 80%) of their effort is spent finding, cleaning, integrating, and accessing data of interest to a task at hand. We propose to demonstrate DATA CIVILIZER to ease the pain faced in analyzing data ""in the wild"". DATA CIVILIZER is an end-to-end big data management system with components for data discovery, data integration and stitching, data cleaning, and querying data from a large variety of storage engines, running in large enterprises.
"
2016,The land sharks are on the squawk box.,"It turns out riding across America is more than a handy metaphor for building system software.
"
2016,The Beckman report on database research.,"Database researchers paint big data as a defining challenge. To make the most of the enormous opportunities at hand will require focusing on five research areas.
"
2016,Detecting Data Errors: Where are we and what needs to be done?,"Data cleaning has played a critical role in ensuring data quality for enterprise applications. Naturally, there has been extensive research in this area, and many data cleaning algorithms have been translated into tools to detect and to possibly repair certain classes of errors such as outliers, duplicates, missing values, and violations of integrity constraints. Since different types of errors may coexist in the same data set, we often need to run more than one kind of tool. In this paper, we investigate two pragmatic questions: (1) are these tools robust enough to capture most errors in real-world data sets? and (2) what is the best strategy to holistically run multiple tools to optimize the detection effort? To answer these two questions, we obtained multiple data cleaning tools that utilize a variety of error detection techniques. We also collected five real-world data sets, for which we could obtain both the raw data and the ground truth on existing errors. In this paper, we report our experimental findings on the errors detected by the tools we tested. First, we show that the coverage of each tool is well below 100%. Second, we show that the order in which multiple tools are run makes a big difference. Hence, we propose a holistic multi-tool strategy that orders the invocations of the available tools to maximize their benefit, while minimizing human effort in verifying results. Third, since this holistic approach still does not lead to acceptable error coverage, we discuss two simple strategies that have the potential to improve the situation, namely domain specific tools and data enrichment. We close this paper by reasoning about the errors that are not detectable by any of the tools we tested."
2016,Clay: Fine-Grained Adaptive Partitioning for General Database Schemas.,"Transaction processing database management systems (DBMSs) are critical for today's data-intensive applications because they enable an organization to quickly ingest and query new information. Many of these applications exceed the capabilities of a single server, and thus their database has to be deployed in a distributed DBMS. The key factor affecting such a system's performance is how the database is partitioned. If the database is partitioned incorrectly, the number of distributed transactions can be high. These transactions have to synchronize their operations over the network, which is considerably slower and leads to poor performance. Previous work on elastic database repartitioning has focused on a certain class of applications whose database schema can be represented in a hierarchical tree structure. But many applications cannot be partitioned in this manner, and thus are subject to distributed transactions that impede their performance and scalability.

In this paper, we present a new on-line partitioning approach, called Clay, that supports both tree-based schemas and more complex ""general"" schemas with arbitrary foreign key relationships. Clay dynamically creates blocks of tuples to migrate among servers during repartitioning, placing no constraints on the schema but taking care to balance load and reduce the amount of data migrated. Clay achieves this goal by including in each block a set of hot tuples and other tuples co-accessed with these hot tuples. To evaluate our approach, we integrate Clay in a distributed, main-memory DBMS and show that it can generate partitioning schemes that enable the system to achieve up to 15◊ better throughput and 99% lower latency than existing approaches."
2016,Database decay and how to avoid it.,"Abstract:
The traditional wisdom for designing database schemas is to use a design tool (typically based on a UML or E-R model) to construct an initial data model for one's data. When one is satisfied with the result, the tool will automatically construct a collection of 3rd normal form relations for the model. Then applications are coded against this relational schema. When business circumstances change (as they do frequently) one should run the tool again to produce a new data model and a new resulting collection of tables. The new schema is populated from the old schema, and the applications are altered to work on the new schema, using relational views whenever possible to ease the migration. In this way, the database remains in 3rd normal form, which represents a ‚Äúgood‚Äù schema, as defined by DBMS researchers. ‚ÄúIn the wild‚Äù, schemas often change once a quarter or more often, and the traditional wisdom is to repeat the above exercise for each alteration. In this paper we report that the traditional wisdom appears to be rarely-to-never followed for large, multi-department applications. Instead DBAs appear to attempt to minimize application maintenance (and hence schema changes) instead of maximizing schema quality. This leads to schemas which quickly diverge from E-R or UML models and actual database semantics tend to drift farther and farther from 3rd normal form. We term this divergence of reality from 3rd normal form principles database decay. Obviously, this is a very undesirable state of affairs, and should be avoided if possible. The paper continues with tactics to slow down database decay. We argue that the traditional development methodology, that of coding applications in ODBC or JDBC, is at least partly to blame for decay. Hence, we propose an alternate methodology that should be more resilient to decay."
2016,STeP: Scalable Tenant Placement for Managing Database-as-a-Service Deployments.,"Public cloud providers with Database-as-a-Service offerings must efficiently allocate computing resources to each of their customers. An effective assignment of tenants both reduces the number of physical servers in use and meets customer expectations at a price point that is competitive in the cloud market. For public cloud vendors like Microsoft and Amazon, this means packing millions of users' databases onto hundreds or thousands of servers.

This paper studies tenant placement by examining a publicly released dataset of anonymized customer resource usage statistics from Microsoft's Azure SQL Database production system over a three-month period. We implemented the STeP framework to ingest and analyze this large dataset. STeP allowed us to use this production dataset to evaluate several new algorithms for packing database tenants onto servers. These techniques produce highly efficient packings by collocating tenants with compatible resource usage patterns. The evaluation shows that under a production-sourced customer workload, these techniques are robust to variations in the number of nodes, keeping performance objective violations to a minimum even for high-density tenant packings. In comparison to the algorithm used in production at the time of data collection, our algorithms produce up to 90% fewer performance objective violations and save up to 32% of total operational costs for the cloud provider."
2016,The BigDawg monitoring framework.,"Abstract:
BigDAWG is a polystore database system designed to work with heterogenous data that may be stored in disparate database and storage engines. A central component of the BigDAWG polystore system is the ability to submit queries that may be executed in different data engines. This paper presents a monitoring framework for the BigDawg federated database system which maintains performance information on benchmark queries. As environmental conditions change, the monitoring framework updates existing performance information to match current conditions. Using this information, the monitoring system can determine the optimal query execution plan for similar incoming queries. We also describe a series of test queries that were run to assess whether the system correctly determines the optimal plans for such queries."
2016,Data transformation and migration in polystores.,"Abstract:
Ever increasing data size and new requirements in data processing has fostered the development of many new database systems. The result is that many data-intensive applications are underpinned by different engines. To enable data mobility there is a need to transfer data between systems easily and efficiently. We analyze the state-of-the-art of data migration and outline research opportunities for a rapid data transfer. Our experiments explore data migration between a diverse set of databases, including PostgreSQL, SciDB, S-Store and Accumulo. Each of the systems excels at specific application requirements, such as transactional processing, numerical computation, streaming data, and large scale text processing. Providing an efficient data migration tool is essential to take advantage of superior processing from that specialized databases. Our goal is to build such a data migration framework that will take advantage of recent advancement in hardware and software."
2016,The BigDAWG polystore system and architecture.,"Abstract:
Organizations are often faced with the challenge of providing data management solutions for large, heterogenous datasets that may have different underlying data and programming models. For example, a medical dataset may have unstructured text, relational data, time series waveforms and imagery. Trying to fit such datasets in a single data management system can have adverse performance and efficiency effects. As a part of the Intel Science and Technology Center on Big Data, we are developing a polystore system designed for such problems. BigDAWG (short for the Big Data Analytics Working Group) is a polystore system designed to work on complex problems that naturally span across different processing or storage engines. BigDAWG provides an architecture that supports diverse database systems working with different data models, support for the competing notions of location transparency and semantic completeness via islands and a middleware that provides a uniform multi-island interface. Initial results from a prototype of the BigDAWG system applied to a medical dataset validate polystore concepts. In this article, we will describe polystore databases, the current BigDAWG architecture and its application on the MIMIC II medical dataset, initial performance results and our future development plans."
2016,Cross-engine query execution in federated database systems.,"Abstract:
We have developed a reference implementation of the BigDAWG system: a new architecture for future Big Data applications, guided by the philosophy that ‚Äúone size does not fit all‚Äù. Such applications not only call for large-scale analytics, but also for real-time streaming support, smaller analytics at interactive speeds, data visualization, and cross-storage-system queries. The importance and effectiveness of such a system has been demonstrated in a hospital application using data from an intensive care unit (ICU). In this article, we describe the implementation and evaluation of the cross-system Query Executor. In particular, we focus on cross-engine shuffle joins within the BigDAWG system, and evaluate various strategies of computing them when faced with varying degrees of data skew."
2016,DataXFormer: A robust transformation discovery system.,"Abstract:
In data integration, data curation, and other data analysis tasks, users spend a considerable amount of time converting data from one representation to another. For example US dates to European dates or airport codes to city names. In a previous vision paper, we presented the initial design of DataXFormer, a system that uses web resources to assist in transformation discovery. Specifically, DataXFormer discovers possible transformations from web tables and web forms and involves human feedback where appropriate. In this paper, we present the full fledged system along with several extensions. In particular, we present algorithms to find (i) transformations that entail multiple columns of input data, (ii) indirect transformations that are compositions of other transformations, (iii) transformations that are not functions but rather relationships, and (iv) transformations from a knowledge base of public data. We report on experiments with a collection of 120 transformation tasks, and show our enhanced system automatically covers 101 of them by using openly available resources."
2016,Towards large-scale data discovery: position paper.,"With thousands of data sources spread across multiple databases and data lakes, modern organizations face a data discovery challenge. Analysts spend more time finding relevant data to answer the questions at hand than analyzing it.

In this paper we introduce a data discovery system that facilitates locating relevant data among thousands of data sources. We represent data sources succinctly through signatures, and then create search paths that permit quick execution of a set of data discovery primitives used for finding relevant data. We have built a prototype that is being used to solve data discovery challenges of two big organizations."
2016,Dynamic Prefetching of Data Tiles for Interactive Visualization.,"In this paper, we present ForeCache, a general-purpose tool for exploratory browsing of large datasets. ForeCache utilizes a client-server architecture, where the user interacts with a lightweight client-side interface to browse datasets, and the data to be browsed is retrieved from a DBMS running on a back-end server. We assume a detail-on-demand browsing paradigm, and optimize the back-end support for this paradigm by inserting a separate middleware layer in front of the DBMS. To improve response times, the middleware layer fetches data ahead of the user as she explores a dataset.

We consider two different mechanisms for prefetching: (a) learning what to fetch from the user's recent movements, and (b) using data characteristics (e.g., histograms) to find data similar to what the user has viewed in the past. We incorporate these mechanisms into a single prediction engine that adjusts its prediction strategies over time, based on changes in the user's behavior. We evaluated our prediction engine with a user study, and found that our dynamic prefetching strategy provides: (1) significant improvements in overall latency when compared with non-prefetching systems (430% improvement); and (2) substantial improvements in both prediction accuracy (25% improvement) and latency (88% improvement) relative to existing prefetching techniques."
2015,"Handling Shared, Mutable State in Stream Processing with Correctness Guarantees.","S-Store is a next-generation stream processing system that is being developed at Brown, Intel, MIT, and Portland State University. It is designed to achieve very high throughput, while maintaining a number of correctness guarantees required to handle shared, mutable state in streaming applications. This paper explores these correctness criteria and describes how S-Store achieves them, including a new model of stream processing that provides support for ACID transactions. "
2015,A Demonstration of the BigDAWG Polystore System.,"This paper presents BigDAWG, a reference implementation of a new architecture for ""Big Data"" applications. Such applications not only call for large-scale analytics, but also for real-time streaming support, smaller analytics at interactive speeds, data visualization, and cross-storage-system queries. Guided by the principle that ""one size does not fit all"", we build on top of a variety of storage engines, each designed for a specialized use case. To illustrate the promise of this approach, we demonstrate its effectiveness on a hospital application using data from an intensive care unit (ICU). This complex application serves the needs of doctors and researchers and provides real-time support for streams of patient data. It showcases novel approaches for querying across multiple storage engines, data visualization, and scalable real-time analytics"
2015,S-Store: Streaming Meets Transaction Processing.,"Stream processing addresses the needs of real-time applications. Transaction processing addresses the coordination and safety of short atomic computations. Heretofore, these two modes of operation existed in separate, stove-piped systems. In this work, we attempt to fuse the two computational paradigms in a single system called S-Store. In this way, S-Store can simultaneously accommodate OLTP and streaming applications. We present a simple transaction model for streams that integrates seamlessly with a traditional OLTP system, and provides both ACID and stream-oriented guarantees. We chose to build S-Store as an extension of H-Store - an open-source, in-memory, distributed OLTP database system. By implementing S-Store in this way, we can make use of the transaction processing facilities that H-Store already provides, and we can concentrate on the additional features that are needed to support streaming. Similar implementations could be done using other main-memory OLTP platforms. We show that we can actually achieve higher throughput for streaming workloads in S-Store than an equivalent deployment in H-Store alone. We also show how this can be achieved within H-Store with the addition of a modest amount of new functionality. Furthermore, we compare S-Store to two state-of-the-art streaming systems, Esper and Apache Storm, and show how S-Store can sometimes exceed their performance while at the same time providing stronger correctness guarantees."
2015,Temporal Rules Discovery for Web Data Cleaning.,"Declarative rules, such as functional dependencies, are widely used for cleaning data. Several systems take them as input for detecting errors and computing a ""clean"" version of the data. To support domain experts, in specifying these rules, several tools have been proposed to profile the data and mine rules. However, existing discovery techniques have traditionally ignored the time dimension. Recurrent events, such as persons reported in locations, have a duration in which they are valid, and this duration should be part of the rules or the cleaning process would simply fail.

In this work, we study the rule discovery problem for temporal web data. Such a discovery process is challenging because of the nature of web data; extracted facts are (i) sparse over time, (ii) reported with delays, and (iii) often reported with errors over the values because of inaccurate sources or non robust extractors. We handle these challenges with a new discovery approach that is more robust to noise. Our solution uses machine learning methods, such as association measures and outlier detection, for the discovery of the rules, together with an aggressive repair of the data in the mining step itself. Our experimental evaluation over real-world data from Recorded Future, an intelligence company that monitors over 700K Web sources, shows that temporal rules improve the quality of the data with an increase of the average precision in the cleaning process from 0.37 to 0.84, and a 40% relative increase in the average F-measure."
2015,The BigDAWG Polystore System.,"This paper presents a new view of federated databases to address the growing need for managing information that spans multiple data models. This trend is fueled by the proliferation of storage engines and query languages based on the observation that ‚ no one size fits all‚ . To address this shift, we propose a polystore architecture; it is designed to unify querying over multiple data models. We consider the challenges and opportunities associated with polystores. Open questions in this space revolve around query optimization and the assignment of objects to storage engines. We introduce our approach to these topics and discuss our prototype in the context of the Intel Science and Technology Center for Big Data"
2015,Dataxformer: Leveraging the Web for Semantic Transformations.,"Data transformation is a crucial step in data integration. While some transformations, such as liters to gallons, can be easily performed by applying a formula or a program on the input values, others, such as zip code to city, require sifting through a repository containing explicit value mappings. There are already powerful systems that provide formulae and algorithms for transformations. However, the automated identication of reference datasets to support value mapping remains largely unresolved. The Web is home to millions of tables with many containing explicit value mappings. This is in addition to value mappings hidden behind Web forms. In this paper, we present DataXFormer, a transformation engine that leverages Web tables and Web forms to perform transformation tasks. In particular, we describe an inductive, lter-rene approach for identifying explicit transformations in a corpus of Web tables and an approach to dynamically retrieve and wrap Web forms. Experiments show that the combination of both resource types covers more than 80% of transformation queries formulated by real-world users."
2015,By their fruits shall ye know them: A Data Analyst's Perspective on Massively Parallel System Design.,"Increasingly parallel systems promise a remedy for the current stagnation of single-core performance. However, the battle to find the most appropriate architecture for the resulting massively parallel systems is still ongoing. Currently, there are two active contenders: Massively Parallel Single Instruction Multiple Threads (SIMT) systems such as GPGPUs and Many Core Single Instruction Multiple Data (SIMD) systems such as Intel's Xeon Phi. While the former is more versatile, the latter is an efficient, time-tested technology with a clear migration path. In this study, we provide a data management perspective to the debate: we study the implementation and performance of a set of common data management operations on an SIMT device (an Nvidia GTX 780) and compare it to a Many Core SIMD system (an Intel Xeon Phi). We interpret the results to pinpoint architectural decisions and tradeoffs that lead to suboptimal performance and point out potential areas for improvement in the next generation of these devices.
"
2015,Skew-Aware Join Optimization for Array Databases.,"Science applications are accumulating an ever-increasing amount of multidimensional data. Although some of it can be processed in a relational database, much of it is better suited to array-based engines. As such, it is important to optimize the query processing of these systems. This paper focuses on efficient query processing of join operations within an array database. These engines invariably ``chunk'' their data into multidimensional tiles that they use to efficiently process spatial queries. As such, traditional relational algorithms need to be substantially modified to take advantage of array tiles. Moreover, most n-dimensional science data is unevenly distributed in array space because its underlying observations rarely follow a uniform pattern. It is crucial that the optimization of array joins be skew-aware. In addition, owing to the scale of science applications, their query processing usually spans multiple nodes. This further complicates the planning of array joins.

In this paper, we introduce a join optimization framework that is skew-aware for distributed joins. This optimization consists of two phases. In the first, a logical planner selects the query's algorithm (e.g., merge join), the granularity of the its tiles, and the reorganization operations needed to align the data. The second phase implements this logical plan by assigning tiles to cluster nodes using an analytical cost model. Our experimental results, on both synthetic and real-world data, demonstrate that this optimization framework speeds up array joins by up to 2.5X in comparison to the baseline."
2015,DataXFormer: An Interactive Data Transformation Tool.,"While syntactic transformations require the application of a formula on the input values, such as unit conversion or date format conversions, semantic transformations, such as ""zip code to city"", require a look-up in some reference data. We recently presented DataXFormer, a system that leverages Web tables, Web forms, and expert sourcing to cover a wide range of transformations. In this demonstration, we present the user-interaction with DataXFormer and show scenarios on how it can be used to transform data and explore the effectiveness and efficiency of several approaches for transformation discovery, leveraging about 112 million tables and online sources.
"
2014,S-Store: A Streaming NewSQL System for Big Velocity Applications.,"First-generation streaming systems did not pay much attention to state management via ACID transactions (e.g., [3, 4]). S-Store is a data management system that combines OLTP transactions with stream processing. To create S-Store, we begin with H-Store, a main-memory transaction processing engine, and add primitives to support streaming. This includes triggers and transaction workflows to implement push-based processing, windows to provide a way to bound the computation, and tables with hidden state to implement scoping for proper isolation. This demo explores the benefits of this approach by showing how a naive implementation of our benchmarks using only H-Store can yield incorrect results. We also show that by exploiting push-based semantics and our implementation of triggers, we can achieve significant improvement in transaction throughput. We demo two modern applications: (i) leaderboard maintenance for a version of ""American Idol"", and (ii) a city-scale bicycle rental scenario."
2014,VERTEXICA: Your Relational Friend for Graph Analytics!,"In this paper, we present Vertexica, a graph analytics tools on top of a relational database, which is user friendly and yet highly efficient. Instead of constraining programmers to SQL, Vertexica offers a popular vertex-centric query interface, which is more natural for analysts to express many graph queries. The programmers simply provide their vertex-compute functions and Vertexica takes care of efficiently executing them in the standard SQL engine. The advantage of using Vertexica is its ability to leverage the relational features and enable much more sophisticated graph analysis. These include expressing graph algorithms which are difficult in vertex-centric but straightforward in SQL and the ability to compose end-to-end data processing pipelines, including pre- and post- processing of graphs as well as combining multiple algorithms for deeper insights. Vertexica has a graphical user interface and we outline several demonstration scenarios including, interactive graph analysis, complex graph analysis, and continuous and time series analysis."
2014,Staring into the Abyss: An Evaluation of Concurrency Control with One Thousand Cores.,"Computer architectures are moving towards an era
dominated by many-core machines with dozens or even hundreds of
cores on a single chip. This unprecedented level of on-chip
parallelism introduces a new dimension to scalability that current
database management systems (DBMSs) were not designed for. In
particular, as the number of cores increases, the problem of
concurrency control becomes extremely challenging. With hundreds of
threads running in parallel, the complexity of coordinating
competing accesses to data will likely diminish the gains from
increased core counts. To better understand just how unprepared
current DBMSs are for future CPU architectures, we performed an
evaluation of concurrency control for on-line transaction
processing (OLTP) workloads on many-core chips. We implemented
seven concurrency control algorithms on a main-memory DBMS and
using computer simulations scaled our system to 1024 cores. Our
analysis shows that all algorithms fail to scale to this magnitude
but for different reasons. In each case, we identify fundamental
bottlenecks that are independent of the particular database
implementation and argue that even state-of-the-art DBMSs suffer
from these limitations. We conclude that rather than pursuing
incremental solutions, many-core chips may require a completely
redesigned DBMS architecture that is built from ground up and is
tightly coupled with the hardware. "
2014,E-Store: Fine-Grained Elastic Partitioning for Distributed Transaction Processing.,"On-line transaction processing (OLTP) database management systems (DBMSs) often serve time-varying workloads due to daily, weekly or seasonal fluctuations in demand, or because of rapid growth in demand due to a company's business success. In addition, many OLTP workloads are heavily skewed to ""hot"" tuples or ranges of tuples. For example, the majority of NYSE volume involves only 40 stocks. To deal with such fluctuations, an OLTP DBMS needs to be elastic; that is, it must be able to expand and contract resources in response to load fluctuations and dynamically balance load as hot tuples vary over time.

This paper presents E-Store, an elastic partitioning framework for distributed OLTP DBMSs. It automatically scales resources in response to demand spikes, periodic events, and gradual changes in an application's workload. E-Store addresses localized bottlenecks through a two-tier data placement strategy: cold data is distributed in large chunks, while smaller ranges of hot tuples are assigned explicitly to individual nodes. This is in contrast to traditional single-tier hash and range partitioning strategies. Our experimental evaluation of E-Store shows the viability of our approach and its efficacy under variations in load across a cluster of machines. Compared to single-tier approaches, E-Store improves throughput by up to 130% while reducing latency by 80%."
2014,Large-scale Semantic Profile Extraction.,"Web-search engines usually can be outperformed by specialized systems optimized for a specific domain or type of data. Halevy et al in [1] demonstrate a use case for a specialized spatial search of Google Fusion Tables, whereby the user searches for bike trails in the San Francisco Bay Area and can see the result on a Google map. The same query submitted to the general-purpose Google Web-search engine returns many irrelevant search results. Relevance of returned search results is a key property for any search-engine and hence an important and appreciated problem in Databases, Information Retrieval and Websearch [2, 3, 4, 5, 6, 7, 8, 9]. Users of any search-engine strongly prefer to get the most relevant search results first; otherwise, they have to spend time curating search-results. Content providers on the Web and in other settings usually exhibit a specific focus for their postings. For example, information at http://www.nasdaq.com is usually in the financial domain, Britney Spears is mostly tweeting about the music, and Business Wire often publishes about acquisitions. It is rare for a source to cover a wide variety of topics. In this paper, we introduce and demonstrate a data structure designed to capture a semantic sketch of a data source, along with algorithms and similarity measures that can be used to extract, populate, and match similar profiles. For example, a newspaper Business Wire often publishes about acquisitions and therefore has this Named Entity type highly ranked in its profile (see Table 1). We run our experiments on a corpus of 45 million Web pages Web45M, provided by the Web aggregator Recorded Future [10], extract ? 1.4 million profiles, and leverage them to outperform general-purpose Web-search on certain types of queries. The paper is organized as follows. Section 2 defines the semantic profile and describes the algorithms to extract semantic profiles. Section 3 describes a large-scale storage engines used to run the experiments. Section 4 introduces the similarity measures useful to match and find information sources that are alike. Section 5 demonstrates how the profiles can be used to improve general-purpose Web-search engines as well as for expert mining. We finish in Section 6 by discussing related and future work."
2014,Enterprise Database Applications and the Cloud: A Difficult Road Ahead.,"Abstract:
There is considerable interest in moving DBMS applications from inside enterprise data centers to the cloud, both to reduce cost and to increase flexibility and elasticity. Some of these applications are ""green field"" projects (i.e., new applications), others are existing legacy systems that must be migrated to the cloud. In another dimension, some are decision support applications while others are update-oriented. In this paper, we discuss the technical and political challenges that these various enterprise applications face when considering cloud deployment. In addition, a requirement for quality-of-service (QoS) guarantees will generate additional disruptive issues. In some circumstances, achieving good DBMS performance on current cloud architectures and future hardware technologies will be non-trivial. In summary, there is a difficult road ahead for enterprise database applications."
2014,Rethinking main memory OLTP recovery.,"Abstract:
Fine-grained, record-oriented write-ahead logging, as exemplified by systems like ARIES, has been the gold standard for relational database recovery. In this paper, we show that in modern high-throughput transaction processing systems, this is no longer the optimal way to recover a database system. In particular, as transaction throughputs get higher, ARIES-style logging starts to represent a non-trivial fraction of the overall transaction execution time. We propose a lighter weight, coarse-grained command logging technique which only records the transactions that were executed on the database. It then does recovery by starting from a transactionally consistent checkpoint and replaying the commands in the log as if they were new transactions. By avoiding the overhead of fine-grained logging of before and after images (both CPU complexity as well as substantial associated 110), command logging can yield significantly higher throughput at run-time. Recovery times for command logging are higher compared to an ARIEs-style physiological logging approach, but with the advent of high-availability techniques that can mask the outage of a recovering node, recovery speeds have become secondary in importance to run-time performance for most applications. We evaluated our approach on an implementation of TPCC in a main memory database system (VoltDB), and found that command logging can offer 1.5 x higher throughput than a main-memory optimized implementation of ARIEs-style physiological logging."
2014,Text and structured data fusion in data tamer at scale.,"Abstract:
Large-scale text data research has recently started to regain momentum [1]-[10], because of the wealth of up to date information communicated in unstructured format. For example, new information in online media (e.g. Web blogs, Twitter, Facebook, news feeds, etc) becomes instantly available and is refreshed regularly, has very broad coverage and other valuable properties unusual for other data sources and formats. Therefore, many enterprises and individuals are interested in integrating and using unstructured text in addition to their structured data."
2014,GenBase: a complex analytics genomics benchmark.,"This paper introduces a new benchmark designed to test database management system (DBMS) performance on a mix of data management tasks (joins, filters, etc.) and complex analytics (regression, singular value decomposition, etc.) Such mixed workloads are prevalent in a number of application areas including most science workloads and web analytics. As a specific use case, we have chosen genomics data for our benchmark and have constructed a collection of typical tasks in this domain. In addition to being representative of a mixed data management and analytics workload, this benchmark is also meant to scale to large dataset sizes and multiple nodes across a cluster. Besides presenting this benchmark, we have run it on a variety of storage systems including traditional row stores, newer column stores, Hadoop, and an array DBMS. We present performance numbers on all systems on single and multiple nodes, and show that performance differs by orders of magnitude between the various solutions. In addition, we demonstrate that most platforms have scalability issues. We also test offloading the analytics onto a coprocessor. The intent of this benchmark is to focus research interest in this area; to this end, all of our data, data generators, and scripts are available on our web site.
"
2014,Incremental elasticity for array databases.,"Relational databases benefit significantly from elasticity, whereby they execute on a set of changing hardware resources provisioned to match their storage and processing requirements. Such flexibility is especially attractive for scientific databases because their users often have a no-overwrite storage model, in which they delete data only when their available space is exhausted. This results in a database that is regularly growing and expanding its hardware proportionally. Also, scientific databases frequently store their data as multidimensional arrays optimized for spatial querying. This brings about several novel challenges in clustered, skew-aware data placement on an elastic shared-nothing database. In this work, we design and implement elasticity for an array database. We address this challenge on two fronts: determining when to expand a database cluster and how to partition the data within it. In both steps we propose incremental approaches, affecting a minimum set of data and nodes, while maintaining high performance. We introduce an algorithm for gradually augmenting an array database's hardware using a closed-loop control system. After the cluster adds nodes, we optimize data placement for n-dimensional arrays. Many of our elastic partitioners incrementally reorganize an array, redistributing data only to new nodes. By combining these two tools, the scientific database efficiently and seamlessly manages its monotonically increasing hardware resources."
2014,A Prolegomenon on OLTP Database Systems for Non-Volatile Memory.,"The design of a database management systemís (DBMS) architecture is predicated on the target storage hierarchy. Traditional diskoriented systems use a two-level hierarchy, with fast volatile memory used for caching, and slower, durable device used for primary storage. As such, these systems use a buffer pool and complex concurrency control schemes to mask disk latencies. Compare this to main memory DBMSs that assume all data can reside in DRAM, and thus do not need these components. But emerging non-volatile memory (NVM) technologies require us to rethink this dichotomy. Such memory devices are slightly slower than DRAM, but all writes are persistent, even after power loss. We explore two possible use cases of NVM for on-line transaction processing (OLTP) DBMSs. The first is where NVM completely replaces DRAM and the other is where NVM and DRAM coexist in the system. For each case, we compare the performance of a disk-oriented DBMS with a memory-oriented DBMS using two OLTP benchmarks. We also evaluate the performance of different recovery algorithms on these NVM devices. Our evaluation shows that in both storage hierarchies, memory-oriented systems are able to outperform their disk-oriented counterparts. However, as skew decreases the performance of the two architectures converge, showing that neither architecture is ideally suited for an NVM-based storage hierarchy."
2013,SciDB: A Database Management System for Applications with Complex Analytics.,"Abstract:
A description and discussion of the SciDB database management system focuses on lessons learned, application areas, performance comparisons against other solutions, and additional approaches to managing data and complex analytics."
2013,The VoltDB Main Memory DBMS.,"This paper describes the VoltDB DBMS as it exists in mid 2013. The focus is on design decisions, especially in concurrency control and high availability, as well as on the application areas where the system has found acceptance. Run-time performance numbers are also included. "
2013,SciDB DBMS Research at M.I.T.,"This paper presents a snapshot of some of our scientific DBMS research at M.I.T. as part of the Intel Science and Technology Center on Big Data. We focus our efforts primarily on SciDB, although some of our work can be used for any backend DBMS. We summarize our work on making SciDB elastic, providing skew-aware join strategies, and producing scalable visualizations of scientific data. "
2013,Anti-Caching: A New Approach to Database Management System Architecture.,"The traditional wisdom for building disk-based relational database management systems (DBMS) is to organize data in heavily-encoded blocks stored on disk, with a main memory block cache. In order to improve performance given high disk latency, these systems use a multi-threaded architecture with dynamic record-level locking that allows multiple transactions to access the database at the same time. Previous research has shown that this results in substantial overhead for on-line transaction processing (OLTP) applications [15].

The next generation DBMSs seek to overcome these limitations with architecture based on main memory resident data. To overcome the restriction that all data fit in main memory, we propose a new technique, called anti-caching, where cold data is moved to disk in a transactionally-safe manner as the database grows in size. Because data initially resides in memory, an anti-caching architecture reverses the traditional storage hierarchy of disk-based systems. Main memory is now the primary storage device.

We implemented a prototype of our anti-caching proposal in a high-performance, main memory OLTP DBMS and performed a series of experiments across a range of database sizes, workload skews, and read/write mixes. We compared its performance with an open-source, disk-based DBMS optionally fronted by a distributed main memory cache. Our results show that for higher skewed workloads the anti-caching architecture has a performance advantage over either of the other architectures tested of up to 9◊ for a data size 8◊ larger than memory."
2013,"Intel ""big data"" science and technology center vision and execution plan.","Intel has moved to a collaboration model with universities consisting of ""Science and Technology Centers"" (ISTCs). These are located at a ""hub"" university with participation from other universities, contain embedded Intel personnel, and are focused on some research theme. Intel held a national competition for a 5th Science and Technology center in 2012 and selected a proposal from M.I.T. with a theme of ""Big Data"". This paper presents the big data vision of this technology center and the execution plan for the first few years."
2013,Dynamic reduction of query result sets for interactive visualizaton.,"Abstract:
Modern database management systems (DBMS) have been designed to efficiently store, manage and perform computations on massive amounts of data. In contrast, many existing visualization systems do not scale seamlessly from small data sets to enormous ones. We have designed a three-tiered visualization system called ScalaR to deal with this issue. ScalaR dynamically performs resolution reduction when the expected result of a DBMS query is too large to be effectively rendered on existing screen real estate. Instead of running the original query, ScalaR inserts aggregation, sampling or filtering operations to reduce the size of the result. This paper presents the design and implementation of ScalaR, and shows results for an example application, displaying satellite imagery data stored in SciDB as the back-end DBMS."
2013,Data Curation at Scale: The Data Tamer System.,"Data curation is the act of discovering a data source(s) of interest, cleaning and transforming the new data, semantically integrating it with other local data sources, and deduplicating the resulting composite. There has been much research on the various components of curation (especially data integration and deduplication). However, there has been little work on collecting all of the curation components into an integrated end-to-end system. In addition, most of the previous work will not scale to the sizes of problems that we are nding in the eld. For example, one web aggregator requires the curation of 80,000 URLs and a second biotech company has the problem of curating 8000 spreadsheets. At this scale, data curation cannot be a manual (human) eort, but must entail machine learning approaches with a human assist only when necessary. This paper describes Data Tamer, an end-to-end curation system we have built at M.I.T. Brandeis, and Qatar Computing Research Institute (QCRI). It expects as input a sequence of data sources to add to a composite being constructed over time. A new source is subjected to machine learning algorithms to perform attribute identication, grouping of attributes into tables, transformation of incoming data and deduplication. When necessary, a human can be asked for guidance. Also, Data Tamer includes a data visualization component so a human can examine a data source at will and specify manual transformations. We have run Data Tamer on three real world enterprise curation problems, and it has been shown to lower curation cost by about 90%, relative to the currently deployed production software."
2013,Standards for graph algorithm primitives.,"Abstract:
It is our view that the state of the art in constructing a large collection of graph algorithms in terms of linear algebraic operations is mature enough to support the emergence of a standard set of primitive building blocks. This paper is a position paper defining the problem and announcing our intention to launch an open effort to define this standard."
2013,SubZero: A fine-grained lineage system for scientific databases.,"Abstract:
Data lineage is a key component of provenance that helps scientists track and query relationships between input and output data. While current systems readily support lineage relationships at the file or data array level, finer-grained support at an array-cell level is impractical due to the lack of support for user defined operators and the high runtime and storage overhead to store such lineage. We interviewed scientists in several domains to identify a set of common semantics that can be leveraged to efficiently store fine-grained lineage. We use the insights to define lineage representations that efficiently capture common locality properties in the lineage data, and a set of APIs so operator developers can easily export lineage information from user defined operators. Finally, we introduce two benchmarks derived from astronomy and genomics, and show that our techniques can reduce lineage query costs by up to 10√ó while incuring substantially less impact on workflow runtime and storage."
2013,We are drowning in a sea of least publishable units (LPUs).,"Our field is drowning in a sea of conference submissions. We assert that the sheer number of papers has begun to seriously hurt the quality of the work that the field is doing and that the field is going to implode unless we take action to remedy the situation. In order to improve the quality of the papers being published we must reduce the number being submitted. This will require a change in the culture of our field where ""more"" is being equated to ""better"" by both hiring and promotion committees. In this panel we will explore some ideas for correcting the situation.
"
2012,A Demonstration of DBWipes: Clean as You Query.,"As data analytics becomes mainstream, and the complexity of the underlying data and computation grows, it will be increasingly important to provide tools that help analysts understand the underlying reasons when they encounter errors in the result. While data provenance has been a large step in providing tools to help debug complex workflows, its current form has limited utility when debugging aggregation operators that compute a single output from a large collection of inputs. Traditional provenance will return the entire input collection, which has very low precision. In contrast, users are seeking precise descriptions of the inputs that caused the errors. We propose a Ranked Provenance System, which identifies subsets of inputs that influenced the output error, describes each subset with human readable predicates and orders them by contribution to the error. In this demonstration, we will present DBWipes, a novel data cleaning system that allows users to execute aggregate queries, and interactively detect, understand, and clean errors in the query results. Conference attendees will explore anomalies in campaign donations from the current US presidential election and in readings from a 54-node sensor deployment. "
2012,EarthDB: scalable analysis of MODIS data using SciDB.,"arth scientists are increasingly experiencing difficulties with analyzing rapidly growing volumes of complex data. Those who must perform analysis directly on low-level National Aeronautics and Space Administration (NASA) Moderate Resolution Imaging Spectroradiometer (MODIS) Level 1B calibrated and geolocated data, for example, encounter an arcane, high-volume data set that is burdensome to make use of. Instead, Earth scientists typically opt to use higher-level ""canned"" products provided by NASA. However, when these higher-level products fail to meet the requirements of a particular project, a cruel dilemma arises: cope with data products that don't exactly meet the project's needs or spend an enormous amount of resources extracting what is needed from the unadulterated low-level data. In this paper, we present EarthDB, a system that eliminates this dilemma by offering the following contributions:
1. Enabling painless importing of MODIS Level 1B data into SciDB, a highly scalable science-oriented database platform that abstracts away the complexity of distributed storage and analysis of complex multi-dimensional data,
2. Defining a schema that unifies storage and representation of MODIS Level 1B data, regardless of its source file,
3. Supporting fast filtering and analysis of MODIS data through the use of an intuitive, high-level query language rather than complex procedural programming and,
4. Providing the ability to easily define and reconfigure entire analysis pipelines within the SciDB database, allowing for rapid ad-hoc analysis. To demonstrate this ability, we provide sample benchmarks for the construction of true-color (RGB) and Normalized Difference Vegetative Index (NDVI) images from raw MODIS Level 1B data using relatively simple queries with scalable performance."
2012,The Future of Scientific Data Bases.,"Abstract:
For many decades, users in scientific fields (domain scientists) have resorted to either home-grown tools or legacy software for the management of their data. Technological advancements nowadays necessitate many of the properties such as data independence, scalability, and functionality found in the roadmap of DBMS technology, DBMS products, however, are not yet ready to address scientific application and user needs. Recent efforts toward building a science DBMS indicate that there is a long way ahead of us, paved by a research agenda that is rich in interesting and challenging problems."
2012,Efficient Versioning for Scientific Array Databases.,"Abstract:
In this paper, we describe a versioned database storage manager we are developing for the SciDB scientific database. The system is designed to efficiently store and retrieve array-oriented data, exposing a ""no-overwrite"" storage model in which each update creates a new ""version"" of an array. This makes it possible to perform comparisons of versions produced at different times or by different algorithms, and to create complex chains and trees of versions. We present algorithms to efficiently encode these versions, minimizing storage space while still providing efficient access to the data. Additionally, we present an optimal algorithm that, given a long sequence of versions, determines which versions to encode in terms of each other (using delta compression) to minimize total storage space or query execution cost. We compare the performance of these algorithms on real world data sets from the National Oceanic and Atmospheric Administration (NOAA), Open Street Maps, and several other sources. We show that our algorithms provide better performance than existing version control systems not optimized for array data, both in terms of storage size and access time, and that our delta-compression algorithms are able to substantially reduce the total storage space when versions exist with a high degree of similarity."
2011,10 rules for scalable performance in 'simple operation' datastores.,"Partition data and operations, keep administration simple, do not assume one size fits all. "
2011,The Architecture of SciDB.,"Abstract
SciDB is an open-source analytical database oriented toward the data management needs of scientists. As such it mixes statistical and linear algebra operations with data management ones, using a natural nested multidimensional array data model. We have been working on the code for two years, most recently with the help of venture capital backing. Release 11.06 (June 2011) is downloadable from our website (SciDB.org).
This paper presents the main design decisions of SciDB. It focuses on our decisions concerning a high-level, SQL-like query language, the issues facing our query optimizer and executor and efficient storage management for arrays. The paper also discusses implementation of features not usually present in DBMSs, including version control, uncertainty and provenance."
2010,MapReduce and parallel DBMSs: friends or foes?,"MapReduce complements DBMSs since databases are not designed for extract-transform-load tasks, a MapReduce specialty. "
2009,The Claremont report on database research.,"Database research is expanding, with major efforts in system architecture, new languages, cloud services, mobile and virtual worlds, and interplay between structure and text.
"
2009,A Demonstration of SciDB: A Science-Oriented DBMS.,"In CIDR 2009, we presented a collection of requirements for SciDB, a DBMS that would meet the needs of scientific users. These included a nested-array data model, science-specific operations such as regrid, and support for uncertainty, lineage, and named versions. In this paper, we present an overview of SciDB's key features and outline a demonstration of the first version of SciDB on data and operations from one of our lighthouse users, the Large Synoptic Survey Telescope (LSST). "
2009,Requirements for Science Data Bases and SciDB.,"For the past year, we have been assembling requirements from a collection of scientific data base users from astronomy, particle physics, fusion, remote sensing, oceanography, and biology. The intent has been to specify a common set of requirements for a new science data base system, which we call SciDB. In addition, we have discovered that very complex business analytics share most of the same requirements as ìbig scienceî. We have also constructed a partnership of companies to fund the development of SciDB, including eBay, the Large Synoptic Survey Telescope (LSST), Microsoft, the Stanford Linear Accelerator Center (SLAC) and Vertica. Lastly, we have identified two ìlighthouse customersî (LSST and eBay) who will run the initial system, once it is constructed. In this paper, we report on the requirements we have identified and briefly sketch some of the SciDB design."
2009,A comparison of approaches to large-scale data analysis.,n/a
2009,A New Direction for TPC?,"Abstract
This paper gives the author‚Äôs opinion concerning the contributions the Transaction Processing Council (TPC) has made in the past, how it is viewed in the present by me and my colleagues, and offers some suggestions on where it should go in the future. In short, TPC has become vendor-dominated, and it is time for TPC to reinvent itself to serve its customer community."
2008,Technical perspective - One size fits all: an idea whose time has come and gone.,"The last 25 years of commercial DBMS development can be summed up in a single phrase: ìOne size fits allî. This phrase refers to the fact that the traditional DBMS architecture (originally designed and optimized for business data processing) has been used to support many data-centric applications with widely varying characteristics and requirements. In this paper, we argue that this concept is no longer applicable to the database market, and that the commercial world will fracture into a collection of independent database engines, some of which may be unified by a common front-end parser. We use examples from the stream-processing market and the datawarehouse market to bolster our claims. We also briefly discuss other markets for which the traditional architecture is a poor fit and argue for a critical rethinking of the current factoring of systems services into products."
2008,"H-store: a high-performance, distributed main memory transaction processing system.","Our previous work has shown that architectural and application shifts have resulted in modern OLTP databases increasingly falling short of optimal performance [10]. In particular, the availability of multiple-cores, the abundance of main memory, the lack of user stalls, and the dominant use of stored procedures are factors that portend a clean-slate redesign of RDBMSs. This previous work showed that such a redesign has the potential to outperform legacy OLTP databases by a significant factor. These results, however, were obtained using a bare-bones prototype that was developed just to demonstrate the potential of such a system. We have since set out to design a more complete execution platform, and to implement some of the ideas presented in the original paper. Our demonstration presented here provides insight on the development of a distributed main memory OLTP database and allows for the further study of the challenges inherent in this operating environment."
2008,Why did Jim Gray win the Turing Award?,"This short paper is intended to describe for the layman why Jim Gray won so many awards, culminating in his being selected to receive the 1998 ACM Turing Award, arguably the ""Nobel Prize of Computer Science"". It briefly summarizes his main contributions to our field. "
2008,The Claremont report on database research.,"In late May, 2008, a group of database researchers, architects, users and pundits met at the Claremont Resort in Berkeley, California to discuss the state of the research field and its impacts on practice. This was the seventh meeting of this sort in twenty years, and was distinguished by a broad consensus that we are at a turning point in the history of the field, due both to an explosion of data and usage scenarios, and to major shifts in computing hardware and platforms. Given these forces, we are at a time of opportunity for research impact, with an unusually large potential for influential results across computing, the sciences and society. This report details that discussion, and highlights the group's consensus view of new focus areas, including new database engine architectures, declarative programming languages, the interplay of structured and unstructured data, cloud data services, and mobile and virtual worlds. We also report on discussions of the community's growth, including suggestions for changes in community processes to move the research agenda forward, and to enhance impact on a broader audience.
"
2008,Fault-tolerance in the borealis distributed stream processing system.,"Over the past few years, Stream Processing Engines (SPEs) have emerged as a new class of software systems, enabling low latency processing of streams of data arriving at high rates. As SPEs mature and get used in monitoring applications that must continuously run (e.g., in network security monitoring), a significant challenge arises: SPEs must be able to handle various software and hardware faults that occur, masking them to provide high availability (HA). In this article, we develop, implement, and evaluate DPC (Delay, Process, and Correct), a protocol to handle crash failures of processing nodes and network failures in a distributed SPE.

Like previous approaches to HA, DPC uses replication and masks many types of node and network failures. In the presence of network partitions, the designer of any replication system faces a choice between providing availability or data consistency across the replicas. In DPC, this choice is made explicit: the user specifies an availability bound (no result should be delayed by more than a specified delay threshold even under failure if the corresponding input is available), and DPC attempts to minimize the resulting inconsistency between replicas (not all of which might have seen the input data) while meeting the given delay threshold. Although conceptually simple, the DPC protocol tolerates the occurrence of multiple simultaneous failures as well as any further failures that occur during recovery.

This article describes DPC and its implementation in the Borealis SPE. We show that DPC enables a distributed SPE to maintain low-latency processing at all times, while also achieving eventual consistency, where applications eventually receive the complete and correct output streams. Furthermore, we show that, independent of system size and failure location, it is possible to handle failures almost up-to the user-specified bound in a manner that meets the required availability without introducing any inconsistency."
2008,"OLTP through the looking glass, and what we found there.","Online Transaction Processing (OLTP) databases include a suite of features - disk-resident B-trees and heap files, locking-based concurrency control, support for multi-threading - that were optimized for computer technology of the late 1970's. Advances in modern processors, memories, and networks mean that today's computers are vastly different from those of 30 years ago, such that many OLTP databases will now fit in main memory, and most OLTP transactions can be processed in milliseconds or less. Yet database architecture has changed little.

Based on this observation, we look at some interesting variants of conventional database systems that one might build that exploit recent hardware trends, and speculate on their performance through a detailed instruction-level breakdown of the major components involved in a transaction processing database system (Shore) running a subset of TPC-C. Rather than simply profiling Shore, we progressively modified it so that after every feature removal or optimization, we had a (faster) working system that fully ran our workload. Overall, we identify overheads and optimizations that explain a total difference of about a factor of 20x in raw performance. We also show that there is no single ""high pole in the tent"" in modern (memory resident) database systems, but that substantial time is spent in logging, latching, locking, B-tree, and buffer management operations."
2007,Architecture of a Database System.,n/a
2007,One Size Fits All? Part 2: Benchmarking Studies.,"Two years ago, some of us wrote a paper predicting the demise of ""One Size Fits All (OSFA)"". In that paper, we examined the stream processing and data warehouse markets and gave reasons for a substantial performance advantage to specialized architectures in both markets. Herein, we make three additional contributions. First, we present reasons why the same performance advantage is enjoyed by specialized implementations in the text processing market. Second, the major contribution of the paper is to show ""apples to apples"" performance numbers between commercial implementations of specialized architectures and relational DBMSs in both stream processing and data warehouses. Finally, we also show comparison numbers between an academic prototype of a specialized architecture for scientific and intelligence applications, a relational DBMS, and a widely used mathematical computation tool. In summary, there appear to be at least four markets where specialized architectures enjoy an overwhelming performance advantage."
2007,The End of an Architectural Era (It's Time for a Complete Rewrite).,"In previous papers [SC05, SBC+07], some of us predicted the end of ""one size fits all"" as a commercial relational DBMS paradigm. These papers presented reasons and experimental evidence that showed that the major RDBMS vendors can be outperformed by 1--2 orders of magnitude by specialized engines in the data warehouse, stream processing, text, and scientific database markets.

Assuming that specialized engines dominate these markets over time, the current relational DBMS code lines will be left with the business data processing (OLTP) market and hybrid markets where more than one kind of capability is required. In this paper we show that current RDBMSs can be beaten by nearly two orders of magnitude in the OLTP market as well. The experimental evidence comes from comparing a new OLTP prototype, H-Store, which we have built at M.I.T. to a popular RDBMS on the standard transactional benchmark, TPC-C.

We conclude that the current RDBMS code lines, while attempting to be a ""one size fits all"" solution, in fact, excel at nothing. Hence, they are 25 year old legacy code lines that should be retired in favor of a collection of ""from scratch"" specialized engines. The DBMS vendors (and the research community) should start with a clean sheet of paper and design systems for tomorrow's requirements, not continue to push code lines and architectures designed for yesterday's needs."
2006,Data integration through transform reuse in the Morpheus project.,"We discuss Morpheus, a data transformation construction tool and associated repository. The architecture of Morpheus is motivated by the goal to reuse (pieces of) previously written transformations to solve data integration problems by finding relevant ones in the repository and then modifying them for repurposing. In addition, Morpheus is integrated with a DBMS so as to leverage existing capabilities including the runtime environment for transforms. We discuss the architecture of Morpheus and illustrate its usage with the help of a simple transform construction scenario.
"
2005,The Lowell database research self-assessment.,"Database needs are changing, driven by the Internet and increasing amounts of scientific and sensor data. In this article, the authors propose research into several important new directions for database management systems.
"
2005,The 8 requirements of real-time stream processing.,"Applications that require real-time processing of high-volume data steams are pushing the limits of traditional data processing infrastructures. These stream-based applications include market feed processing and electronic trading on Wall Street, network and infrastructure monitoring, fraud detection, and command and control in military environments. Furthermore, as the ""sea change"" caused by cheap micro-sensor technology takes hold, we expect to see everything of material significance on the planet get ""sensor-tagged"" and report its state or location in real time. This sensorization of the real world will lead to a ""green field"" of novel monitoring and control applications with high-volume and low-latency processing requirements.Recently, several technologies have emerged---including off-the-shelf stream processing engines---specifically to address the challenges of processing high-volume, real-time data without requiring the use of custom code. At the same time, some existing software technologies, such as main memory DBMSs and rule engines, are also being ""repurposed"" by marketing departments to address these applications.In this paper, we outline eight requirements that a system software should meet to excel at a variety of real-time stream processing applications. Our goal is to provide high-level guidance to information technologists so that they will know what to look for when evaluation alternative stream processing solutions. As such, this paper serves a purpose comparable to the requirements papers in relational DBMSs and on-line analytical processing. We also briefly review alternative system software technologies in the context of our requirements.The paper attempts to be vendor neutral, so no specific commercial products are mentioned.
"
2005,"""One Size Fits All"": An Idea Whose Time Has Come and Gone (Abstract).","Abstract:
The last 25 years of commercial DBMS development can be summed up in a single phrase: ""one size fits all"". This phrase refers to the fact that the traditional DBMS architecture (originally designed and optimized for business data processing) has been used to support many data-centric applications with widely varying characteristics and requirements. In this paper, we argue that this concept is no longer applicable to the database market, and that the commercial world will fracture into a collection of independent database engines, some of which may be unified by a common front-end parser. We use examples from the stream-processing market and the data-warehouse market to bolster our claims. We also briefly discuss other markets for which the traditional architecture is a poor fit and argue for a critical rethinking of the current factoring of systems services into products."
2005,THALIA: Test Harness for the Assessment of Legacy Information Integration Approaches.,"Abstract:
We introduce our new, publicly available testbed and benchmark called THALIA (Test Harness for the Assessment of Legacy information Integration Approaches) for testing and evaluating integration technologies. THALIA provides researchers with a collection of 40 downloadable data sources representing University course catalogs from computer science departments worldwide. In addition, THALIA currently provides a set of twelve challenge queries as well as a scoring function for ranking the performance of an integration system. A second contribution is a systematic classification of the types of syntactic and semantic heterogeneities, which directly lead to the twelve challenge. We have chosen course information as our domain of discourse because it is well known and easy to understand. Furthermore, there is an abundance of data sources publicly available that allowed us to develop a testbed exhibiting all of the syntactic and semantic heterogeneities that we have identified."
2005,High-Availability Algorithms for Distributed Stream Processing.,"Abstract:
Stream-processing systems are designed to support an emerging class of applications that require sophisticated and timely processing of high-volume data streams, often originating in distributed environments. Unlike traditional data-processing applications that require precise recovery for correctness, many stream-processing applications can tolerate and benefit from weaker recovery guarantees. In this paper, we study various recovery guarantees and pertinent recovery techniques that can meet the correctness and performance requirements of stream-processing applications. We discuss the design and algorithmic challenges associated with the proposed recovery techniques and describe how each can provide different guarantees with proper combinations of redundant processing, checkpointing, and remote logging. Using analysis and simulations, we quantify the cost of our recovery guarantees and examine the performance and applicability of the recovery techniques. We also analyze how the knowledge of query network properties can help decrease the cost of high availability."
2005,Fault-tolerance in the Borealis distributed stream processing system.,"We present a replication-based approach to fault-tolerant distributed stream processing in the face of node failures, network failures, and network partitions. Our approach aims to reduce the degree of inconsistency in the system while guaranteeing that available inputs capable of being processed are processed within a specified time threshold. This threshold allows a user to trade availability for consistency: a larger time threshold decreases availability but limits inconsistency, while a smaller threshold increases availability but produces more inconsistent results based on partial data. In addition, when failures heal, our scheme corrects previously produced results, ensuring eventual consistency.Our scheme uses a data-serializing operator to ensure that all replicas process data in the same order, and thus remain consistent in the absence of failures. To regain consistency after a failure heals, we experimentally compare approaches based on checkpoint/redo and undo/redo techniques and illustrate the performance trade-offs between these schemes.
"
2005,C-Store: A Column-oriented DBMS.,"This paper presents the design of a read-optimized relational DBMS that contrasts sharply with most current systems, which are write-optimized. Among the many differences in its design are: storage of data by column rather than by row, careful coding and packing of objects into storage including main memory during query processing, storing an overlapping collection of column-oriented projections, rather than the current fare of tables and indexes, a non-traditional implementation of transactions which includes high availability and snapshot isolation for read-only transactions, and the extensive use of bitmap indexes to complement B-tree structures.We present preliminary performance data on a subset of TPC-H and show that the system we are building, C-Store, is substantially faster than popular commercial products. Hence, the architecture looks very encouraging."
2004,Retrospective on Aurora.,"Abstract.
This experience paper summarizes the key lessons we learned throughout the design and implementation of the Aurora stream-processing engine. For the past 2 years, we have built five stream-based applications using Aurora. We first describe in detail these applications and their implementation in Aurora. We then reflect on the design of Aurora based on this experience. Finally, we discuss our initial ideas on a follow-on project, called Borealis, whose goal is to eliminate the limitations of Aurora as well as to address new key challenges and applications in the stream-processing domain."
2004,Contract-Based Load Management in Federated Distributed Systems.,"Abstract
This paper focuses on load management in loosely-coupled federated distributed systems. We present a distributed mechanism for moving load between autonomous participants using bilateral contracts that are negotiated offline and that set bounded prices for moving load. We show that our mechanism has good incentive properties, efficiently redistributes excess load, and has a low overhead in practice.
Our load management mechanism is especially well-suited for distributed stream-processing applications, an emerging class of data-intensive applications that employ a ""continuous query processing"" model. In this model, streams of data are processed and composed continuously as they arrive rather than after they are indexed and stored. We have implemented the mechanism in the Medusa distributed stream processing system, and we demonstrate its properties using simulations and experiments."
2004,Load Management and High Availability in the Medusa Distributed Stream Processing System.,"Medusa [3, 6] is a distributed stream processing system based on the Aurora single-site stream processing engine [1]. We demonstrate how Medusa handles time-varying load spikes and provides high availability in the face of network partitions. We demonstrate Medusa in the context of Borealis, a second generation stream processing engine based on Aurora and Medusa.
"
2004,Linear Road: A Stream Data Management Benchmark.,"This paper specifies the Linear Road Benchmark for Stream Data Management Systems (SDMS). Stream Data Management Systems process streaming data by executing continuous and historical queries while producing query results in real-time. This benchmark makes it possible to compare the performance characteristics of SDMS' relative to each other and to alternative (e.g., Relational Database) systems. Linear Road has been endorsed as an SDMS benchmark by the developers of both the Aurora [1] (out of Brandeis University, Brown University and MIT) and STREAM [8] (out of Stanford University) stream systems.

Linear Road simulates a toll system for the motor vehicle expressways of a large metropolitan area. The tolling system uses ""variable tolling"" [6, 11, 9]: an increasingly prevalent tolling technique that uses such dynamic factors as traffic congestion and accident proximity to calculate toll charges. Linear Road specifies a variable tolling system for a fictional urban area including such features as accident detection and alerts, traffic congestion measurements, toll calculations and historical queries. After specifying the benchmark, we describe experimental results involving two implementations: one using a commercially available Relational Database and the other using Aurora. Our results show that a dedicated Stream Data Management System can outperform a Relational Database by at least a factor of 5 on streaming data applications."
2003,The Aurora and Medusa Projects.,n/a
2003,Aurora: a new model and architecture for data stream management.,"Abstract.
This paper describes the basic processing model and architecture of Aurora, a new system to manage data streams for monitoring applications. Monitoring applications differ substantially from conventional business data processing. The fact that a software system must process and react to continual inputs from many sources (e.g., sensors) rather than from human operators requires one to rethink the fundamental architecture of a DBMS for this application area. In this paper, we present Aurora, a new DBMS currently under construction at Brandeis University, Brown University, and M.I.T. We first provide an overview of the basic Aurora model and architecture and then describe in detail a stream-oriented set of operators."
2003,Visionary: A Next Generation Visualization System for Databases.,n/a
2003,Aurora: A Data Stream Management System.,"The Aurora system [1] is an experimental data stream management system with a fully functional prototype. It includes both a graphical development environment, and a runtime system. We propose to demonstrate the Aurora system with its development environment and runtime system, with several example monitoring applications developed in consultation with defense, financial, and natural science communities. We will also demonstrate the effect of various system alternatives on various workloads. For example, we will show how different scheduling algorithms affect tuple latency and internal queue lengths. We will use some of our visualization tools to accomplish this. Data Stream Management Aurora is a data stream management system for monitoring applications. Streams are continuous data feeds from such sources as sensors, satellites and stock feeds. Monitoring applications track the data from numerous streams, filtering them for signs of abnormal activity and processing them for purposes of aggregation, reduction and correlation. The management requirements for monitoring applications differ profoundly from those satisfied by a traditional DBMS: o A traditional DBMS assumes a passive model where most data processing results from humans issuing transactions and queries. Data stream management requires a more active approach, monitoring data feeds from unpredictable external sources (e.g., sensors) and alerting humans when abnormal activity is detected. o A traditional DBMS manages data that is currently in its tables. Data stream management often requires processing data that is bounded by some finite window of values, and not over an unbounded past. o A traditional DBMS provides exact answers to exact queries, and is blind to real-time deadlines. Data stream management often must respond to real-time deadlines (e.g., military applications monitoring positions of enemy platforms) and therefore must often provide reasonable approximations to queries. o A traditional query processor optimizes all queries in the same way (typically focusing on response time). A stream data manager benefits from application specific optimization criteria (QoS). o A traditional DBMS assumes pull-based queries to be the norm. Push-based data processing is the norm for a data stream management system. A Brief Summary of Aurora Aurora has been designed to deal with very large numbers of data streams. Users build queries out of a small set of operators (a.k.a. boxes). The current implementation provides a user interface for tapping into pre-existing inputs and network flows and for wiring boxes together to produces answers at the outputs. While it is certainly possible to accept input as declarative queries, we feel that for a very large number of such queries, the process of common sub-expression elimination is too difficult. An example of an Aurora network is given in Screen Shot 1. A simple stream is a potentially infinite sequence of tuples that all have the same stream ID. An arc carries multiple simple streams. This is important so that simple streams can be added and deleted from the system without having to modify the basic network. A query, then, is a sub-network that ends at a single output and includes an arbitrary number of inputs. Boxes can connect to multiple downstream boxes. All such path splits carry identical tuples. Multiple streams can be merged since some box types accept more than one input (e.g., Join, Union). We do not allow any cycles in an operator network. Each output is supplied with a Quality of Service (QoS) specification. Currently, QoS is captured by three functions (1) a latency graph, (2) a value-based graph, and (3) a loss-tolerance graph. The latency graph indicates how utility drops as an answer is delayed. The value-based graph shows which values of the output space are most important. The loss-tolerance graph is a simple way to describe how averse the application is to approximate answers. Tuples arrive at the input and are queued for processing. A scheduler selects a box with waiting tuples and executes that box on one or more of the input tuples. The output tuples of a box are queued at the input of the next box in sequence. In this way, tuples make their way from the inputs to the outputs. If the system is overloaded, QoS is adversely affected. In this case, we invoke a load shedder to strategically eliminate Aurora supports persistent storage in two different ways. First, when box queues consume more storage than available RAM, the system will spill tuples that are less likely to be needed soon to secondary storage. Second, ad hoc queries can be connected to (and disconnected from) any arc for which a connection point has been defined. A connection point stores a historical portion of a stream that has flowed on the arc. For example, one could define a connection point as the last hourís worth of data that has been seen on a given arc. Any ad hoc query that connects to a connection point has access to the full stored history as well as any additional data that flows past while the query is connected."
2003,Load Shedding in a Data Stream Manager.,"A Data Stream Manager accepts push-based inputs from a set of data sources, processes these inputs with respect to a set of standing queries, and produces outputs based on Quality-of-Service (QoS) specifications. When input rates exceed system capacity, the system will become overloaded and latency will deteriorate. Under these conditions, the system will shed load, thus degrading the answer, in order to improve the observed latency of the results. This paper examines a technique for dynamically inserting and removing drop operators into query plans as required by the current load. We examine two types of drops: the first drops a fraction of the tuples in a randomized fashion, and the second drops tuples based on the importance of their content. We address the problems of determining when load shedding is needed, where in the query plan to insert drops, and how much of the load should be shed at that point in the plan. We describe efficient solutions and present experimental evidence that they can bring the system back into the useful operating range with minimal degradation in answer quality."
2003,Operator Scheduling in a Data Stream Manager.,"Many stream-based applications have sophisticated data processing requirements and real-time performance expectations that need to be met under high-volume, time-varying data streams. In order to address these challenges, we propose novel operator scheduling approaches that specify (1) which operators to schedule (2) in which order to schedule the operators, and (3) how many tuples to process at each execution step. We study our approaches in the context of the Aurora data stream manager.

We argue that a fine-grained scheduling approach in combination with various scheduling techniques (such as batching of operators and tuples) can significantly improve system efficiency by reducing various system overheads. We also discuss application-aware extensions that make scheduling decisions according to per-application Quality of Service (QoS) specifications. Finally, we present prototype-based experimental results that characterize the efficiency and effectiveness of our approaches under various stream workloads and processing scenarios"
2002,Too Much Middleware.,"The movement from client-server computing to multi-tier computing has created a potpourri of so-called middleware systems, including application servers, workflow products, EAI systems, ETL systems and federated data systems. In this paper we argue that the explosion in middleware has created a myriad of poorly integrated systems with overlapping functionality. The world would be well served by considerable consolidation, and we present some of the ways this might happen. Some of the points covered in this paper have been previously explored in [BERN96]."
2002,Monitoring Streams - A New Class of Data Management Applications.,"This paper introduces monitoring applications, which we will show differ substantially from conventional business data processing. The fact that a software system must process and react to continual inputs from many sources (e.g., sensors) rather than from human operators requires one to rethink the fundamental architecture of a DBMS for this application area. In this paper, we present Aurora, a new DBMS that is currently under construction at Brandeis University, Brown University, and M.I.T. We describe the basic system architecture, a stream-oriented set of operators, optimization tactics, and support for real-time operation."
2001,DataSplash: A Direct Manipulation Environment for Programming Semantic Zoom Visualizations of Tabular Data.,"Abstract
We describe DataSplash, a direct manipulation system for creating semantic zoom visualizations of tabular (relational) data. DataSplash makes contributions in three areas that are key to the construction of such visualizations. First, DataSplash helps users graphically specify the visual appearance of groups of objects. Second, the system helps users visually program the way the appearance of groups of objects changes as users browse the visualization. Third, DataSplash allows users to create groups of graphical links between canvases. These direct manipulation facilities simplify the process of constructing semantic zoom applications, particularly ones that display large data sets."
2001,"The Semantic Web As ""Perfection Seeking"": A View from Drug Terminology.","To date, the Semantic Web has viewed formal terminology, or ontology, as either immutable, or something that can change but that has no past and no future -- only a present. Change, or process -- such as ""perfection seeking,"" is outside the scope of the proposed ""semantics,"" except in so far as it is represented in attributes. In contrast, current U.S. Government efforts to formalize drug (medication) terminology are being driven by the need to manage changes in this terminology asynchronously and longitudinally. For example, each year the FDA (Federal Drug Administration) approves about 150 new drugs and thousands of changes to the ""label"" of existing drugs, the VHA (Veterans Health Administration) must manage new drugs, label changes, and tens of thousands of drug ""packaging"" changes, and the NLM (National Library of Medicine) must maintain a current index of references to proposed or approved medications in the world's biomedical literature. We propose that an emerging multi-federal-agency reference terminology model for medications, mRT, be used to drive development of the necessary repertoire of ""semantic"" change management mechanisms for the Semantic Web, and that these ""process"" mechanisms be organized into an ontology of change."
2001,"The Semantic Web as ""Perfection Seeking"": A View from Drug Terminology.","To date, the Semantic Web has viewed formal terminology, or ontology, as either immutable, or something that can change but that has no past and no future -- only a present. Change, or process -- such as ""perfection seeking,"" is outside the scope of the proposed ""semantics,"" except in so far as it is represented in attributes. In contrast, current U.S. Government efforts to formalize drug (medication) terminology are being driven by the need to manage changes in this terminology asynchronously and longitudinally. For example, each year the FDA (Federal Drug Administration) approves about 150 new drugs and thousands of changes to the ""label"" of existing drugs, the VHA (Veterans Health Administration) must manage new drugs, label changes, and tens of thousands of drug ""packaging"" changes, and the NLM (National Library of Medicine) must maintain a current index of references to proposed or approved medications in the world's biomedical literature. We propose that an emerging multi-federal-agency reference terminology model for medications, mRT, be used to drive development of the necessary repertoire of ""semantic"" change management mechanisms for the Semantic Web, and that these ""process"" mechanisms be organized into an ontology of change."
2001,Content Integration for E-Business.,"We define the problem of content integration for E-Business, and show how it differs in fundamental ways from traditional issues surrounding data integration, application integration, data warehousing and OLTP. Content integration includes catalog integration as a special case, but encompasses a broader set of applications and challenges. We explore the characteristics of content integration and required services for any solution. In addition, we explore architectural alternatives and discuss the use of XML in this arena"
1999,"Independent, Open Enterprise Data Integration.","Database researchers and practitioners have long espoused the virtues of data independence. When logical and physical representations of data are separated, an when multiple users can see different views of data, then the flexibility of usage, evolution, and perfor mance of a database system is maximized. This tenet has been lost in the marketing crush of Data Warehousin g, which prescribes a tight coupling of physical representation and high-level usability. In this paper we describe the Cohera Federated DBMS, which re ntroduces data independence to the heterogeneous databases present in todayís enterprises. C oheraís physical independence features provide a scalable spectrum of solutions for physical design of ente rprise-wide data. Its logical independence features remove the distinction between data transformati on and querying, by using industry-standard SQL99 as a unified open conversion framework"
1999,VIDA: (Visual Information Density Adjuster).,"Multiple studies have shown that clutter or sparsity in visual representations can have negative effects ranging from decreased user performance to diminished visual appeal. We have developed a system that assists users in the construction and navigation of visualizations with appropriate visual information density. This system, VIDA (Visual Information Density Adjuster), applies a cartographic principle to minimize clutter and sparsity in visual displays of information.
"
1998,"Interoperability, Distributed Applications and Distributed Databases: The Virtual Table Interface.","Users of distributed databases and of distributed application frameworks require interoperation of heterogeneous data and components, respectively. In this paper, we examine how an extensible, objectrelational database system can integrate both modes of interoperability. We describe the Virtual Table Interface, a facility of the INFORMIX-Dynamic Server Universal Data Option, that provides simplified access to heterogeneous components and discuss the benefits of integrating data with application components."
1998,The Asilomar Report on Database Research.,"The database research community is rightly proud of success in basic research, and its remarkable record of technology transfer. Now the field needs to radically broaden its research focus to attack the issues of capturing, storing, analyzing, and presenting the vast array of online data. The database research community should embrace a broader research agenda ó broadening the definition of database management to embrace all the content of the Web and other online data stores, and rethinking our fundamental assumptions in light of technology shifts. To accelerate this transition, we recommend changing the way research results are evaluated and presented. In particular, we advocate encouraging more speculative and long-range work, moving conferences to a poster format, and publishing all research literature on the Web."
1998,Constant information density in zoomable interfaces.,"We introduce a system that helps users construct interactive visualizations with constant information density. This work is an extension of the DataSplash database visulaization environment. DataSplash is a direct manipulation system in which users can construct and navigate visualizations. Objects' appearances change as users zoom closer to or further away from the visualization. Users specify graphically the point at which these changes occur.Our experience with DataSplash indicates that users find it difficult to construct visualizations that display an appropriate amount of detail. In this paper, we introduce an extension to DataSplash based on the Principle of Constant Information Density. This extension gives users feedback about the density of visualizations as they create them. We also introduce an extension that suggests improvements to existing visualizations.We have performed an informal study of user navigation in applications with and without constant information density. We suggest that designers take density into account when designing applications to avoid biasing user navigation in unexpected ways."
1998,Goal-directed zoom.,"We introduce a novel zoom method, goal-directed zoom. In a goal-directed zoom system, users specify which representation of an object they wish to see. The system automatically zooms to the elevation at which that representation appears at appropriate detail. We have extended a database visualization environment to support end-user construction of visualizations that have goaldirected zoom. We present a sample visualization we have constructed using this environment. "
1998,DataSplash.,"Database visualization is an area of growing importance as database systems become larger and more accessible. DataSplash is an easy-to-use, integrated environment for navigating, creating, and querying visual representations of data. We will demonstrate the three main components which make up the DataSplash environment: a navigation system, a direct-manipulation interface for creating and modifying visualizations, and a direct-manipulation visual query system.
"
1998,Constant Density Visualizations of Non-Uniform Distributions of Data.,"The cartographic Principle of Constant Information Density suggests that the amount of information in an interactive visualization should remain constant as the user pans and zooms. In previous work, we presented a system, VIDA (Visual Information Density Adjuster), which helps users manually construct applications in which overall display density remains constant. In the context of semantic zoom systems, this approach ensures uniformity in the z dimension, but does not extend naturally to ensuring uniformity in the x and y dimensions. In this paper, we present a new approach that automatically creates displays that are uniform in the x, y, and z dimensions. In the new system, users express constraints about visual representations that should appear in the display. The system applies these constraints to subdivisions of the display such that each subdivision meets a target density value. We have implemented our technique in the DataSplash/VIDA database visualization environment. We describe our algorithm, implementation, and the advantages and disadvantages of our approach."
1998,VIQING: Visual Interactive Querying.,"Abstract:
The paper presents VIQING, an environment for expressing queries via direct manipulation of data visualizations. VIQING provides a simple graphical interface for connecting visualizations, and has the expressive power of the basic relational operators select, project and join. VIQING has been implemented in the Tioga DataSplash visualization system to provide a seamless integration of querying and browsing. The resulting system is unique in providing a unified visual interface for developing database applications, encompassing both querying and data visualization."
1997,Supporting Fine-grained Data Lineage in a Database Visualization Environment.,"Abstract:
The lineage of a datum records its processing history. Because such information can be used to trace the source of anomalies and errors in processed data sets, it is valuable to users for a variety of applications, including the investigation of anomalies and debugging. Traditional data lineage approaches rely on metadata. However, metadata does not scale well to fine-grained lineage, especially in large data sets. For example, it is not feasible to store all of the information that is necessary to trace from a specific floating-point value in a processed data set to a particular satellite image pixel in a source data set. In this paper, we propose a novel method to support fine-grained data lineage. Rather than relying on metadata, our approach lazily computes the lineage using a limited amount of information about the processing operators and the base data. We introduce the notions of weak inversion and verification. While our system does not perfectly invert the data, it uses weak inversion and verification to provide a number of guarantees about the lineage it generates. We propose a design for the implementation of weak inversion and verification in an object-relational database management system."
1997,ESMDIS: Earth System Model Data Information System.,"Abstract:
The goal of the development of the Earth System Model Data Information System (ESMDIS) are to provide Earth scientists with: 1) an output management system of Earth System Model (ESM) to browse the metadata and retrieve a desired subset of ESM output; 2) an analysis system of ESM output and other related datasets; 3) an automated pipelining system for ESM data processing; 4) a visualization system; and 5) a Web based user interface to utilize the system. ESMDIS is based on DBMS centric approach, built upon the ""BigSur"" Earth science data schema, and developed using an object relational DBMS. We have built a prototype ESMDIS, and present the results of its development."
1996,Query Processing in a Parallel Object-Relational Database System.,n/a
1996,Mariposa: A Wide-Area Distributed Database System.,"The requirements of wide-area distributed database systems differ dramatically from those of local-area network systems. In a wide-area network (WAN) configuration, individual sites usually report to different system administrators, have different access and charging algorithms, install site-specific data type extensions, and have different constraints on servicing remote requests. Typical of the last point are production transaction environments, which are fully engaged during normal business hours, and cannot take on additional load. Finally, there may be many sites participating in a WAN distributed DBMS. In this world, a single program performing global query optimization using a cost-based optimizer will not work well. Cost-based optimization does not respond well to site-specific type extension, access constraints, charging algorithms, and time-of-day constraints. Furthermore, traditional cost-based distributed optimizers do not scale well to a large number of possible processing sites. Since traditional distributed DBMSs have all used cost-based optimizers, they are not appropriate in a WAN environment, and a new architecture is required. We have proposed and implemented an economic paradigm as the solution to these issues in a new distributed DBMS called Mariposa. In this paper, we present the architecture and implementation of Mariposa and discuss early feedback on its operating characteristics.
"
1996,Tioga-2: A Direct Manipulation Database Visualization Environment.,"Abstract:
The paper reports on user experience with Tioga, a DBMS centric visualization tool developed at Berkeley. Based on this experience, we have designed Tioga-2 as a direct manipulation system that is more powerful and much easier to program. A detailed design of the revised system is presented, together with an extensive example of its application."
1996,Data Replication in Mariposa.,"Abstract:
The Mariposa distributed data manager uses an economic model for managing the allocation of both storage objects and queries to servers. We present extensions to the economic model which support replica management, as well as our mechanisms for propagating updates among replicas. We show how our replica control mechanism can be used to provide consistent, although potentially stale, views of data across many machines without expensive per-transaction synchronization. We present a rule-based conflict resolution mechanism, which can be used to enhance traditional time-stamp serialization. We discuss the effects of our replica system on query processing for both read-only and read-write queries. We further demonstrate how the replication model and mechanisms naturally support name service in Mariposa."
1996,Reordering Query Execution in Tertiary Memory Databases.,"In the relational model the order of fetching data does not affect query correctness. This flexibility is exploited in query optimization by statically reordering data accesses. However, once a query is optimized, it is executed in a fixed order in most systems, with the result that data requests are made in a fixed order. Only limited forms of runtime reordering can be provided by low-level device managers. More aggressive reordering strategies are essential in scenarios where the latency of access to data objects varies widely and dynamically, as in tertiary devices. This paper presents such a strategy. Our key innovation is to exploit dynamic reordering to match execution order to the optimal data fetch order, in all parts of the plan-tree. To demonstrate the practicality of our approach and the impact of our optimizations, we report on a prototype implementation based on Postgres. Using our system, typical I/O cost for queries on tertiary memory databases is as much as an order of magnitude smaller than with conventional query processing techniques."
1995,Chabot: Retrieval from a Relational Database of Images.,"Abstract:
Selecting from a large, expanding collection of images requires carefully chosen search criteria. We present an approach that integrates a relational database retrieval system with a color analysis technique. The Chabot project was initiated at our university to study storage and retrieval of a vast collection of digitized images. These images are from the State of California Department of Water Resources. The goal was to integrate a relational database retrieval system with content analysis techniques that would give our querying system a better method for handling images. Our simple color analysis method, if used in conjunction with other search criteria, improves our ability to retrieve images efficiently. The best result is obtained when text-based search criteria are combined with content-based criteria and when a coarse granularity is used for content analysis.< >"
1995,An Overview of the Sequoia 2000 Project.,"The author describes the Sequoia 2000 research program at the University of California. In this project, refinements in computing-specifically involving storage, networking, file systems, extensible database management, and visualization-will be applied to specific global change applications on the planet Earth. Sequoia 2000 is organized around an interconnected collection of hardware, file systems, DBMS (database management system), networking, visualization, and repository projects. The author discusses each in turn."
1995,The Tioga-2 Database Visualization Environment.,"Abstract
This paper reports on user experience with Tioga, a DBMS-centric visualization tool developed at Berkeley. Based on this experience, we have designed Tioga-2 as a direct manipulation system that is more powerful and much easier to program. We present a detailed design of the revised system together with an extensive example of its application. We also give a progress report on a Tioga-2 implementation."
1995,Navigation and Coordination Primitives for Multidimensional Visual Browsers.,"This paper describes extensions to the Tioga flight-simulator browsing protocol presented by Stonebraker et al. (1993a). These extensions allow users to navigate a multidimensional data space using sophisticated zooming capabilities. This design also allows users to move easily between different multidimensional spaces. Tunneling between different data spaces is shown to be a substantial generalization of hyperlinks in a hypermedia system. Finally, our design provides for the coordination of multiple browsers. This preserves context and allows users to explore multiple paths simultaneously."
1995,Buffering of Intermediate Results in Dataflow Diagrams.,"Abstract:
Buffering of intermediate results in dataflow diagrams can significantly reduce latency when a user browses these results or re-executes a diagram with slightly different inputs. We define the optimal buffer allocation problem of determining the buffer contents which minimize the average response time to such user requests. We show that this problem has several characteristics which render traditional latency reduction techniques ineffective. Since optimal buffer allocation is NP-hard, we propose heuristic methods for buffer management of intermediate results. We present a simulation of the behavior of these heuristics under a variety of conditions, varying graph structure and access pattern. We argue that history mechanisms which track user access patterns can be used to improve performance. We further show that graph structure and access pattern determine the factor of improvement which is possible. The performance enhancements we describe can be applied to minimize query response time in visual dataflow languages. We examine strategies for buffering of intermediate results in dataflow diagrams in the context of Tioga a graphical application development tool."
1995,BigSur: A System For the Management of Earth Science Data.,"In this paper we present a prototype system for the management of earth science data which is novel in that it takes a DBMS centric view of the the task. Our prototype -called ""BigSur"" -is shown in the context of its use by two geographically distributed scientific groups with demanding data storage and processing requirements. BigSur currently stores 1 Terabyte of data, about one thousandth of the volume EOSDIS must store. We claim that the design principles embodied in BigSur provide sufficient flexibility to achieve the difficult scientific and technical objectives of Mission to Planet Earth."
1994,SEQUOIA 2000 Metadata Schema for Satellite Images.,"Sequoia 2000 schema development is based on emerging geospatial standards to accelerate development and facilitate data exchange. This paper focuses on the metadata schema for digital satellite images. We examine how satellite metadata are defined, used, and maintained. We discuss the geospatial standards we are using, and describe a SQL prototype that is based on the Spatial Archive and Interchange Format (SAIF) standard and implemented in the Illustra object-relational database. "
1994,Mariposa: A New Architecture for Distributed Data.,"Abstract:
We describe the design of Mariposa, an experimental distributed data management system that provides high performance in an environment of high data mobility and heterogeneous host capabilities. The Mariposa design unifies the approaches taken by distributed file systems and distributed databases. In addition, Mariposa provides a general, flexible platform for the development of new algorithms for distributed query optimization, storage management, and scalable data storage structures. This flexibility is primarily due to a unique rule-based design that permits autonomous, local-knowledge decisions to be made regarding data placement, query execution location, and storage management.< >"
1994,Implementing Calendars and Temporal Rules in Next Generation Databases.,"Abstract:
In applications like financial trading, scheduling, manufacturing and process control, time based predicates in queries and rules are very important. There is also a need to define lists of time points or intervals. The authors refer to these lists as calendars. The authors present a system of calendars that allow specification of natural-language time-based expressions, maintenance of valid time in databases, specification of temporal conditions in database queries and rules, and user-defined semantics for date manipulation. A simple list based language is proposed to define, manipulate and query calendars. The design of the parser and an algorithm for efficient evaluation of calendar expressions is also described. The paper also describes the implementation of time-based rules in POSTGRES using the proposed system of calendars.< >"
1994,Efficient Organization of Large Multidimensional Arrays.,"Abstract:
Large multidimensional arrays are widely used in scientific and engineering database applications. The authors present methods of organizing arrays to make their access on secondary and tertiary memory devices fast and efficient. They have developed four techniques for doing this: (1) storing the array in multidimensional ""chunks"" to minimize the number of blocks fetched, (2) reordering the chunked array to minimize seek distance between accessed blocks, (3) maintaining redundant copies of the array, each organized for a different chunk size and ordering and (4) partitioning the array onto platters of a tertiary memory device so as to minimize the number of platter switches. The measurements on real data obtained from global change scientists show that accesses on arrays organized using these techniques are often an order of magnitude faster than on the unoptimized data.< >"
1994,Sequoia 2000: A Next-Generation Information System for the Study of Global Change.,"Abstract:
Better data management is crucial to the success of scientific investigations of global change. New modes of research about the Earth, especially the synergistic interactions between observations and models, require massive amounts of diverse data to be stored, organized, accessed, distributed, visualized, and analyzed. To address technical issues of better data management, participants in Sequoia 2000, a collaborative effort between computer scientists and Earth scientists at several campuses of the University of California and at Digital Equipment Corporation (DEC), apply refinements in computing to specific applications. The software architecture includes layers for a common device interface, the file system, the database management system (DBMS), applications, and the network. Early prototype applications of this software include a global-change data schema, integration of a general circulation model (GCM), remote sensing, and a data system for climate studies. Longer range efforts include transfer protocols for moving elements of the database, controllers for secondary and tertiary storage, distributed file system, and a distributed DBMS.< >"
1994,An Economic Paradigm for Query Processing and Data Migration in Mariposa.,"Abstract:
Many new database applications require very large volumes of data. Mariposa is a database system under construction at Berkeley responding to this need. This system combines the best features of traditional distributed database systems, object-oriented DBMSs, tertiary memory file systems and distributed file systems. Mariposa objects can be stored over thousands of autonomous sites and on memory hierarchies with very large capacity. This scale of the system leads to complex query execution and storage management issues, unsolvable in practice with traditional techniques. We propose an economic paradigm as the solution. A query receives a budges which it spends to obtain the answers. Each site attempts to maximize income by buying and selling storage objects, and processing queries for locally stored objects. We present the protocols which underlie the Mariposa economy.< >"
1994,SEQUOIA 2000: A Reflection of the First Three Years.,"Abstract:
The purpose of the SEQUOIA 2000 project is to build a better computing environment for global change researchers. Such researchers investigate issues such as global warming, ozone depletion, environment toxification, and species extinction, and are members of Earth Sciences Departments at Universities and National Laboratories. SEQUOIA 2000 is the Digital Equipment Corporation flagship research project for the 1990's. We describe the SEQUOIA 2000 project and its implementation efforts during the first three years. Included are the objectives we had, how we chose to address them and some of the lessons we learned from this endeavour.< >"
1994,Zooming and Tunneling in Tioga: Supporting Navigation in Multimedia Space.,"Abstract:
The Tioga system applies a boxes and arrows programming notation to allow nonexpert users to graphically construct database applications. Users connect database procedures using a dataflow model. Browsers are used to visualize the resulting data. This paper describes extensions to the Tioga browser protocol. These extensions allow sophisticated, flight-simulator navigation through a multidimensional data space. This design also incorporates wormholes to allow tunneling between different multidimensional spaces. Wormholes are shown to be substantial generalizations of hyperlinks in a hypertext system. These powerful mechanisms for relating data provide users with great flexibility. For example, users can create magnifying glasses that provide an enhanced view of the underlying data.< >"
1993,The SEQUOIA 2000 project.,"This paper describes the objectives of the SEQUOIA 2000 project and the software development that is being done to achieve these objectives. In addition, several lessons relevant to Geographic Information Systems (GIS) that have have been learned from the project are explained. "
1993,Optimization of Parallel Query Execution Plans in XPRS.,"The authors describe their approach to the optimization of query execution plans in XPRS, a multi-user parallel database machine based on a shared-memory multi-processor and a disk array. The main difficulties in this optimization problem are the compile-time unknown parameters such as available buffer size and number of free processors, and the enormous search space of possible parallel plans. The authors deal with these problems with a novel two phase optimization strategy which dramatically reduces the search space and allows run time parameters without significantly compromising plan optimality. They present their two phase strategy and give experimental evidence from XPRS benchmarks that indicate that it almost always produces optimal plans."
1993,Large Object Support in POSTGRES.,"Abstract:
Four implementations that support large objects in the POSTGRES database system are presented. The four implementations offer varying levels of support for security, transactions, compression, and time travel. All are implemented using the POSTGRES abstract data type paradigm, support user-defined operators and functions, and allow file-oriented access to large objects in the database. The support for user-defined storage managers available in POSTGRES is also detailed. The performance of all four large object implementations on two different storage devices is described.< >"
1993,HighLight: a file system for tertiary storage.,"Abstract:
HighLight, a file system combining secondary disk storage and tertiary robotic storage that is being developed as part of the Sequoia 200 Project, is described. HighLight is an extension of the 4.4BSD log-structured file system (LFS), which provides hierarchical storage management without requiring any special support from applications. The authors present HighLight's design and various policies for automatic migration of file data between the hierarchy levels. The performance of HighLight was compared with that of the 4.4BSD LFS implementation. The initial results indicate that HighLight's performance is comparable to that of 4.4BSD LFS for disk-resident data, and the overhead associated with accessing data from the tertiary cache is negligible.< >"
1993,The Sequoia 2000 Benchmark.,"This paper presents a benchmark that concisely captures the data base requirements of a collection of Earth Scientists working in the SEQUOIA 2000 project on various aspects of global change research. This benchmark has the novel characteristic that it uses real data sets and real queries that are representative of Earth Science tasks. Because it appears that Earth Science problems are typical of the problems of engineering and scientific DBMS users, we claim that this benchmark represents the needs of this more general community. Also included in the paper are benchmark results for three example DBMSs: GRASS, IPW and POSTGRES.
"
1993,Predicate Migration: Optimizing Queries with Expensive Predicates.,"The traditional focus of relational query optimization schemes has been on the choice of join methods and join orders. Restrictions have typically been handled in query optimizers by ìpredicate pushdownî rules, which apply restrictions in some random order before as many joins as possible. These rules work under the assumption that restriction is essentially a zero-time operation. However, today's extensible and object-oriented database systems allow users to define time-consuming functions, which may be used in a query's restriction and join predicates. Furthermore, SQL has long supported subquery predicates, which may be arbitrarily time-consuming to check. Thus restrictions should not be considered zero-time operations, and the model of query optimization must be enhanced.

In this paper we develop a theory for moving expensive predicates in a query plan so that the total cost of the plan ó including the costs of both joins and restrictions ó is minimal. We present an algorithm to implement the theory, as well as results of our implementation in POSTGRES. Our experience with the newly enhanced POSTGRES query optimizer demonstrates that correctly optimizing queries with expensive predicates often produces plans that are orders of magnitude faster than plans generated by a traditional query optimizer. The additional complexity of considering expensive predicates during optimization is found to be manageably small."
1993,The Miro DBMS.,"This short paper explains the key object-relational (OR) DBMS technology used by the Miro DBMS.
"
1993,The SEQUOIA 2000 Project.,"Abstract
This paper describes the objectives of the SEQUOIA 2000 project and the software development that is being done to achieve these objectives. In addition, several lessons relevant to Geographic Information Systems (GIS) that have have been learned from the project are explained."
1993,HighLight: Using a Log-structured File System for Tertiary Storage Management.,"Robotic storage devices offer huge storage capacity at a low cost per byte, but with large access times. Integrating these devices into the storage hierarchy presents a challenge to file system designers. Log-structured file systems (LFSs) were developed to reduce latencies involved in accessing disk devices, but their sequential write patterns match well with tertiary storage characteristics. Unfortunately, existing versions only manage memory caches and disks, and do not support a broader storage hierarchy. HighLight extends 4.4BSD LFS to incorporate both secondary storage devices (disks) and tertiary storage devices (such as robotic tape jukeboxes), providing a hierarchy within the file system that does not require any application support. This paper presents the design of HighLight, proposes various policies for automatic migration of file data between the hierarchy levels, and presents initial migration mechanism performance figures."
1993,Tioga: A Database-Oriented Visualization Tool.,"Abstract:
In the work we present a new architecture for visualization systems that is based on data base management system (DBMS) technology. By building on the mechanisms present in a next-generation DBMS, rather than merely on the capabilities of a standard file manager, we show that a simpler and more powerful visualization system can be constructed. We retain the popular ""boxes and arrows"" programming notation for constructing visualization programs, but add a ""flight simulator"" model of movement to navigate the output of such programs. In addition, we provide a means to specify a hierarchy of abstracts of data of different types and resolutions, so that a ""zoom"" capability can be supported. The underlying DBMS support for this system, Tioga, is briefly described, as well as the current state of the implementation.< >"
1993,Tioga: Providing Data Management Support for Scientific Visualization Applications.,"We present a user interface paradigm for database management systems that is motivated by scientific visualization applications. Our graphical user interface includes a ""boxes and arrows"" notation for database access and a flight simulator model of movement through information space. We also provide means to specify a hierarchy of abstracts of data of different types and resolutions, so that a ""zoom"" capability can be supported. The underlying DBMS support for this system is described and includes the compilation of query plans into megaplans, new algorithms for data buffering, and provisions for a guaranteed rate of data delivery. The current state of the Tioga implementation is also described."
1992,The Integration of Rule Systems and Database Systems.,"Abstract:
The integration of rule systems into database management systems is explored. Research activities in this area over the past decade are surveyed. The focus is on prototype systems that have been completely specified and the implementation issues encountered. A research agenda which should be addressed by the research community over the next few years is presented.< >"
1991,The Postgres Next Generation Database Management System.,n/a
1991,Database Systems: Achievements and Opportunities.,"The history of database system research in the U.S. is one of exceptional productivity and startling economic impact. Barely twenty years old as a basic science research field, database research conducted with Federal support in the nation's universities and in its industrial research laboratories has fueled an information services industry estimated at $10 billion per year in the U.S. alone. This industry has grown at an average rate of 20 percent per year since 1965 and is continuing to expand at this rate. Achievements in database research underpin fundamental advances in communications systems, transportation and logistics, financial management, knowledge-based systems, accessibility to scientific literature, and a host of other civilian and defense applications. They also serve as the foundation for considerable progress in basic science in various fields ranging from computing to biology."
1991,Read Optimized File System Designs: A Performance Evaluation.,"Abstract:
A performance comparison is presented of several file system allocation policies. The file systems are designed to provide high bandwidth between disks and main memory by taking advantage of parallelism in an underlying disk array catering to large units of transfer, and minimizing the bandwidth dedicated to the transfer of metadata. All of the file systems described use a multiblock allocation strategy which allows both large and small files to be allocated efficiently. Simulation results show that these multiblock policies result in systems that are able to utilize a large percentage of the underlying disk bandwidth (more than 90% in sequential cases). As general-purpose systems are called upon to support more data intensive applications such as databases and supercomputing, these policies offer an opportunity to provide superior performance to a larger class of users.< >"
1991,Optimization of Parallel Query Execution Plans in XPRS.,"Abstract:
The authors describe their approach to the optimization of query execution plans in XPRS, a multi-user parallel database machine based on a shared-memory multi-processor and a disk array. The main difficulties in this optimization problem are the compile-time unknown parameters such as available buffer size and number of free processors, and the enormous search space of possible parallel plans. The authors deal with these problems with a novel two phase optimization strategy which dramatically reduces the search space and allows run time parameters without significantly compromising plan optimality. They present their two phase strategy and give experimental evidence from XPRS benchmarks that indicate that it almost always produces optimal plans.< >"
1991,Managing Persistent Objects in a Multi-Level Store.,"This paper presents an architecture for a persistent object store in which multi-level storage is explicitly included. Traditionally. DBMSs have assumed that all accessible data resides on magnetic disk, and recently several researchers have begun to consider the possibility that significant amounts of data will occupy space m a main memory cache. We feel that object bases in which time critical objects reside in main memory, other objects are disk resident, and the remainder occupy tertiary memory. Moreover, it is possible that more than three levels will be present, and that some of these levels will be on remote hardware. This paper contains an architectural proposal addressing these needs along with a sketch of the required query optimizer."
1991,Segment Indexes: Dynamic Indexing Techniques for Multi-Dimensional Interval Data.,"We propose new indexing techniques for interval data in K 2.1 dimensions consisting of a set of extensions to a class of database indexing structures. These techniques are useful for improving index search performance for spatial data composed of multi-dimensional intervals that have non-uniform length distributiona. Interval data collections having non-uniform length distributions are likely to occur in practice, and may be typical of historical data collections in which tuples represent intervals in the time dimension. We present these indexing techniques, illustrate how they may be applied to the R-Tree index, end provide the results of performance experiments."
1991,Using Write Protected Data Structures To Improve Software Fault Tolerance in Highly Available Database Management Systems.,"This paper describes a database management system (DBMS) modified to use hardware write protection to guard critical DBMS data structures against software errors. Guarding (write-protecting) DBMS data improves software reliability by providing quick detection of corrupted pointers and array bounds overruns. Guarding will be especially helpful in an extensible DBMS since it limits the power of extension code to corrupt unrelated parts of the system. Read-write data structures can be guarded as long as correct software is able to temporarily unprotect the data structures during updates. The paper discusses the effects of three different update models on performance, software complexity, and error protection Measurements of a DBMS which uses guarding to protect its buffer pool show two to eleven percent performance degradation in a debit/credit benchmark."
1990,Architecture of Future Data Base Systems.,"In this paper we first discuss three typical programming interfaces which future DBMS systems may support. These interfaces are: I) non-procedural, set-oriented (relational), 2) navigational (CODASYL), and 3) access method. After briefly comparing the language levels provided by each interface, four data base system architectures are described which can support a high level interface on top of one of the other two interfaces. We believe these architectures are the only reasonable candidates for future DBMS packages."
1990,Third-Generation Database System Manifesto - The Committee for Advanced DBMS Function.,"The preparation of beehives from a synthetic resinous material which is acceptable to the bees, which is not attacked by vermin, and which exhibits the requisite physical properties to provide a desirable beehive is accomplished by assembling the hives from molded urethane foam panels, the urethane foam being formulated so as to produce a product which is not rejected by the bees and which does not make the bees nervous or otherwise interfere with their normal habits in secreting honey in said beehive."
1990,Data Base Research at Berkeley.,"Data base research at Berkeley can be decomposed into four categories. First there is the POSTGRES project led by Michael Stonebraker, which is building a next-generation DBMS with novel object management, knowledge management and time travel capabilities. Second, Larry Rowe leads the PICASSO project which is constructing an advanced application development tool kit for use with POSTGRES and other DBMSs. Third, there is the XPRS project (eXtended Postgres on Raid and Sprite) which is led by four Berkeley faculty, Randy Katz, John Ousterhout, Dave Patterson and Michael Stonebraker. XPRS is exploring high performance I/O systems and their efficient utilization by operating systems and data managers. The last project is one aimed at improving the reliability of DBMS software by dealing more effectively with software errors that is also led by Michael Stonebraker."
1990,The Implementation of Postgres.,"Abstract:
The design and implementation decisions made for the three-dimensional data manager POSTGRES are discussed. Attention is restricted to the DBMS backend functions. The POSTGRES data model and query language, the rules system, the storage system, the POSTGRES implementation and the current status and performance are discussed.< >"
1990,The Third-Generation Database Manifesto: A Brief Retrospection.,n/a
1990,Third-Generation Database System Manifesto - The Committee for Advanced DBMS Function.,n/a
1990,Alternatives in Complex Object Representation: A Performance Perspective.,"Abstract:
With database systems finding wider use in CAD, office information systems, and logic programming applications, the importance of efficiently representing and manipulating complex objects is growing. In this study a classification of the alternatives for representing complex objects is examined. Consideration is given to the performance aspects of one representation technique based on object identifiers. It is shown that clustering of subobjects with their referencing objects is rarely a good idea. In contrast, it is shown that caching the intermediate results of query processing can yield large benefits.< >"
1990,Distributed RAID - A New Multiple Copy Algorithm.,"Abstract:
A new multicopy algorithm is proposed; it has the potentially attractive property that much less space is required and equal performance is provided during normal operation. On the other hand, during failures the new algorithm offers lower performance than a conventional scheme. As such, this algorithm may be attractive in various multicopy environments, as well as in disaster recovery. The algorithm is presented and compared with various other multicopy and disaster recovery techniques.< >"
1990,"On Rules, Procedures, Caching and Views in Data Base Systems.","This paper demonstrates that a simple rule system can be constructed that supports a more powerful view system than available in current commercial systems. Not only can views be specified by using rules but also special semantics for resolving ambiguous view updates are simply additional rules. Moreover, procedural data types as proposed in POSTGRES are also efficiently simulated by the same rules system. Lastly, caching of the action part of certain rules is a possible performance enhancement and can be applied to materialize views as well as to cache procedural data items. Hence, we conclude that a rule system is a fundamental concept in a next generation DBMS, and it subsumes both views and procedures as special cases."
1990,The Postgres DBMS.,n/a
1990,"""The Committee for Advanced DBMS Function"": Third Generation Data Base System Manifesto.",n/a
1990,Transaction Support in Read Optimizied and Write Optimized File Systems.,"This paper provides a comparative analysis of five implementations of transaction support. The first of the methods is the traditional approach of implementing transaction processing within a data manager on top of a read optimized file system. The second also assumes a traditional file system but embeds transaction support inside the file system. The third model considers a traditional data manager on top of a write optimized file system. The last two models both embed transaction support inside a write optimized file system, each using a different logging mechanism. Our results show that in a transaction processing environment, a write optimized file system often yields better performance than one optimized for reads. In addition, we show that file system embedded transaction managers can perform as well as data managers when transaction throughput is limited by I/O bandwidth. Finally, even when the CPU is the critical resource, the difference in performance between a data manager and an embedded system is much smaller than previous work has shown."
1990,Highly Redundant Management of Distributed Data.,"An algorithm for redundant management of distributed data using a minimal amount of data replication is described and analyzed. The main results on storage space utilization, I/O (input/output) performance, and reliability are outlined. The recently introduced RAID (redundant array of inexpensive disks) concept is extended to a distributed computing system. The resulting distributed storage architecture, called RADD (redundant array of distributed disks), is shown to support redundant copies of data across a computer network at the same space cost as RAIDs do for local data. Thus, a RADD increases data availability in the presence of both temporary and permanent failures (disasters) at local sites. During normal operation, the RADD scheme offers performance comparable to the two-copies systems. As such, RADDs should be considered a possible alternative to traditional multiple-copy techniques as well as to other high-availability schemes."
1989,A Commentary on the POSTGRES Rule System.,"This paper suggests modifications to the POSTGRES rules system (PRS) to increase its usability and function. Specifically, we suggest changing the rule syntax to a more powerful one and propose additional keywords, introduce the notion of rulesets whose purpose is to increase the user's control over the rule activation process, and expand the versioning facility to support a broader range of applications than is currently possible. "
1989,The Case for Partial Indexes.,Current data managers support secondary and/or primary indexes on columns of relations. In this paper we suggest the advantages that result from indexes which contain only some of the possible values in a column of a relation. 
1989,Future Trends in Database Systems.,"Abstract:
The author discusses the likely evolution of commercial data managers over the next several years. Topics to be covered include the following: why SQL (structured query language) has become a universal standard; who can benefit from SQL standardization; why the current SQL standard has no chance of lasting; why all database systems can be distributed soon; what new technologies are likely to be commercialized; and why vendor independence may be achievable.< >"
1989,Performance Considerations for an Operating System Transaction Manager.,"Abstract:
Results of a previous comparison study (A. Kumar and M. Stonebraker, 1987) between a conventional transaction manager and an operating system (OS) transaction manager indicated that the OS transaction manager incurs a severe performance penalty and appears to be feasible only in special circumstances. Three approaches for enhancing the performance of an OS transaction manager are considered. The first strategy is to improve performance by reducing the cost of lock acquisition and by compressing the log. The second strategy explores the possibility of still further improvements from additional semantics to be built into an OS transaction system. The last strategy is to use a modified index structure that makes update operations less expensive to perform. The results show that the OS will have to implement essentially all of the specialized tactics for transaction management that are currently used by a database management system (DBMS) in order to match DBMS performance.< >"
1989,Indexing Techniques for Historical Databases.,"Abstract:
Two indexing structures based on R-trees are proposed for historical data, which can span magnetic disk and optical disk media. The performance of these indexes is compared to that of two other indexing candidates that are each contained entirely on one medium. Test results indicate that the proposed indexes perform well when compared to an index that is contained entirely on optical disk.< >"
1988,A Project on High Performance I/O Subsystems.,n/a
1988,The POSTGRES Rule Manager.,"Abstract:
The rule subsystem that is being implemented in the POSTGRES DBMS is explained. It is novel in several ways. First, it gives users the capability of defining rules as well as data. Moreover, depending on the scope of each rule defined, optimization is handled differently. This leads to good performance both when there are many rules each of small scope and when there are a few rules each of large scope. In addition, rules provide either a forward-chaining or a backward-chaining control flow, and the system chooses the control mechanism that optimizes performance whenever possible. Priority rules can be defined, allowing a user to specify rule systems that have conflicts. This use of exceptions seems necessary in many applications. Database services such as views, protection, integrity constraints, and referential integrity can be obtained simply by applying the rules system in the appropriate way. Consequently, no special-purpose code need be included in POSTGRES to handle these tasks.< >"
1988,Future Trends in Expert Data Base Systems.,In this paper we discuss how we see the capabilities of DBMSs evolving over the next several years to meet the needs of expert data base applications. We also present some of the research thrusts which we see as important that appear to be receiving insufficient attention in the research community. 
1988,Future Trends in Data Base Systems.,"Abstract:
A discussion is presented of the likely evolution of commercial data managers over the next several years. Topics to be covered include: why SQL is becoming a de facto standard; who will benefit from SQL standardization; why the SQL standard has no chance of lasting; why all database systems will be distributed soon; what technologies are likely to be commercialized; and why vendor independence may be achievable. The objective is to present the author's vision of the future.< >"
1988,A Performance Comparison of Two Architectures for Fast Transaction Processing.,"Abstract:
Investigates the issues involved in using multiprocessors for transaction processing. The authors use a simulation model to study the behavior of two different architectures, namely shared everything and shared nothing. In shared everything, any processor can access any disk and all memory is shared. In shared nothing, neither disks nor memory is shared. They study the effects of data contention and resource contention in both of these architectures. They quantify the effects of intraquery parallelism on both architectures under different operating conditions.< >"
1988,Semantics Based Transaction Management Techniques for Replicated Data.,"Data is often replicated in distributed database applications to improve availability and response time. Conventional multi-copy algorithms deliver fast response times and high availability for read-only transactions while sacrificing these goals for updates. In this paper, we propose a multi-copy algorithm that works well in both retrieval and update environments by exploiting special application semantics. By subdividing transactions into various categories, and utilizing a commutativity property, we demonstrate cheaper techniques and show that they guarantee correctness. A performance comparison between our techniques and conventional ones quantifies the extent of the savings."
1988,Extended User-Defined Indexing with Application to Textual Databases.,"A number of application-specific searching mechanisms, including keyword searching in textual databases, can be implemented naturally in a relational DBMS using abstract datatypes and userdefined operators. For query efficiency these operators and abstract datatypes must be supported by indices. A new indexing scheme is proposed which allows a large class of query predicates to be evaluated using indices, including many key operators for textual databases. The indexing scheme also significantly reduces the space required to store indexed textual data in a relational database system."
1988,The Design of XPRS.,"This paper presents an overview of the techniques we are using to build a DBMS at Berkeley that will simultaneously provide high performance and high availability in transaction processing environments, in applications with complex ad-hoc queries and in applications with large objects such as images or CAD layouts. We plan to achieve these goals using a general purpose DBMS and operating system and a shared memory multiprocessor. The hardware and software tactics which we are using to accomplish these goals are described in this paper and include a novel ìfast pathî feature, a special purpose concurrency control scheme, a twodimensional file system, exploitation of parallelism and a novel method to efficiently mirror disks. We strive for high performance in three different application areas: 1) transaction processing 2) complex ad-hoc queries 3) management of large objects"
1987,Extendability in POSTGRES.,n/a
1987,The Effect of Join Selectivities on Optimal Nesting Order.,"A heuristic query optimizer must choose the best way to process an incoming query. This choice is based on comparing the expected cost of many (or all) of the ways that a command might be processed. This expected cost calculation is determined by statistics on the sizes of the relations involved and the selectivities of the operations being performed. Of course, such estimates are subject to error, and in this paper we investigate the sensitivity of the best query plan to errors in the selectivity estimates. We treat the common case of join queries and show that the optimal plan for most queries is very insensitive to selectivity inaccuracies. Hence, there is little reason for a data manager to spend a lot of effort making accurate estimates of join selectivities."
1987,Extending a Database System with Procedures.,"This paper suggests that more powerful database systems (DBMS) can be built by supporting database procedures as full-fledged database objects. In particular, allowing fields of a database to be a collection of queries in the query language of the system is shown to allow the natural expression of complex data relationships. Moreover, many of the features present in object-oriented systems and semantic data models can be supported by this facility.
In order to implement this construct, extensions to a typical relational query language must be made, and considerable work on the execution engine of the underlying DBMS must be accomplished. This paper reports on the extensions for one particular query language and data manager and then gives performance figures for a prototype implementation. Even though the performance of the prototype is competitive with that of a conventional system, suggestions for improvement are presented."
1987,Performance Issues in High Performance Transaction Processing Architectures.,"Abstract
In this paper, we investigate the issues involved in using multiprocessors for high performance transaction processing applications. We use a simulation model to compare the performance of two different architectures, namely, Shared Everything and Shared Nothing. In Shared Everything, any processor can access any disk and all memory is shared. In Shared Nothing, neither disks nor memory is shared. We study the effects of response time constraints in both of these architectures and compare their performance in the presence of load imbalances. In addition, we study how intra-query parallelism affects the performance of both architectures under different operating conditions."
1987,The Design of the Postgres Rules System.,"Abstract:
This paper explains the rules subsystem that is being implemented in the POSTGRES DBMS. It is novel in several ways. First, it gives to users the capability of defining rules as well as data to a DBMS. Moreover, depending on the scope of each rule defined, optimization is handled differently. This leads to good performance both in the case that there are many rules each of small scope and a few rules each of large scope. In addition, rules provide either a forward chaining control flow or a backward chaining one, and the system will choose the control mechanism that optimizes performance in the cases that it is possible. Furthermore, priority rules can be defined, thereby allowing a user to specify rules systems that have conflicts. This use of exceptions seems necessary in many applications. Lastly, our rule system can support an implementation of views, protection and integrity control, simply by applying the rules system in a particular way. Consequently, no special purpose code need be included to handle these tasks."
1987,The POSTGRES Data Model.,"This paper describes the data model for POSTGRES, a next-generation extensible database management system being developed at the University of California StR86. The data model is a relational model that has been extended with abstract data types, data of type procedure, and attribute and procedure inheritance. These mechanisms can be used to simulate a wide variety of semantic and object-oriented data modeling constructs including aggregation and generalization, complex objects with shared subobjects, and attributes that reference tuples in other relations."
1987,The Design of the POSTGRES Storage System.,"This paper presents the design of the storage system for the POSTGRES data base system under construction at Berkeley. It is novel in several ways. First, the storage manager supports transaction management but does so without using a conventional write ahead log (WAL). In fact, there is no code to run at recovery time, and consequently recovery from crashes is essentially instantaneous. Second, the storage manager allows a user to optionally keep the entire past history of data base objects by closely integrating an archival storage system to which historical records are spooled. Lastly, the storage manager is consciously constructed as a collection of asynchronous processes. Hence, a large monolithic body of code is avoided and opportunities for parallelism can be exploited. The paper concludes with a analysis of the storage system which suggests that it is performance competitive with WAL systems in many situations."
1987,Performance Evaluation of an Operating System Transaction Manager.,"A conventional transaction manager implemented by a database management system (DBMS) was compared against one implemented within an operating system (OS) in a variety of simulated situations. Models of concurrency control and crash recovery were constructed for both environments, and the results of a collection of experiments are presented in this paper. The results indicate that an OS transaction manager incurs a severe performance disadvantage and appears to be feasible only in special circumstances. "
1986,The Case for Shared Nothing.,"There are three dominent themes in building high transaction rate multiprocessor systems, namely shared memory (e.g. Synapse, IBM/AP configurations), shared disk (e.g. VAX/cluster, any multi-ported disk system), and shared nothing (e.g. Tandem, Tolerant). This paper argues that shared nothing is the preferred approach. "
1986,Operating System Support for Data Management.,"Several operating system services are examined with a view toward their applicability to support of database management functions. These services include buffer pool management; the file system; scheduling, process management, and interprocess communication; and consistency control. "
1986,Object Management in a Relational Data Base System.,"This paper first presents a collection of capabilities in the area of object management that are desired by ""non business data processing"" applications. Three approaches to providing this function, application specific systems, semantic data models and high leverage extensions to the relational model are examined. The advantages of the latter approach are described. "
1986,An Analysis of Rule Indexing Implementations in Data Base Systems.,"In this paper we discuss several alternate implementation schemes for rule indexing in a data base system. Two of the proposals have much in common with predicate locking, while four others resemble versions of physical locking. A performance analysis is conducted based on an abstract model of the rule indexing problem. "
1986,Inclusion of New Types in Relational Data Base Systems.,"Abstract:
This paper explores a mechanism to support user-defined data types for columns in a relational data base system. Previous work suggested how to support new operators and new data types. The contribution of this work is to suggest ways to allow query optimization on commands which include new data types and operators and ways to allow access methods to be used for new data types."
1986,Object Management in Postgres Using Procedures.,"This paper presents the object management facilities being designed into a next-generation data manager, POSTGRES. This system is unique in that it does not invent a new data model for support of objects but chooses instead to extend the relational model with a powerful abstract data typing capability and procedures as full-fledged data base objects. The reasons to remain with the relational model are indicated in this paper along with the POSTGRES relational extensions. "
1986,The Design of Postgres.,"This paper presents the preliminary design of a new database management system, called POSTGRES, that is the successor to the INGRES relational database system. The main design goals of the new system are to

provide better support for complex objects,
provide user extendibility for data types, operators and access methods,
provide facilities for active databases (i.e., alerters and triggers) and inferencing including forward- and backward-chaining,
simplify the DBMS code for crash recovery,
produce a design that can take advantage of optical disks, workstations composed of multiple tightly-coupled processors, and custom designed VLSI chips, and
make as few changes as possible (preferably none) to the relational model.
The paper describes the query language, programming language interface, system architecture, query processing strategy, and storage system for the new system."
1985,Tips on Benchmarking Data Base Systems.,n/a
1985,Problems in Supporting Data Base Transactions in an Operating System Transaction Manager.,"This paper reports on the experience of the authors in attempting to support data base transactions on top of an existing operating system transaction manager. It will be seen that significant modifications to both the example data base system and the example operating system are required to support the concept. The conclusion to be drawn is that operating system transaction managers will have to be designed more generally than is now suggested and that application programs (such as data base systems) will have to participate in the transaction management process.
"
1985,Triggers and inference in data base systems.,"There is a collection of database applications (such as real time control) which may be best accomplished using collections of triggers. The paradigm in which an initial action recursively triggers dependent actions is often called forward chaining. In addition, database support for large knowledge bases requires at least a simple inferencing capability. When a retrieve command cannot be satisfied using only stored data, a data manager must determine if a rule in the knowledge base can be used to reformulate the query. In this way, one works from the desired data toward database facts which must be ascertained using backward chaining."
1985,Expert database systems/Bases de Donn√©es et syst√®mes experts.,n/a
1985,Triggers and Inference In Database Systems.,"There is a collection of database applications (such as real time control) which may be best accomplished using collections of triggers. The paradigm in which an initial action recursively triggers dependent actions is often called forward chaining. In addition, database support for large knowledge bases requires at least a simple inferencing capability. When a retrieve command cannot be satisfied using only stored data, a data manager must determine if a rule in the knowledge base can be used to reformulate the query. In this way, one works from the desired data toward database facts which must be ascertained using backward chaining."
1985,The Case for Shared Nothing.,"There are three dominent themes in building high transaction rate multiprocessor systems, namely shared memory (e.g. Synapse, IBM/AP configurations), shared disk (e.g. VAX/cluster, any multi-ported disk system), and shared nothing (e.g. Tandem, Tolerant). This paper argues that shared nothing is the preferred approach. "
1984,Using a Relational Database Management System for Computer Aided Design Data - An Update.,n/a
1984,Performance analysis of distributed data base systems.,"In this paper we briefly present the design of a distributed relational data base system. Then, we discuss experimental observations of the performance of that system executing both short and long commands. Conclusions are also drawn concerning metrics that distributed query processing heuristics should attempt to minimize. Lastly, we comment on architectures which appear viable for distributed data base applications. "
1984,Implementation of Data Abstraction in the Relational Database System Ingres.,"This paper discusses the design and implementation of an abstract data type (ADT) facility which was added to the INGRES database manager. Our implementation of ADTs allows a user to register ADTs and ADT operators with the run-time database manager, declare column values of relations to be instances of ADTs, and formulate queries containing references to ADTs and ADT operators. The user view, implementation, performance, and possible extensions to this new facility are described.
"
1984,Virtual Memory Transaction Management.,"In this paper we examine the consequences of an operating system providing transaction management in an environment where files are bound into a user's address space. The discussion focuses on inherent limitations in providing concurrency control and crash recovery services in this environment and on hardware extensions needed to overcome these deficiencies.
"
1984,Heuristic Search in Data Base Systems.,n/a
1984,Implementation Techniques for Main Memory Database Systems.,"With the availability of very large, relatively inexpensive main memories, it is becoming possible keep large databases resident in main memory In this paper we consider the changes necessary to permit a relational database system to take advantage of large amounts of main memory We evaluate AVL vs B+-tree access methods for main memory databases, hash-based query processing strategies vs sort-merge, and study recovery issues when most or all of the database fits in main memory As expected, B+-trees are the preferred storage mechanism unless more than 80--90% of the database fits in main memory A somewhat surprising result is that hash based query processing strategies are advantageous for large memory situations "
1984,Quel as a Data Type.,"This paper explores the use of commands in a query language as an abstract data type (ADT) in data base management systems Basically, an ADT facility allows new data types, such as polygons, lines, money, time, arrays of floating point numbers, bit vectors, etc, to supplement the built-in data types in a data base system. In this paper we demonstrate the power of adding a data type corresponding to commands in a query language We also propose three extensions to the query language QUEL to enhance its power in this augmented environment."
1984,Database Portals: A New Application Program Interface.,"This paper describes the design and proposed implementation of a new application program interface to a database management system. Programs which browse through a database making ad-hoc updates are not well served by conventional embedding of DBMS commands in programming languages. a new embedding is suggested which overcomes all deficiencies. This construct, called a portal, allows a program to request a collection Of tuples at once and supports novel concurrency control schemes. "
1984,The Performance of Concurrency Control Algorithms for Database Management Systems.,"This paper describes a study of the performance of centralized concurrency control algorithms. An algorithm-independent simulation framework was developed in order to support comparative studies of various concurrency control algorithms. We describe this framework in detail and present performance results which were obtained for what we believe to be a representative cross-section of the many proposed algorithms. The basic algorithms studied include four locking algorithms. two timestamp algorithms. and one optimistic algorithm. Also. we briefly summarize studies of several multiple version algorithms and several hierarchical algorithms. We show that. in general, locking algorithms provide the best performance."
1983,Implementation of Rules in Relational Data Base Systems.,"This paper contains a proposed implementation of a rules system in a relational data base system. Such a rules system can provide data base services including integrity control, protection, alerters, triggers, and view processing. Moreover, it can be used for user specified rules. The proposed implementation makes efficient use of an abstract data type facility by introducing new data types which assist with rule specification and enforcement. "
1983,Performance Enhancements to a Relational Database System.,"In this paper we examine four performance enhancements to a database management system: dynamic compilation, microcoded routines, a special-purpose file system, and a special-purpose operating system. All were examined in the context of the INGRES database management system. Benchmark timings that are included suggest the attractiveness of dynamic compilation and a special-purpose file system. Microcode and a special-purpose operating system are analyzed and appear to be of more limited utility in the INGRES context. "
1983,Document Processing in a Relational Database System.,"This paper contains a proposal to enhance a relational database manager to support document processing. Basically, it suggests support for data items that are variable-length strings, support for ordered relations, support for substring operations, and support for new operators that concatenate and break apart string fields."
1983,A Formal Model of Crash Recovery in a Distributed System.,"Abstract:
A formal model for atomic commit protocols for a distributed database system is introduced. The model is used to prove existence results about resilient protocols for site failures that do not partition the network and then for partitioned networks. For site failures, a pessimistic recovery technique, called independent recovery, is introduced and the class of failures for which resilient protocols exist is identified. For partitioned networks, two cases are studied: the pessimistic case in which messages are lost, and the optimistic case in which no messages are lost. In all cases, fundamental limitations on the resiliency of protocols are derived."
1983,Application of Abstract Data Types and Abstract Indices to CAD Data Bases.,n/a
1983,DBMS and AI: Is There any Common Point of View?,"Recent workshops have explored commonality among the points of view of AL, DBMS and programming language researchers. On the surface, it would appear that DBMS researchers are building smarter data base systems and that AL researchers are building expert systems containing a knowledge base. However, it appears that there is minimal commonality to the points of view expressed by these communities. Therefore, the author is dubious of any significant cross fertilisation. He illustrates a few differences in points of view in the two communities by 3 examples"
1983,Performance Analysis of Distributed Data Base Systems.,"In this paper we briefly present the design of a distributed relational data base system. Then, we discuss experimental observations of the performance of that system executing both short and long commands. Conclusions are also drawn concerning metrics that distributed query processing heuristics should attempt to minimize. Lastly, we comment on architectures which appear viable for distributed data base applications. "
1983,An Implementation of Hypothetical Relations.,"In this paper we develop a different approach to implementing hypothetical relations than those previously proposed. Our design, which borrows ideas from tactics based on views and differential files, offers several advantages over other schemes. An actual implementation is described and performance statistics are presented. "
1982,Observations on the Evolution of a Software System.,n/a
1982,Using a Relational Database Management System for Computer Aided Design Data.,n/a
1982,Performance Analysis of Distributed Data Base Systems.,"In this paper we briefly present the design of a distributed relational data base system. Then, we discuss experimental observations of the performance of that system executing both short and long commands. Conclusions are also drawn concerning metrics that distributed query processing heuristics should attempt to minimize. Lastly, we comment on architectures which appear viable for distributed data base applications. "
1982,Implementation of a Time Expert in a Data Base System.,"This paper reports on the design and implementation of a time expert for a relational data base system. It demonstrates that a sophisticated expert can be written easily and cause minimal performance degradation. As a result, extending a data base system to support user defined data types is an easy operation.
"
1982,Adding Semantic Knowledge to a Relational Database System.,"This chapter suggests two mechanisms for adding semantic knowledge to a data manager, namely inclusion of an AI oriented rules system and a particular use of abstract data types. Both topics are explored in the context of the INGRES relational database system. "
1982,A Database Perspective.,n/a
1982,A Rules System for a Relational Data Base Management System.,"This paper presents the specification and proposed implementation of a rules system for a relational data base manager. The motivation for this proposal is the fact that integrity constraints, protection, triggers, alerters, and views are ALL examples of special purpose rules systems. We suggest that all five services can be obtained in one unified way through a single rules system. "
1982,TIMBER: A Sophisticated Relation Browser (Invited Paper).,"This paper discusses the functions present in a sophisticated relation browser with support for icons, maps, text and normal fixed format relations. A discussion of some of the required database extensions to support such a browser is also presented."
1981,Operating System Support for Database Management.,"Several operating system services are examined with a view toward their applicability to support of database management functions. These services include buffer pool management; the file system; scheduling, process management, and interprocess communication; and consistency control. s"
1981,Architecture of Future Data Base Systems.,n/a
1981,A Formal Model of Crash Recovery in a Distributed System.,"A formal model for atomic commit protocols for a distributed database system is introduced. The model is used to prove existence results about resilient protocols for site failures that do not partition the network and then for partitioned networks. For site failures, a pessimistic recovery technique, called independent recovery, is introduced and the class of failures for which resilient protocols exist is identified. For partitioned networks, two cases are studied: the pessimistic case in which messages are lost, and the optimistic case in which no messages are lost. In all cases, fundamental limitations on the resiliency of protocols are derived. "
1981,Hypothetical Data Bases as Views.,"In this paper we show that hypothetical data bases can be effectively supported by slight extensions to conventional view support mechanisms. Moreover, we argue that the resulting structure may well be quite efficient and that there are advantages to making hypothetical data bases central to the operation of a DBMS. "
1980,Retrospection on a Database System.,"This paper describes the implementation history of the INGRES database system. It focuses on mistakes that were made in progress rather than on eventual corrections. Some attention is also given to the role of structured design in a database system implementation and to the problem of supporting nontrivial users. Lastly, miscellaneous impressions of UNIX, the PDP-11, and data models are given. "
1980,Embedding Expert Knowledge and Hypothetical Data Bases Into a Data Base System.,"This paper is concerned with adding knowledge to a data base management system and suggests two appropriate mechanisms, namely hypothetical data bases (HDB's) and experts. Herein we indicate the need for HDB's and define the extensions that are needed to a data base system to support HDB's.In addition, we suggest that the notion of ""experts"" is an appropriate way to add semantic knowledge to a data base system. Unlike most other proposals which extend an underlying data model to capture more meaning, our proposal does not require extensions to the schema. Moreover, the DBMS does not even have to know how an expert functions. In this paper we define an expert and indicate how it would be added to one existing data base system."
1980,Analysis of Distributed Data Base Processing Strategies.,In this paper we report on query processing experiments that were performed in one distributed data base environment. In this environment we compared the strategy produced by a collection of algorithms on the basis of number of bytes moved. Among other conclusions we found that limited search algorithms do not perform very well compared to algorithms which exhaust all possible processing plans. 
1979,Locking Granularity Revisited.,"Locking granularity refers to the size and hence the number of locks used to ensure the consistency of a database during multiple concurrent updates. In an earlier simulation study we concluded that coarse granularity, such as area or file locking, is to be preferred to fine granularity such as individual page or record locking.
However, alternate assumptions than those used in the original paper can change that conclusion. First, we modified the assumptions concerning the placement of the locks on the database with respect to the accessing transactions. In the original model the locks were assumed to be well placed. Under worse case and random placement assumptions when only very small transactions access the database, fine granularity is preferable.
Second, we extended the simulation to model a lock hierarchy where large transactions use large locks and small transactions use small locks. In this scenario, again under the random and worse case lock placement assumptions, fine granularity is preferable if all transactions accessing more than 1 percent of the database use large locks.
Finally, the simulation was extended to model a ìclaim as neededî locking strategy together with the resultant possibility of deadlock. In the original study all locks were claimed in one atomic operation at the beginning of a transaction. The claim as needed strategy does not change the conclusions concerning the desired granularity. "
1979,Concurrency Control and Consistency of Multiple Copies of Data in Distributed INGRES.,"Abstract:
This paper contains algorithms for ensuring the consistency of a distributed relational data base subject to multiple, concurrent updates. Also included are mechanisms to correctly update multiple copies of objects and to continue operation when less than all machines in the network are operational. Together with [4] and [12], this paper constitutes the significant portions of the design for a distributed data base version of INGRES."
1979,Performance Analysis of a Relational Data Base Management System.,"The effect on the performance of data management systems of the use of extended storage devices, multiple processors and prefetching data blocks is analyzed with respect to one system, INGRES. Benchmark query streams, derived from user queries, were run on the INGRES system and their CPU usage and data reference patterns traced. The results show that the performance characteristics of two query types: data-intensive queries and overhead-intensive queries, are so different that it may be difficult to design a single architecture to optimize the performance of both types. It is shown that the random access model of data references holds only for overhead-intensive queries, and then only if references to system catalogs are not considered data references. Significant sequentiality of reference was found in the data-intensive queries. It is shown that back-end data management machines that distribute processing toward the data may be cost effective only for data-intensive queries. It is proposed that the best method of distributing the processing of the overhead-intensive query is through the use of intelligent terminals. A third benchmark set, multi-relation queries, was devised, and proposals are made for taking advantage of the locality of reference which was found."
1978,B-trees Re-examined.,"The B-tree and its variants have, with increasing frequency, been proposed as a basic storage structure for multiuser database applications. Here, three potential problems which must be dealt with in such a structure that do not arise in more traditional static directory structures are indicated. One problem is a possible performance penalty. "
1978,Concurrency Control and Consistency of Multiple Copies of Data in Distributed INGRES.,"This paper contains algorithms for ensuring the consistency of a distributed relational data base subject to multiple, concurrent updates. Also included are mechanisms to correctly update multiple copies of objects and to continue operation when less than all machines in the network are operational. Together with [4] and [12], this paper constitutes the significant portions of the design for a distributed data base version of INGRES. "
1978,Distributed Query Processing in a Relational Data Base System.,"In this paper we present a new algorithm for retrieving and updating data from a distributed relational data base. Within such a data base, any number of relations can be distributed over any number of sites. Moreover, a user supplied distribution criteria can optionally be used to specify what site a tuple belongs to.The algorithm is an efficient way to process any query by ""breaking"" the qualification into separate ""pieces"" using a few simple heuristics. The cost criteria considered are minimum response time and minimum communications traffic. In addition, the algorithm can optimize separately for two models of a communication network representing respectively ARPANET and ETHERNET like networks. This algorithm is being implemented as part of the INGRES data base system"
1977,Effects of Locking Granularity in a Database Management System.,"Many database systems guarantee some form of integrity control upon multiple concurrent updates by some form of locking. Some ìgranuleî of the database is chosen as the unit which is individually locked, and a lock management algorithm is used to ensure integrity. Using a simulation model, this paper explores the desired size of a granule. Under a wide variety of seemingly realistic conditions, surprisingly coarse granularity is called for. The paper concludes with some implications of these results concerning the viability of so-called ìpredicate lockingî."
1977,A Distributed Database Version of INGRES.,n/a
1977,GEO-OUEL: a system for the manipulation and display of geographic data.,"This paper briefly summarizes the implementation of GEO-OUEL, a special purpose geographic information retrieval and display system. Basically, it is a rather small ""front end"" to a powerful general purpose relational data base system, INGRES, implemented at Berkeley. Also discussed are the problems that were discovered during the implementation of the original proposal (presented at the 1975 ACM SIGMOD/SIGGRAPH Workshop in Waterloo, Ontario) and the corrective steps taken. Lastly, experiments are described which indicate the performance penalty paid for this ""front end"" approach and the savings in development time realized."
1977,A Study of the Effects of Locking Granularity in a Data Base Management System (Abstract).,"Many data base systems guarantee some form of integrity control upon multiple concurrent updates by some form of locking. Some ""granule"" of the data base is chosen as the unit which is individually locked, and a lock management algorithm is used to ensure integrity. By a simulation model this paper explores the desired size of a ""granule"". Under a wide variety of seemingly realistic conditions, surprisingly coarse granularity is called for. The paper concludes with some implications of these results concerning the viability of so called ""predicate locking""."
1977,Observations on Data Manipulation Languages and Their Embedding in General Purpose Programming Languages.,"Many data base query languages, both stand-alone and coupled to a general purpose programming language, have been proposed. A number of issues that various designs have addressed in different ways are treated in this paper. These issues include the specification of performance options, side effects, implicitness, the handling of types and the time of binding. In all cases, the emphasis is on a comparative analysis, rather than on an exhaustive survey of proposals. Several general observations on language design for data base access are also made.s"
1976,The Design and Implementation of INGRES.,"The currently operational (March 1976) version of the INGRES database management system is described. This multiuser system gives a relational view of data, supports two high level nonprocedural data sublanguages, and runs as a collection of user processes on top of the UNIX operating system for Digital Equipment Corporation PDP 11/40, 11/45, and 11/70 computers. Emphasis is on the design decisions and tradeoffs related to (1) structuring the system into processes, (2) embedding one command language in a general purpose programming language, (3) the algorithms implemented to process interactions, (4) the access methods implemented, (5) the concurrency and recovery control currently provided, and (6) the data structures used for system catalogs and the role of the database administrator.
Also discussed are (1) support for integrity constraints (which is only partly operational), (2) the not yet supported features concerning views and protection, and (3) future plans concerning the system"
1976,SIGBDP (Paper Session).,n/a
1976,The INGRES protection system.,"This paper presents the design of a protection system being implemented for the INGRES relational data base management system. A brief description of the INGRES system and its operational environment is first presented to provide the setting for the protection scheme. Mechanisms for protecting physical data files and enforcing sophisticated access control rules for shared relations are then presented. Lastly, the important design decisions concerning protection are discussed. "
1976,SIGMOD (Paper Session).,n/a
1976,Proposal for a Network INGRES.,n/a
1976,The Data Base Management System INGRES.,n/a
1976,A Comparison of the Use of Links and Secondary Indices in a Relational Data Base System.,"The possibility of supporting relational data sublanguages on top of a data base system with an underlying network data structure has been widely suggested. In this paper we present a formal model of a mix of interactions in one non procedural relational language. Under a collection of assumptions concerning the data base and the performance criteria, we compare the following performance oriented data structures:

(1) secondary indices

(2) a network structure similar to a pointer array implementation of DBTG sets

(3) structures 1) and 2) together

It is shown that option 2) is never preferred to both @@@@ and 3) over the range of model parameters, Hence, the sole use of sets or ìlinksî as a performance oriented access path is questionable."
1976,Embedding a Relational Data Sublanguage in a General Purpose Programming Language.,"This paper describes EQUEL, a programming language which embeds the relational data sublanguage QUEL into the general purpose programming language ìCî. Both QUEL and EQUEL are operational parts of the INGRES relational data base management system at Berkeley. Also briefly described are two operational subsystems written in this combined language. Lastly some of the language oriented shortcomings that have been observed in QUEL and EQUEL are discussed. "
1975,INGRES: A Relational Data Base System.,"INGRES (Interactive Graphics and Retrieval System) is a relational data base and graphics system which is being implemented on a PDP-11/40 based hardware configuration at Berkeley. INGRES runs as a normal user job on top of the UNIX operating system developed at the Bell Telephone Laboratories. The only significant modification to UNIX that INGRES requires is a substantial increase in the maximum file size allowed. This change was implemented by the UNIX designers. The implementation of INGRES is primarily programmed in ""C"", a high level language in which UNIX itself is written. Parsing is done with the assistance of YACC, a compiler-compiler available on UNIX."
1975,"Networks, Hierarchies and Relations in Data Base Management Systems.",n/a
1975,Storage Structures and Access Methods in the Relational Data Base Management System INGRES.,n/a
1975,CUPID - The Friendly Query Language.,n/a
1975,Implementation of Integrity Constraints and Views by Query Modification.,"Because the user interface in a relatonal data base management system may be decoupled from the storage representation of data, novel, powerful and efficient integrity control schemes are possible. This paper indicates the mechanism being implemented in one relational system to prevent integrity violations which can result from improper updates by a process. Basically each interaction with the data is immediately modified at the query language level to one guaranteed to have no integrity violations. Also, a similar modification technique is indicated to support the use of ""views,"" i.e. relations which are not physically present in the data base but are defined in terms of ones that are. "
1975,An Approach to Implementing a Geo-Data System.,"It is often undesirable or impossible to provide redundant indices for all domains of a file existing on a secondary storage device. The problem considered in this paper is the selection of a limited number of indices which best facilitate interaction with a file. A probabilistic model of interaction activity encompassing queries and updates is presented, and a parametric description of the storage medium is assumed. Significant results which are independent of many file and storage characteristics are found concerning the best choice of indices in two cases. The first is the choice of domains to include in a partial inversion. Here it is desired to find the best possible subset of domains for which to provide indices. The second case concerns the choice of combined indices. In this situation the best way of grouping domains is sought in order to provide one index for each group.
"
1974,The choice of partial inversions and combined indices.,"It is often undesirable or impossible to provide redundant indices for all domains of a file existing on a secondary storage device. The problem considered in this paper is the selection of a limited number of indices which best facilitate interaction with a file. A probabilistic model of interaction activity encompassing queries and updates is presented, and a parametric description of the storage medium is assumed. Significant results which are independent of many file and storage characteristics are found concerning the best choice of indices in two cases. The first is the choice of domains to include in a partial inversion. Here it is desired to find the best possible subset of domains for which to provide indices. The second case concerns the choice of combined indices. In this situation the best way of grouping domains is sought in order to provide one index for each group."
1974,Access control in a relational data base management system by query modification.,"This work describes the access control system being implemented in INGRES. The scheme can be applied to any relational data base management system and has several advantages over other suggested schemes. These include: a) implementation ease b) small execution time overhead c) powerful and flexible controls d) conceptual simplicity The basic idea utilized is that a user interaction with the data base is modified to an alternate form which is guaranteed to have no access violations. This modification takes place in a high level interaction language. Hence, the processing of a resulting interaction can be accomplished with no further regard for protection. In particular, any procedure calls in the access paths for control purposes, such as in [1, 2] are avoided."
1974,A Functional View of Data Independence.,"Many researchers have used the term ìdata independenceî without indicating a precise meaning. One common definition isóthe isolation of a program from considerations of the data which it processes [1,2].Another isóthe ability of an applications program to execute correctly regardless of the actual storage of its data[3,4].Although these suggest the general concept, a precise framework is clearly needed. The current paper provides such a framework and explores its ramifications. "
1972,A Simplification of Forrester's Model of an Urban Area.,"Abstract:
A greatly simplified version of Forrester's model of an urban area is described. Whereas Forrester's original model contained 20 states and 150 equations, the revised one contains 9 states and 81 equations. This revised model converges to a close approximation to Forrester's equilibrium through similar temporal behavior and produces comparable results when Forrester's 11 urban programs are applied."
1972,Retrieval Efficiency Using Combined Indexes.,"The problem considered here involves choosing the best set of indices for indexing a file on a secondary storage device where space may be limited. For a general class of queries and a specific index organization, approximations to the expected retrieval time for any choice of indices are developed. Subject to the simplifying assumptions the best selection of indices is obtained for several cases, both where the number of possible lists is constrained and where it is not. The examples indicate that retrieval time is quite sensitive to the choice made."
